I spent $10 testing Vercel's Ship 2025 features.

The first model timed out at 179 seconds.

That's when I learned the real lesson.

Started with the obvious ask: "Build me a complete Next.js application."
Grok died at 28 seconds.
Claude made it to 64.
Gemini was still thinking at 179 seconds when Vercel killed it.

Watching the logs was educational.
Models generating thousands of tokens.
Trying to output entire codebases.
Costs spiraling. Nothing finishing.

So I changed one thing.
"Just share pseudo code, not complete implementations."

Response times dropped from 3 minutes to 80 seconds.
Token usage fell 85%.
All three models suddenly worked.

But here's what really happened:

AI Gateway: Works perfectly. One line for any model.
Queues: "Limited beta" means doesn't exist.
Sandbox: "Public beta" means no API.
Microfrontends: Not available.
BotID: Invite only fiction.

Five features announced. One actually shipped.

So I built anyway. 

An AI showdown where models compete in real-time.
With animations that solve real UX problems.
With cost tracking to fractions of pennies.
With a judge that evaluates responses.

The vision I couldn't build haunts me:

Each model running in its own sandbox.
Generating actual React components.
The judge evaluating running apps, not text.
"Who builds better software" not "who writes better words."

That's the future. When Sandbox ships.

Until then, I learned something valuable.

Constraints don't limit builders.
They reveal platform limitations.

The timeout at 179 seconds wasn't a failure.
It was data.

The code is public: github.com/ramakay/vercelship25

See what $10 teaches about AI economics.
See why pseudo code beats complete code.
See how animation solves async anxiety.

The gap between vision and API is where builders live.

Ship used to mean it works.
Now it means we hope.

I spent $10 testing features.
I learned to build with what ships.