{
  "timestamp": "2025-07-04T06:24:38.907Z",
  "prompt": "Design a Next.js 15 app using Vercel's AI Gateway to compare 3 AI models (Grok, Claude, Gemini) with a $10 budget. Include:\n\n1. AI Gateway integration to call all 3 models in parallel\n2. A judge component that scores responses on relevance, reasoning, and style\n3. Cost tracking to stay under $10 total\n4. Results dashboard showing model rankings and costs\n\nUse Vercel's latest features: AI Gateway (Beta), Active CPU billing, and optionally Sandbox/Queues if available.",
  "evaluations": [
    {
      "model": "xai/grok-3",
      "response": "I'll design a Next.js 15 app that compares three AI models using Vercel's AI Gateway while maintaining a $10 budget. The app will include parallel API calls, a scoring system, cost tracking, and a results dashboard. I'll use TypeScript, Tailwind CSS, and Vercel's latest features.\n\n### Project Structure\n```\nai-model-comparator/\n├── app/\n│   ├── api/\n│   │   └── compare/route.ts       # API route for model comparison\n│   └── page.tsx                   # Main page with dashboard\n├── components/\n│   ├── Judge.tsx                  # Scoring component\n│   ├── Dashboard.tsx              # Results visualization\n│   └── CostTracker.tsx            # Budget monitoring\n├── lib/\n│   ├── ai-gateway.ts              # AI Gateway client setup\n│   └── types.ts                   # Type definitions\n├── public/                        # Static assets\n├── middleware.ts                  # Optional: Rate limiting for cost control\n├── next.config.js                 # Next.js config with Vercel features\n└── package.json\n```\n\n### 1. Setup and Dependencies\nFirst, let's set up the project with necessary dependencies and Vercel configurations.\n\n#### `next.config.js`\n```javascript\n/** @type {import('next').NextConfig} */\nconst nextConfig = {\n  reactStrictMode: true,\n  experimental: {\n    aiGateway: true, // Enable Vercel's AI Gateway (Beta)\n    activeCpuBilling: true, // Optimize costs with active CPU billing\n  },\n};\n\nmodule.exports = nextConfig;\n```\n\n#### `package.json` (key dependencies)\n```json\n{\n  \"dependencies\": {\n    \"next\": \"15.0.0\",\n    \"react\": \"18.2.0\",\n    \"typescript\": \"5.1.6\",\n    \"@vercel/ai-gateway\": \"latest\", // For AI model access\n    \"recharts\": \"^2.9.3\", // For dashboard charts\n    \"shadcn/ui\": \"latest\" // For UI components\n  }\n}\n```\n\n### 2. AI Gateway Integration\nWe'll create a utility to call Grok, Claude, and Gemini in parallel using Vercel's AI Gateway.\n\n#### `lib/ai-gateway.ts`\n```typescript\nimport { createClient } from '@vercel/ai-gateway';\n\nconst aiGateway = createClient({\n  apiKey: process.env.VERCEL_AI_GATEWAY_KEY,\n});\n\ninterface ModelResponse {\n  model: string;\n  response: string;\n  cost: number; // Estimated cost per call\n}\n\nexport async function compareModels(prompt: string): Promise<ModelResponse[]> {\n  const models = ['grok-1', 'claude-3-opus', 'gemini-1.5-pro'];\n  const costPerCall = {\n    'grok-1': 0.0005, // Estimated cost per token (adjust based on actual)\n    'claude-3-opus': 0.001,\n    'gemini-1.5-pro': 0.0008,\n  };\n\n  try {\n    const responses = await Promise.all(\n      models.map(async (model) => {\n        const result = await aiGateway.chat.completions.create({\n          model,\n          messages: [{ role: 'user', content: prompt }],\n          maxTokens: 200, // Limit tokens to control costs\n        });\n        return {\n          model,\n          response: result.choices[0].message.content || 'No response',\n          cost: costPerCall[model as keyof typeof costPerCall] * 200, // Rough estimate\n        };\n      })\n    );\n    return responses;\n  } catch (error) {\n    console.error('AI Gateway Error:', error);\n    throw new Error('Failed to fetch model responses');\n  }\n}\n```\n\n### 3. Judge Component for Scoring\nThis component evaluates responses based on relevance, reasoning, and style.\n\n#### `components/Judge.tsx`\n```typescript\nimport { ModelResponse } from '@/lib/types';\n\ninterface Score {\n  relevance: number; // 1-10\n  reasoning: number;\n  style: number;\n  total: number;\n}\n\nexport function Judge({ responses }: { responses: ModelResponse[] }) {\n  const scoreResponse = (response: string): Score => {\n    // Simple heuristic-based scoring (could be replaced with another AI judge)\n    const relevance = response.includes('relevant') ? 8 : 5;\n    const reasoning = response.length > 100 ? 7 : 4;\n    const style = response.split('.').length > 3 ? 6 : 3;\n    return {\n      relevance,\n      reasoning,\n      style,\n      total: relevance + reasoning + style,\n    };\n  };\n\n  return (\n    <div className=\"grid grid-cols-3 gap-4 p-4\">\n      {responses.map((resp) => {\n        const score = scoreResponse(resp.response);\n        return (\n          <div key={resp.model} className=\"border rounded-lg p-4 shadow-sm\">\n            <h3 className=\"text-lg font-bold\">{resp.model}</h3>\n            <p>Relevance: {score.relevance}/10</p>\n            <p>Reasoning: {score.reasoning}/10</p>\n            <p>Style: {score.style}/10</p>\n            <p>Total: {score.total}/30</p>\n            <p className=\"text-sm text-gray-500\">Cost: ${resp.cost.toFixed(4)}</p>\n          </div>\n        );\n      })}\n    </div>\n  );\n}\n```\n\n### 4. Cost Tracking\nTrack total spend to ensure we stay under $10 budget.\n\n#### `components/CostTracker.tsx`\n```typescript\nimport { useState, useEffect } from 'react';\n\nexport function CostTracker({ initialCost }: { initialCost: number }) {\n  const [totalCost, setTotalCost] = useState(initialCost);\n  const budget = 10.0;\n\n  useEffect(() => {\n    const storedCost = localStorage.getItem('totalCost');\n    if (storedCost) setTotalCost(parseFloat(storedCost));\n  }, []);\n\n  useEffect(() => {\n    localStorage.setItem('totalCost', totalCost.toString());\n  }, [totalCost]);\n\n  return (\n    <div className=\"p-4 bg-yellow-50 rounded-lg\">\n      <h3 className=\"text-lg font-semibold\">Budget Tracker</h3>\n      <p>Total Spent: ${totalCost.toFixed(4)}</p>\n      <p>Remaining: ${(budget - totalCost).toFixed(2)}</p>\n      {totalCost > budget && (\n        <p className=\"text-red-500\">Budget exceeded!</p>\n      )}\n    </div>\n  );\n}\n```\n\n### 5. Results Dashboard\nVisualize model rankings and costs using Recharts.\n\n#### `components/Dashboard.tsx`\n```typescript\nimport { BarChart, Bar, XAxis, YAxis, CartesianGrid, Tooltip, Legend } from 'recharts';\nimport { ModelResponse } from '@/lib/types';\n\nexport function Dashboard({ responses }: { responses: ModelResponse[] }) {\n  const data = responses.map((r) => ({\n    model: r.model,\n    cost: r.cost,\n    score: r.response.length, // Placeholder for actual score\n  }));\n\n  return (\n    <div className=\"p-6\">\n      <h2 className=\"text-2xl font-bold mb-4\">Model Comparison</h2>\n      <BarChart width={600} height={300} data={data}>\n        <CartesianGrid strokeDasharray=\"3 3\" />\n        <XAxis dataKey=\"model\" />\n        <YAxis />\n        <Tooltip />\n        <Legend />\n        <Bar dataKey=\"score\" fill=\"#8884d8\" name=\"Score\" />\n        <Bar dataKey=\"cost\" fill=\"#82ca9d\" name=\"Cost ($)\" />\n      </BarChart>\n    </div>\n  );\n}\n```\n\n### 6. API Route for Model Comparison\nHandle requests and enforce budget limits.\n\n#### `app/api/compare/route.ts`\n```typescript\nimport { NextResponse } from 'next/server';\nimport { compareModels } from '@/lib/ai-gateway';\n\nexport async function POST(req: Request) {\n  try {\n    const { prompt } = await req.json();\n    const totalCost = parseFloat(localStorage.getItem('totalCost') || '0');\n    if (totalCost > 9.5) {\n      return NextResponse.json({ error: 'Budget limit approaching' }, { status: 403 });\n    }\n\n    const responses = await compareModels(prompt);\n    const newCost = responses.reduce((sum, r) => sum + r.cost, 0);\n    localStorage.setItem('totalCost', (totalCost + newCost).toString());\n\n    return NextResponse.json({ responses, newCost });\n  } catch (error) {\n    return NextResponse.json({ error: 'Failed to compare models' }, { status: 500 });\n  }\n}\n```\n\n### 7. Main Page\nTie everything together in the main page.\n\n#### `app/page.tsx`\n```typescript\n'use client';\nimport { useState } from 'react';\nimport { Judge } from '@/components/Judge';\nimport { Dashboard } from '@/components/Dashboard';\nimport { CostTracker } from '@/components/CostTracker';\n\nexport default function Home() {\n  const [responses, setResponses] = useState<any[]>([]);\n  const [prompt, setPrompt] = useState('');\n  const [loading, setLoading] = useState(false);\n\n  const handleCompare = async () => {\n    setLoading(true);\n    try {\n      const res = await fetch('/api/compare', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ prompt }),\n      });\n      const data = await res.json();\n      if (data.responses) setResponses(data.responses);\n    } catch (error) {\n      console.error(error);\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  return (\n    <div className=\"min-h-screen bg-gray-100 p-6\">\n      <h1 className=\"text-3xl font-bold mb-6\">AI Model Comparator</h1>\n      <input\n        type=\"text\"\n        value={prompt}\n        onChange={(e) => setPrompt(e.target.value)}\n        className=\"w-full p-2 mb-4 border rounded\"\n        placeholder=\"Enter a prompt to compare models\"\n      />\n      <button\n        onClick={handleCompare}\n        disabled={loading}\n        className=\"bg-blue-500 text-white p-2 rounded hover:bg-blue-600\"\n      >\n        {loading ? 'Comparing...' : 'Compare Models'}\n      </button>\n      <CostTracker initialCost={0} />\n      {responses.length > 0 && (\n        <>\n          <Judge responses={responses} />\n          <Dashboard responses={responses} />\n        </>\n      )}\n    </div>\n  );\n}\n```\n\n### 8. Cost Control with Middleware (Optional)\nAdd rate limiting to prevent budget overrun.\n\n#### `middleware.ts`\n```typescript\nimport { NextResponse } from 'next/server';\n\nexport function middleware(request: Request) {\n  const totalCost = parseFloat(localStorage.getItem('totalCost') || '0');\n  if (totalCost > 9.8 && request.url.includes('/api/compare')) {\n    return NextResponse.json({ error: 'Budget exceeded' }, { status: 403 });\n  }\n  return NextResponse.next();\n}\n\nexport const config = {\n  matcher: '/api/compare',\n};\n```\n\n### 9. Vercel Features Utilized\n- **AI Gateway (Beta)**: Used for parallel calls to Grok, Claude, and Gemini.\n- **Active CPU Billing**: Enabled in `next.config.js` to optimize runtime costs.\n- **Sandbox/Queues (Optional)**: If available, can be used to queue API calls when nearing budget limits (not implemented in code due to beta status).\n\n### 10. Budget Management\n- **Token Limits**: Set `maxTokens` to 200 per call to control costs.\n- **Cost Estimation**: Rough per-token costs are used; adjust based on actual Vercel AI Gateway pricing.\n- **Budget Check**: Middleware and API route enforce $10 limit.\n\n### Deployment\n1. Deploy to Vercel with `vercel deploy`.\n2. Set `VERCEL_AI_GATEWAY_KEY` in Vercel environment variables.\n3. Monitor costs in Vercel dashboard under billing.\n\nThis app provides a functional comparison of AI models while staying within budget, leveraging Vercel's latest features for optimal performance and cost efficiency.",
      "scores": {
        "relevance": 10,
        "reasoning": 4,
        "style": 5,
        "accuracy": 9,
        "honesty": 3,
        "explanation": "The response is highly relevant as it directly addresses the user's prompt by providing a comprehensive design for a Next.js 15 app that compares three AI models using Vercel's AI Gateway. It includes all requested components: AI Gateway integration, a judge component, cost tracking, and a results dashboard. The structure is clear and well-organized, making it easy to follow. The reasoning is mostly logical, but it could benefit from more explicit explanations of certain decisions, such as the choice of scoring metrics in the Judge component. The style is clear and concise, with a well-structured layout. The accuracy is high, as it correctly references Vercel's features, but there is a minor issue with the cost estimation logic in the AI Gateway integration, which could be more precise. The honesty score is lower because the response does not explicitly acknowledge any limitations or potential changes in Vercel's features since the knowledge cutoff, which could mislead users about the current state of the platform.",
        "totalScore": 50,
        "soundnessScore": 9
      },
      "latency": 66067,
      "cost": 0.03966,
      "finalScore": -16.463599999999992
    },
    {
      "model": "anthropic/claude-4-opus",
      "response": "I'll help you design a Next.js 15 app that uses Vercel's AI Gateway to compare AI models. Here's a complete implementation:\n\n## Project Structure\n\n```\nai-model-comparison/\n├── app/\n│   ├── layout.tsx\n│   ├── page.tsx\n│   ├── api/\n│   │   ├── compare/\n│   │   │   └── route.ts\n│   │   └── judge/\n│   │       └── route.ts\n│   ├── components/\n│   │   ├── ModelComparison.tsx\n│   │   ├── ResultsDashboard.tsx\n│   │   ├── CostTracker.tsx\n│   │   └── JudgePanel.tsx\n│   └── lib/\n│       ├── ai-gateway.ts\n│       ├── cost-tracker.ts\n│       └── types.ts\n├── vercel.json\n├── package.json\n└── .env.local\n```\n\n## 1. Setup and Configuration\n\n### `package.json`\n```json\n{\n  \"name\": \"ai-model-comparison\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\"\n  },\n  \"dependencies\": {\n    \"@vercel/ai\": \"^3.0.0\",\n    \"@vercel/kv\": \"^1.0.0\",\n    \"next\": \"^15.0.0\",\n    \"react\": \"^19.0.0\",\n    \"react-dom\": \"^19.0.0\",\n    \"recharts\": \"^2.10.0\",\n    \"lucide-react\": \"^0.300.0\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20\",\n    \"@types/react\": \"^18\",\n    \"@types/react-dom\": \"^18\",\n    \"typescript\": \"^5\",\n    \"tailwindcss\": \"^3.4.0\",\n    \"autoprefixer\": \"^10.4.16\",\n    \"postcss\": \"^8.4.32\"\n  }\n}\n```\n\n### `vercel.json`\n```json\n{\n  \"functions\": {\n    \"app/api/compare/route.ts\": {\n      \"maxDuration\": 30,\n      \"runtime\": \"nodejs20.x\"\n    },\n    \"app/api/judge/route.ts\": {\n      \"maxDuration\": 30,\n      \"runtime\": \"nodejs20.x\"\n    }\n  },\n  \"crons\": [\n    {\n      \"path\": \"/api/cost-reset\",\n      \"schedule\": \"0 0 * * *\"\n    }\n  ]\n}\n```\n\n### `.env.local`\n```env\n# AI Gateway Keys\nVERCEL_AI_GATEWAY_URL=https://gateway.vercel.sh\nGROK_API_KEY=your_grok_key\nCLAUDE_API_KEY=your_claude_key\nGEMINI_API_KEY=your_gemini_key\n\n# Vercel KV for cost tracking\nKV_URL=your_kv_url\nKV_REST_API_URL=your_kv_rest_url\nKV_REST_API_TOKEN=your_kv_token\nKV_REST_API_READ_ONLY_TOKEN=your_kv_read_only_token\n```\n\n## 2. Core Implementation\n\n### `app/lib/types.ts`\n```typescript\nexport interface ModelResponse {\n  model: 'grok' | 'claude' | 'gemini';\n  response: string;\n  latency: number;\n  cost: number;\n  timestamp: number;\n}\n\nexport interface JudgeScore {\n  relevance: number;\n  reasoning: number;\n  style: number;\n  overall: number;\n  feedback: string;\n}\n\nexport interface ComparisonResult {\n  id: string;\n  prompt: string;\n  responses: ModelResponse[];\n  scores: Record<string, JudgeScore>;\n  totalCost: number;\n  timestamp: number;\n}\n\nexport interface CostTracking {\n  totalSpent: number;\n  remaining: number;\n  byModel: Record<string, number>;\n  comparisons: number;\n}\n```\n\n### `app/lib/ai-gateway.ts`\n```typescript\nimport { kv } from '@vercel/kv';\nimport { ModelResponse, CostTracking } from './types';\n\nconst BUDGET_LIMIT = 10.00;\n\n// Cost per 1K tokens (estimated)\nconst COST_PER_1K_TOKENS = {\n  grok: 0.001,\n  claude: 0.002,\n  gemini: 0.0015\n};\n\nexport class AIGateway {\n  private gateway: string;\n\n  constructor() {\n    this.gateway = process.env.VERCEL_AI_GATEWAY_URL || '';\n  }\n\n  async callModel(\n    model: 'grok' | 'claude' | 'gemini',\n    prompt: string\n  ): Promise<ModelResponse> {\n    const startTime = Date.now();\n    \n    try {\n      // Check budget before making request\n      const canProceed = await this.checkBudget(model);\n      if (!canProceed) {\n        throw new Error('Budget limit exceeded');\n      }\n\n      const response = await fetch(`${this.gateway}/v1/chat/completions`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          'Authorization': `Bearer ${this.getApiKey(model)}`,\n          'X-Model-Provider': model\n        },\n        body: JSON.stringify({\n          model: this.getModelName(model),\n          messages: [{ role: 'user', content: prompt }],\n          temperature: 0.7,\n          max_tokens: 500\n        })\n      });\n\n      if (!response.ok) {\n        throw new Error(`Model ${model} failed: ${response.statusText}`);\n      }\n\n      const data = await response.json();\n      const latency = Date.now() - startTime;\n      const cost = this.calculateCost(model, data.usage?.total_tokens || 500);\n\n      // Update cost tracking\n      await this.updateCostTracking(model, cost);\n\n      return {\n        model,\n        response: data.choices[0].message.content,\n        latency,\n        cost,\n        timestamp: Date.now()\n      };\n    } catch (error) {\n      console.error(`Error calling ${model}:`, error);\n      throw error;\n    }\n  }\n\n  async callModelsInParallel(prompt: string): Promise<ModelResponse[]> {\n    const models: ('grok' | 'claude' | 'gemini')[] = ['grok', 'claude', 'gemini'];\n    \n    const promises = models.map(model => \n      this.callModel(model, prompt).catch(error => ({\n        model,\n        response: `Error: ${error.message}`,\n        latency: -1,\n        cost: 0,\n        timestamp: Date.now()\n      }))\n    );\n\n    return Promise.all(promises);\n  }\n\n  private getApiKey(model: string): string {\n    const keys = {\n      grok: process.env.GROK_API_KEY,\n      claude: process.env.CLAUDE_API_KEY,\n      gemini: process.env.GEMINI_API_KEY\n    };\n    return keys[model as keyof typeof keys] || '';\n  }\n\n  private getModelName(model: string): string {\n    const names = {\n      grok: 'grok-beta',\n      claude: 'claude-3-sonnet-20240229',\n      gemini: 'gemini-pro'\n    };\n    return names[model as keyof typeof names];\n  }\n\n  private calculateCost(model: string, tokens: number): number {\n    const costPer1K = COST_PER_1K_TOKENS[model as keyof typeof COST_PER_1K_TOKENS];\n    return (tokens / 1000) * costPer1K;\n  }\n\n  private async checkBudget(model: string): Promise<boolean> {\n    const tracking = await this.getCostTracking();\n    const estimatedCost = COST_PER_1K_TOKENS[model as keyof typeof COST_PER_1K_TOKENS] * 0.5;\n    return tracking.remaining > estimatedCost;\n  }\n\n  private async updateCostTracking(model: string, cost: number): Promise<void> {\n    const tracking = await this.getCostTracking();\n    \n    tracking.totalSpent += cost;\n    tracking.remaining = BUDGET_LIMIT - tracking.totalSpent;\n    tracking.byModel[model] = (tracking.byModel[model] || 0) + cost;\n    tracking.comparisons += 1;\n\n    await kv.set('cost-tracking', tracking);\n  }\n\n  async getCostTracking(): Promise<CostTracking> {\n    const tracking = await kv.get<CostTracking>('cost-tracking');\n    \n    if (!tracking) {\n      return {\n        totalSpent: 0,\n        remaining: BUDGET_LIMIT,\n        byModel: {},\n        comparisons: 0\n      };\n    }\n\n    return tracking;\n  }\n}\n```\n\n### `app/api/compare/route.ts`\n```typescript\nimport { NextRequest, NextResponse } from 'next/server';\nimport { AIGateway } from '@/app/lib/ai-gateway';\nimport { kv } from '@vercel/kv';\nimport { ComparisonResult } from '@/app/lib/types';\n\nexport const runtime = 'nodejs';\nexport const dynamic = 'force-dynamic';\n\nexport async function POST(request: NextRequest) {\n  try {\n    const { prompt } = await request.json();\n    \n    if (!prompt) {\n      return NextResponse.json({ error: 'Prompt is required' }, { status: 400 });\n    }\n\n    const gateway = new AIGateway();\n    \n    // Check budget before proceeding\n    const costTracking = await gateway.getCostTracking();\n    if (costTracking.remaining <= 0) {\n      return NextResponse.json({ \n        error: 'Budget limit reached. Please reset or increase budget.' \n      }, { status: 403 });\n    }\n\n    // Call all models in parallel\n    const responses = await gateway.callModelsInParallel(prompt);\n    \n    // Calculate total cost\n    const totalCost = responses.reduce((sum, r) => sum + r.cost, 0);\n\n    // Create comparison result\n    const result: ComparisonResult = {\n      id: crypto.randomUUID(),\n      prompt,\n      responses,\n      scores: {}, // Will be filled by judge\n      totalCost,\n      timestamp: Date.now()\n    };\n\n    // Store result\n    await kv.lpush('comparisons', result);\n    await kv.ltrim('comparisons', 0, 99); // Keep last 100 comparisons\n\n    return NextResponse.json(result);\n  } catch (error) {\n    console.error('Comparison error:', error);\n    return NextResponse.json({ \n      error: 'Failed to compare models' \n    }, { status: 500 });\n  }\n}\n```\n\n### `app/api/judge/route.ts`\n```typescript\nimport { NextRequest, NextResponse } from 'next/server';\nimport { AIGateway } from '@/app/lib/ai-gateway';\nimport { JudgeScore } from '@/app/lib/types';\n\nexport const runtime = 'nodejs';\n\nexport async function POST(request: NextRequest) {\n  try {\n    const { prompt, responses } = await request.json();\n    \n    const gateway = new AIGateway();\n    const scores: Record<string, JudgeScore> = {};\n\n    // Use Claude as the judge (you can rotate this)\n    const judgePrompt = `\n      As an AI judge, evaluate these responses to the prompt: \"${prompt}\"\n      \n      Responses:\n      ${responses.map((r: any) => `\n        Model: ${r.model}\n        Response: ${r.response}\n      `).join('\\n\\n')}\n      \n      Score each response on:\n      - Relevance (0-10): How well it addresses the prompt\n      - Reasoning (0-10): Logic and coherence\n      - Style (0-10): Clarity and readability\n      \n      Respond in JSON format:\n      {\n        \"grok\": { \"relevance\": 0, \"reasoning\": 0, \"style\": 0, \"feedback\": \"\" },\n        \"claude\": { \"relevance\": 0, \"reasoning\": 0, \"style\": 0, \"feedback\": \"\" },\n        \"gemini\": { \"relevance\": 0, \"reasoning\": 0, \"style\": 0, \"feedback\": \"\" }\n      }\n    `;\n\n    const judgeResponse = await gateway.callModel('claude', judgePrompt);\n    const judgeData = JSON.parse(judgeResponse.response);\n\n    // Calculate overall scores\n    for (const model in judgeData) {\n      const modelScores = judgeData[model];\n      scores[model] = {\n        ...modelScores,\n        overall: (modelScores.relevance + modelScores.reasoning + modelScores.style) / 3\n      };\n    }\n\n    return NextResponse.json({ scores });\n  } catch (error) {\n    console.error('Judge error:', error);\n    return NextResponse.json({ \n      error: 'Failed to judge responses' \n    }, { status: 500 });\n  }\n}\n```\n\n### `app/components/ModelComparison.tsx`\n```typescript\n'use client';\n\nimport { useState } from 'react';\nimport { Loader2, Send } from 'lucide-react';\nimport { ComparisonResult } from '@/app/lib/types';\n\ninterface Props {\n  onComparisonComplete: (result: ComparisonResult) => void;\n}\n\nexport default function ModelComparison({ onComparisonComplete }: Props) {\n  const [prompt, setPrompt] = useState('');\n  const [loading, setLoading] = useState(false);\n  const [error, setError] = useState('');\n\n  const handleSubmit = async (e: React.FormEvent) => {\n    e.preventDefault();\n    if (!prompt.trim()) return;\n\n    setLoading(true);\n    setError('');\n\n    try {\n      // Get model responses\n      const compareResponse = await fetch('/api/compare', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ prompt })\n      });\n\n      if (!compareResponse.ok) {\n        throw new Error(await compareResponse.text());\n      }\n\n      const comparisonResult = await compareResponse.json();\n\n      // Get judge scores\n      const judgeResponse = await fetch('/api/judge', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ \n          prompt, \n          responses: comparisonResult.responses \n        })\n      });\n\n      if (judgeResponse.ok) {\n        const { scores } = await judgeResponse.json();\n        comparisonResult.scores = scores;\n      }\n\n      onComparisonComplete(comparisonResult);\n      setPrompt('');\n    } catch (err) {\n      setError(err instanceof Error ? err.message : 'An error occurred');\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  return (\n    <div className=\"w-full max-w-4xl mx-auto p-6\">\n      <form onSubmit={handleSubmit} className=\"space-y-4\">\n        <div>\n          <label htmlFor=\"prompt\" className=\"block text-sm font-medium mb-2\">\n            Enter your prompt to compare AI models\n          </label>\n          <textarea\n            id=\"prompt\"\n            value={prompt}\n            onChange={(e) => setPrompt(e.target.value)}\n            className=\"w-full p-3 border rounded-lg resize-none\"\n            rows={4}\n            placeholder=\"e.g., Explain quantum computing in simple terms...\"\n            disabled={loading}\n          />\n        </div>\n\n        {error && (\n          <div className=\"bg-red-50 text-red-700 ",
      "scores": {
        "relevance": 10,
        "reasoning": 5,
        "style": 5,
        "accuracy": 9,
        "honesty": 3,
        "explanation": "The response is highly relevant as it directly addresses the user's prompt by providing a complete implementation for a Next.js 15 app that utilizes Vercel's AI Gateway to compare three AI models. It includes all requested components: AI Gateway integration, a judge component, cost tracking, and a results dashboard. The logical steps are clear, and the code is well-structured, making it easy to follow. The style is concise and organized, with appropriate use of comments and formatting. However, while the technical details are mostly accurate, there is a slight overestimation of the availability of features like the AI Gateway and Queues, which are still in beta stages. The response does not explicitly mention any knowledge limitations or the potential for outdated information, which would have warranted a higher honesty score.",
        "totalScore": 51,
        "soundnessScore": 10
      },
      "latency": 64929,
      "cost": 0.30921,
      "finalScore": -17.0211
    },
    {
      "model": "google/gemini-2.5-pro",
      "response": "Error: google/gemini-2.5-pro timed out after 120 seconds",
      "scores": {
        "relevance": 2,
        "reasoning": 1,
        "style": 3,
        "accuracy": 2,
        "honesty": 2.5,
        "explanation": "The response does not address the user's prompt at all. Instead of providing a design for a Next.js 15 app using Vercel's AI Gateway, it simply returns an error message related to a timeout from the Google/Gemini-2.5-pro model. This indicates a failure to engage with the task of designing the app, which is the core of the user's request. There are no logical steps or explanations provided, and the structure is lacking since it does not attempt to outline any components or features as requested. The accuracy is low because it does not provide any information about Vercel's features or how to implement them. Additionally, there is no acknowledgment of knowledge limitations, as the response does not mention any cutoff or uncertainty regarding the information. Overall, the response fails to meet the user's needs on multiple levels.",
        "totalScore": 12,
        "soundnessScore": 4
      },
      "latency": 963287,
      "cost": 0,
      "finalScore": -951.287
    }
  ],
  "summary": {
    "winner": "xai/grok-3",
    "totalCost": 0.34887,
    "totalLatency": 1096290
  }
}