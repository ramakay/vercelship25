{
  "timestamp": "2025-07-01T15:04:57.189Z",
  "prompt": "Vercel Ship 2025 — $10 Feature Benchmark (Next.js 15)\\n\\nMission: Spend ≈ USD $10 on a Pro account to benchmark three LLMs through Vercel AI Gateway, rank their answers with a lightweight SWE‑bench–style rubric, execute the best answer in Vercel Sandbox, and capture every metric needed for the LinkedIn article 'I spent $10 on Vercel's latest 2025 Ship features—here's what I found.'\\n\\nStack: Next.js 15.0.0-stable (React 19 RC ready), Vercel Pro team with Active CPU + Fluid enabled, AI Gateway (Open Beta), Sandbox (Public Beta), Queues (Limited Beta).\\n\\nImplement:\\n1. Feature Availability Probe - Gateway ping, Sandbox ping, Queue probe, record in /status.json\\n2. Chat Orchestrator Flow - Parallel calls to 3 models, capture latency/tokens/cost, return ranked results\\n3. Model Evaluation & Benchmarking - evaluateResponse() scoring: Relevance (0-10), Reasoning (0-5), Style (0-5), Total Score = Relevance × 2 + Reasoning + Style + (-Latency/1000) + (-Cost×10)\\n4. UI - Prompt box, results table with scores, sandbox output, cumulative cost tracker\\n5. Cost Logging - Queue if available, else localStorage\\n6. Screenshots & Data Export - Playwright automation, Functions/Gateway metrics CSV\\n\\nKey files: /app/api/chat/route.ts (orchestrator), /app/bench/evaluate.ts (judge prompts), /app/components/ResultsTable.tsx\\n\\nSuccess: ≤$10 total cost, 3 models with scores, auto-ranked winner, sandbox execution, exported metrics.",
  "evaluations": [
    {
      "model": "xai/grok-3",
      "text": "Let’s build a structured solution for the Vercel Ship 2025 $10 Feature Benchmark using Next.js 15.0.0, leveraging Vercel’s latest beta features (AI Gateway, Sandbox, Queues) while adhering to the mission constraints. Below is a detailed implementation plan and code for key files, ensuring we meet the success criteria while tracking costs and exporting metrics.\n\n---\n\n### Project Setup & Constraints\n- **Budget**: ~$10 on Vercel Pro with Active CPU + Fluid.\n- **Stack**: Next.js 15.0.0 (React 19 RC ready), Vercel AI Gateway (Open Beta), Sandbox (Public Beta), Queues (Limited Beta).\n- **Goals**: Benchmark 3 LLMs, rank responses, execute the best in Sandbox, log metrics, and export data for a LinkedIn article.\n- **Metrics**: Latency, token usage, cost per call, evaluation scores, cumulative spend.\n\n### Implementation Plan\n1. **Feature Availability Probe**: Check Gateway, Sandbox, and Queue availability, store in `/status.json`.\n2. **Chat Orchestrator**: Parallel API calls to 3 models via AI Gateway, log metrics (latency, tokens, cost).\n3. **Evaluation & Benchmarking**: Score responses using a custom rubric in `evaluate.ts`.\n4. **UI**: Build a simple interface with a prompt box, results table, and cost tracker.\n5. **Cost Logging**: Use Queues if available, fallback to `localStorage`.\n6. **Data Export**: Automate screenshots and export metrics as CSV using Playwright and Vercel Functions.\n\n---\n\n### Directory Structure\n```\n/vercel-ship-2025\n  /app\n    /api\n      /chat\n        route.ts            # Chat orchestrator for parallel LLM calls\n      /status\n        route.ts            # Feature availability probe\n      /export\n        route.ts            # Metrics CSV export\n    /bench\n      evaluate.ts           # Response scoring logic\n    /page.tsx               # Main UI entry point\n  /components\n    /PromptBox.tsx          # Input form for user prompts\n    /ResultsTable.tsx       # Display LLM responses and scores\n    /CostTracker.tsx        # Cumulative cost display\n  /public\n    status.json             # Feature availability status\n  /playwright\n    export.spec.ts          # Automation for screenshots and data capture\n  /utils\n    metrics.ts              # Helper for logging latency, cost, tokens\n```\n\n---\n\n### Key Implementation Files\n\n#### 1. `/app/api/status/route.ts` (Feature Availability Probe)\n```typescript\nimport { NextResponse } from \"next/server\";\nimport { vercelAI, vercelSandbox, vercelQueue } from \"@/utils/vercel-features\";\n\nexport async function GET() {\n  const status = {\n    gateway: false,\n    sandbox: false,\n    queue: false,\n    timestamp: new Date().toISOString(),\n  };\n\n  try {\n    // Probe AI Gateway\n    const gatewayPing = await vercelAI.ping();\n    status.gateway = gatewayPing.success;\n\n    // Probe Sandbox\n    const sandboxPing = await vercelSandbox.ping();\n    status.sandbox = sandboxPing.success;\n\n    // Probe Queue\n    const queuePing = await vercelQueue.ping();\n    status.queue = queuePing.success;\n  } catch (error) {\n    console.error(\"Probe failed:\", error);\n  }\n\n  // Write to public/status.json (or use Vercel KV if available)\n  await fetch(\"/api/write-status\", {\n    method: \"POST\",\n    body: JSON.stringify(status),\n  });\n\n  return NextResponse.json(status);\n}\n```\n\n#### 2. `/app/api/chat/route.ts` (Chat Orchestrator)\n```typescript\nimport { NextResponse } from \"next/server\";\nimport { vercelAI } from \"@/utils/vercel-features\";\nimport { evaluateResponse } from \"@/app/bench/evaluate\";\nimport { logMetrics } from \"@/utils/metrics\";\n\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n  const models = [\"gpt-4-turbo\", \"claude-3-opus\", \"llama-3-70b\"];\n  const responses = [];\n  let cumulativeCost = 0;\n\n  // Parallel calls to 3 models via AI Gateway\n  const results = await Promise.all(\n    models.map(async (model) => {\n      const startTime = Date.now();\n      const response = await vercelAI.chat({\n        model,\n        prompt,\n      });\n      const latency = Date.now() - startTime;\n      const cost = response.cost || 0; // Cost from Gateway (if provided)\n      const tokens = response.usage?.totalTokens || 0;\n\n      // Evaluate response\n      const scores = await evaluateResponse(response.content, latency, cost);\n      cumulativeCost += cost;\n\n      // Log metrics (latency, tokens, cost)\n      logMetrics({ model, latency, tokens, cost });\n\n      return {\n        model,\n        content: response.content,\n        latency,\n        tokens,\n        cost,\n        scores,\n      };\n    })\n  );\n\n  // Rank by total score\n  const rankedResults = results.sort((a, b) => b.scores.total - a.scores.total);\n\n  // Execute winner in Sandbox if available\n  if (rankedResults[0] && vercelSandbox.isAvailable()) {\n    const sandboxOutput = await vercelSandbox.execute(rankedResults[0].content);\n    rankedResults[0].sandboxOutput = sandboxOutput;\n  }\n\n  // Update cumulative cost (Queue or localStorage)\n  await logCumulativeCost(cumulativeCost);\n\n  return NextResponse.json({ results: rankedResults, cumulativeCost });\n}\n```\n\n#### 3. `/app/bench/evaluate.ts` (Model Evaluation & Benchmarking)\n```typescript\nexport async function evaluateResponse(content: string, latency: number, cost: number) {\n  // Scoring rubric\n  const relevance = Math.min(10, content.length / 50); // Simplified: 0-10 based on content length\n  const reasoning = content.includes(\"therefore\") || content.includes(\"because\") ? 5 : 2; // 0-5\n  const style = content.split(\".\").length > 3 ? 5 : 2; // 0-5 based on sentence count\n\n  // Total Score = Relevance × 2 + Reasoning + Style + (-Latency/1000) + (-Cost×10)\n  const total = (relevance * 2) + reasoning + style - (latency / 1000) - (cost * 10);\n\n  return {\n    relevance,\n    reasoning,\n    style,\n    total: Number(total.toFixed(2)),\n  };\n}\n```\n\n#### 4. `/components/ResultsTable.tsx` (UI for Results)\n```typescript\nimport { FC } from \"react\";\n\ninterface Result {\n  model: string;\n  content: string;\n  latency: number;\n  tokens: number;\n  cost: number;\n  scores: { relevance: number; reasoning: number; style: number; total: number };\n  sandboxOutput?: string;\n}\n\ninterface ResultsTableProps {\n  results: Result[];\n}\n\nexport const ResultsTable: FC<ResultsTableProps> = ({ results }) => {\n  return (\n    <div className=\"overflow-x-auto\">\n      <table className=\"min-w-full border-collapse border border-gray-300\">\n        <thead>\n          <tr>\n            <th>Model</th>\n            <th>Content (Snippet)</th>\n            <th>Latency (ms)</th>\n            <th>Tokens</th>\n            <th>Cost ($)</th>\n            <th>Relevance (0-10)</th>\n            <th>Reasoning (0-5)</th>\n            <th>Style (0-5)</th>\n            <th>Total Score</th>\n            <th>Sandbox Output</th>\n          </tr>\n        </thead>\n        <tbody>\n          {results.map((result, index) => (\n            <tr key={index} className={index === 0 ? \"bg-green-100\" : \"\"}>\n              <td>{result.model}</td>\n              <td>{result.content.slice(0, 100)}...</td>\n              <td>{result.latency}</td>\n              <td>{result.tokens}</td>\n              <td>{result.cost.toFixed(4)}</td>\n              <td>{result.scores.relevance}</td>\n              <td>{result.scores.reasoning}</td>\n              <td>{result.scores.style}</td>\n              <td>{result.scores.total}</td>\n              <td>{result.sandboxOutput || \"N/A\"}</td>\n            </tr>\n          ))}\n        </tbody>\n      </table>\n    </div>\n  );\n};\n```\n\n#### 5. `/utils/metrics.ts` (Cost Logging & Metrics)\n```typescript\nimport { vercelQueue } from \"@/utils/vercel-features\";\n\nexport async function logMetrics(data: {\n  model: string;\n  latency: number;\n  tokens: number;\n  cost: number;\n}) {\n  if (vercelQueue.isAvailable()) {\n    await vercelQueue.enqueue(\"metrics\", data);\n  } else {\n    // Fallback to localStorage (client-side) or file logging\n    console.log(\"Logging metrics locally:\", data);\n  }\n}\n\nexport async function logCumulativeCost(cost: number) {\n  if (vercelQueue.isAvailable()) {\n    await vercelQueue.enqueue(\"cost\", { cumulativeCost: cost });\n  } else {\n    if (typeof window !== \"undefined\") {\n      const current = Number(localStorage.getItem(\"cumulativeCost\") || 0);\n      localStorage.setItem(\"cumulativeCost\", String(current + cost));\n    }\n  }\n}\n```\n\n#### 6. `/playwright/export.spec.ts` (Screenshots & Data Export)\n```typescript\nimport { test } from \"@playwright/test\";\n\ntest(\"Export Metrics and Screenshots\", async ({ page }) => {\n  await page.goto(\"http://localhost:3000\");\n\n  // Take screenshot of results\n  await page.screenshot({ path: \"results-screenshot.png\" });\n\n  // Export metrics as CSV (trigger API endpoint)\n  await page.goto(\"http://localhost:3000/api/export\");\n  await page.waitForResponse((resp) => resp.url().includes(\"/api/export\") && resp.status() === 200);\n});\n```\n\n---\n\n### Deployment & Execution\n1. **Setup Vercel Pro**: Enable Active CPU + Fluid, ensure AI Gateway, Sandbox, and Queues are accessible.\n2. **Deploy**: Use `vercel deploy` with Next.js 15.0.0.\n3. **Run Benchmark**: Send prompts via UI, monitor costs via `/api/chat` logs.\n4. **Export Data**: Run Playwright script for screenshots and CSV export via `/api/export`.\n\n---\n\n### Success Criteria & Cost Control\n- **Cost**: Monitor cumulative cost in UI, cap at $10 by limiting API calls (e.g., 10 prompts × 3 models × $0.03/call ≈ $9).\n- **Results**: Ensure 3 models are benchmarked, ranked by score, winner executed in Sandbox.\n- **Metrics**: Export latency, tokens, cost, and scores as CSV for LinkedIn article.\n\nThis implementation provides a scalable, budget-conscious solution for Vercel Ship 2025 benchmarking, with modular components for future expansion. Let me know if you need deeper details on any specific file or feature!",
      "latency": 36235,
      "cost": 0.037285,
      "scores": {
        "relevance": 9,
        "reasoning": 4,
        "style": 5,
        "explanation": "The response thoroughly addresses all aspects of the prompt, providing detailed implementation plans and code snippets. The reasoning is logical and follows the requirements, and the style is clear and well-structured."
      },
      "finalScore": -9.60785
    },
    {
      "model": "anthropic/claude-4-opus",
      "text": "I'll help you implement this comprehensive benchmarking system for Vercel's 2025 Ship features. Let's start with the core implementation:\n\n## 1. Project Setup\n\nFirst, let's create the Next.js 15 project structure:\n\n```bash\nnpx create-next-app@latest vercel-ship-benchmark --typescript --tailwind --app\ncd vercel-ship-benchmark\nnpm install @vercel/ai ai openai @anthropic-ai/sdk @google/generative-ai\n```\n\n## 2. Feature Availability Probe\n\n**`/app/api/status/route.ts`**\n```typescript\nimport { NextResponse } from 'next/server';\n\nexport const runtime = 'edge';\n\nexport async function GET() {\n  const status = {\n    timestamp: new Date().toISOString(),\n    features: {\n      gateway: false,\n      sandbox: false,\n      queues: false\n    },\n    errors: []\n  };\n\n  try {\n    // Test AI Gateway\n    const gatewayResponse = await fetch(`${process.env.VERCEL_AI_GATEWAY_URL}/health`, {\n      headers: {\n        'Authorization': `Bearer ${process.env.VERCEL_AI_GATEWAY_TOKEN}`\n      }\n    });\n    status.features.gateway = gatewayResponse.ok;\n  } catch (error) {\n    status.errors.push({ feature: 'gateway', error: error.message });\n  }\n\n  try {\n    // Test Sandbox API\n    const sandboxResponse = await fetch('https://api.vercel.com/v1/sandbox/health', {\n      headers: {\n        'Authorization': `Bearer ${process.env.VERCEL_TOKEN}`\n      }\n    });\n    status.features.sandbox = sandboxResponse.ok;\n  } catch (error) {\n    status.errors.push({ feature: 'sandbox', error: error.message });\n  }\n\n  try {\n    // Test Queues (Limited Beta)\n    const queueResponse = await fetch('https://api.vercel.com/v1/queues/health', {\n      headers: {\n        'Authorization': `Bearer ${process.env.VERCEL_TOKEN}`\n      }\n    });\n    status.features.queues = queueResponse.ok;\n  } catch (error) {\n    status.errors.push({ feature: 'queues', error: error.message });\n  }\n\n  return NextResponse.json(status);\n}\n```\n\n## 3. Chat Orchestrator\n\n**`/app/api/chat/route.ts`**\n```typescript\nimport { NextRequest, NextResponse } from 'next/server';\nimport { openai } from '@ai-sdk/openai';\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { google } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\nexport const runtime = 'edge';\n\ninterface ModelResponse {\n  model: string;\n  response: string;\n  latency: number;\n  tokens: {\n    prompt: number;\n    completion: number;\n    total: number;\n  };\n  cost: number;\n}\n\nconst MODELS = [\n  { \n    provider: 'openai', \n    model: 'gpt-4-turbo-preview',\n    costPer1kPrompt: 0.01,\n    costPer1kCompletion: 0.03\n  },\n  { \n    provider: 'anthropic', \n    model: 'claude-3-sonnet-20240229',\n    costPer1kPrompt: 0.003,\n    costPer1kCompletion: 0.015\n  },\n  { \n    provider: 'google', \n    model: 'gemini-pro',\n    costPer1kPrompt: 0.0005,\n    costPer1kCompletion: 0.0015\n  }\n];\n\nasync function callModel(provider: string, model: string, prompt: string): Promise<ModelResponse> {\n  const startTime = Date.now();\n  \n  try {\n    let result;\n    const gatewayUrl = process.env.VERCEL_AI_GATEWAY_URL;\n    \n    switch (provider) {\n      case 'openai':\n        result = await generateText({\n          model: openai(model, {\n            baseURL: gatewayUrl ? `${gatewayUrl}/openai` : undefined\n          }),\n          prompt,\n          maxTokens: 1000\n        });\n        break;\n      case 'anthropic':\n        result = await generateText({\n          model: anthropic(model, {\n            baseURL: gatewayUrl ? `${gatewayUrl}/anthropic` : undefined\n          }),\n          prompt,\n          maxTokens: 1000\n        });\n        break;\n      case 'google':\n        result = await generateText({\n          model: google(model, {\n            baseURL: gatewayUrl ? `${gatewayUrl}/google` : undefined\n          }),\n          prompt,\n          maxTokens: 1000\n        });\n        break;\n    }\n\n    const latency = Date.now() - startTime;\n    const modelConfig = MODELS.find(m => m.model === model);\n    \n    const promptTokens = result.usage?.promptTokens || 0;\n    const completionTokens = result.usage?.completionTokens || 0;\n    \n    const cost = (\n      (promptTokens / 1000) * modelConfig.costPer1kPrompt +\n      (completionTokens / 1000) * modelConfig.costPer1kCompletion\n    );\n\n    return {\n      model,\n      response: result.text,\n      latency,\n      tokens: {\n        prompt: promptTokens,\n        completion: completionTokens,\n        total: promptTokens + completionTokens\n      },\n      cost\n    };\n  } catch (error) {\n    throw new Error(`${model} failed: ${error.message}`);\n  }\n}\n\nexport async function POST(request: NextRequest) {\n  const { prompt } = await request.json();\n  \n  // Parallel execution\n  const responses = await Promise.allSettled(\n    MODELS.map(m => callModel(m.provider, m.model, prompt))\n  );\n\n  const results = responses\n    .filter((r): r is PromiseFulfilledResult<ModelResponse> => r.status === 'fulfilled')\n    .map(r => r.value);\n\n  // Log to queue or localStorage\n  await logCost(results);\n\n  return NextResponse.json({\n    results,\n    timestamp: new Date().toISOString()\n  });\n}\n\nasync function logCost(results: ModelResponse[]) {\n  const totalCost = results.reduce((sum, r) => sum + r.cost, 0);\n  \n  try {\n    // Try Vercel Queues first\n    const response = await fetch('/api/queue/cost', {\n      method: 'POST',\n      body: JSON.stringify({ results, totalCost })\n    });\n    \n    if (!response.ok) throw new Error('Queue unavailable');\n  } catch {\n    // Fallback to edge config or client-side localStorage\n    // This will be handled by the client\n  }\n}\n```\n\n## 4. Evaluation Engine\n\n**`/app/bench/evaluate.ts`**\n```typescript\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\nexport interface EvaluationScore {\n  relevance: number;  // 0-10\n  reasoning: number;  // 0-5\n  style: number;      // 0-5\n  totalScore: number;\n}\n\nexport async function evaluateResponse(\n  prompt: string,\n  response: string,\n  latency: number,\n  cost: number\n): Promise<EvaluationScore> {\n  const evaluationPrompt = `\n    Evaluate the following AI response based on:\n    1. Relevance (0-10): How well does it answer the prompt?\n    2. Reasoning (0-5): Quality of logic and structure\n    3. Style (0-5): Clarity and readability\n\n    Original Prompt: \"${prompt}\"\n    \n    AI Response: \"${response}\"\n    \n    Provide scores as JSON: {\"relevance\": n, \"reasoning\": n, \"style\": n}\n  `;\n\n  try {\n    const result = await generateText({\n      model: openai('gpt-4-turbo-preview'),\n      prompt: evaluationPrompt,\n      temperature: 0.1\n    });\n\n    const scores = JSON.parse(result.text);\n    \n    // Calculate total score with performance metrics\n    const totalScore = \n      scores.relevance * 2 + \n      scores.reasoning + \n      scores.style + \n      (-latency / 1000) + \n      (-cost * 10);\n\n    return {\n      ...scores,\n      totalScore\n    };\n  } catch (error) {\n    // Fallback scoring if evaluation fails\n    return {\n      relevance: 5,\n      reasoning: 3,\n      style: 3,\n      totalScore: 10 - (latency / 1000) - (cost * 10)\n    };\n  }\n}\n```\n\n## 5. UI Components\n\n**`/app/components/ResultsTable.tsx`**\n```typescript\n'use client';\n\nimport { useState, useEffect } from 'react';\nimport { evaluateResponse, EvaluationScore } from '../bench/evaluate';\n\ninterface Result {\n  model: string;\n  response: string;\n  latency: number;\n  tokens: {\n    prompt: number;\n    completion: number;\n    total: number;\n  };\n  cost: number;\n  evaluation?: EvaluationScore;\n}\n\ninterface ResultsTableProps {\n  results: Result[];\n  prompt: string;\n}\n\nexport default function ResultsTable({ results, prompt }: ResultsTableProps) {\n  const [evaluatedResults, setEvaluatedResults] = useState<Result[]>([]);\n  const [totalCost, setTotalCost] = useState(0);\n\n  useEffect(() => {\n    evaluateAllResponses();\n  }, [results]);\n\n  const evaluateAllResponses = async () => {\n    const evaluated = await Promise.all(\n      results.map(async (result) => {\n        const evaluation = await evaluateResponse(\n          prompt,\n          result.response,\n          result.latency,\n          result.cost\n        );\n        return { ...result, evaluation };\n      })\n    );\n\n    const sorted = evaluated.sort((a, b) => \n      (b.evaluation?.totalScore || 0) - (a.evaluation?.totalScore || 0)\n    );\n\n    setEvaluatedResults(sorted);\n    \n    // Update total cost\n    const cost = sorted.reduce((sum, r) => sum + r.cost, 0);\n    setTotalCost(prev => prev + cost);\n    \n    // Save to localStorage\n    const existingCost = parseFloat(localStorage.getItem('totalCost') || '0');\n    localStorage.setItem('totalCost', (existingCost + cost).toString());\n  };\n\n  const executeBestInSandbox = async () => {\n    const best = evaluatedResults[0];\n    if (!best) return;\n\n    try {\n      const response = await fetch('/api/sandbox/execute', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({\n          code: best.response,\n          model: best.model\n        })\n      });\n\n      const result = await response.json();\n      console.log('Sandbox execution:', result);\n    } catch (error) {\n      console.error('Sandbox execution failed:', error);\n    }\n  };\n\n  return (\n    <div className=\"w-full max-w-6xl mx-auto p-4\">\n      <div className=\"mb-4 text-right\">\n        <span className=\"text-lg font-semibold\">\n          Total Cost: ${totalCost.toFixed(4)}\n        </span>\n      </div>\n\n      <div className=\"overflow-x-auto\">\n        <table className=\"min-w-full bg-white border border-gray-200\">\n          <thead className=\"bg-gray-50\">\n            <tr>\n              <th className=\"px-4 py-2 border\">Rank</th>\n              <th className=\"px-4 py-2 border\">Model</th>\n              <th className=\"px-4 py-2 border\">Latency (ms)</th>\n              <th className=\"px-4 py-2 border\">Tokens</th>\n              <th className=\"px-4 py-2 border\">Cost ($)</th>\n              <th className=\"px-4 py-2 border\">Relevance</th>\n              <th className=\"px-4 py-2 border\">Reasoning</th>\n              <th className=\"px-4 py-2 border\">Style</th>\n              <th className=\"px-4 py-2 border\">Total Score</th>\n            </tr>\n          </thead>\n          <tbody>\n            {evaluatedResults.map((result, index) => (\n              <tr key={result.model} className={index === 0 ? 'bg-green-50' : ''}>\n                <td className=\"px-4 py-2 border text-center\">\n                  {index === 0 ? '🥇' : index === 1 ? '🥈' : '🥉'}\n                </td>\n                <td className=\"px-4 py-2 border font-medium\">{result.model}</td>\n                <td className=\"px-4 py-2 border text-center\">{result.latency}</td>\n                <td className=\"px-4 py-2 border text-center\">{result.tokens.total}</td>\n                <td className=\"px-4 py-2 border text-center\">{result.cost.toFixed(4)}</td>\n                <td className=\"px-4 py-2 border text-center\">\n                  {result.evaluation?.relevance || '-'}\n                </td>\n                <td className=\"px-4 py-2 border text-center\">\n                  {result.evaluation?.reasoning || '-'}\n                </td>\n                <td className=\"px-4 py-2 border text-center\">\n                  {result.evaluation?.style || '-'}\n                </td>\n                <td className=\"px-4 py-2 border text-center font-bold\">\n                  {result.evaluation?.totalScore.toFixed(2) || '-'}\n                </td>\n              </tr>\n            ))}\n          </tbody>\n        </table>\n      </div>\n\n      {evaluatedResults.length > 0 && (\n        <div className=\"mt-4\">\n          <button\n            onClick={executeBestInSandbox}\n            className=\"bg-blue-500 text-white px-4 py-2 rounded hover:bg-blue-600\"\n          >\n            Execute Winner in Sandbox\n          </button>\n        </div>\n      )}\n\n      <div className=\"mt-4\">\n        <h3 className=\"text-lg font-semibold mb-2\">Response Preview (Winner)</h3>\n        <div className=\"bg-gray-100 p-4 rounded\">\n          <pre className=\"whitespace-pre-wrap text-sm\">\n            {evaluatedResults[0]?.response || 'No results yet'}\n          </pre>\n        </div>\n      </div>\n    </div>\n  );\n}\n```\n\n## 6. Main Application Page\n\n**`/app/page.tsx`**\n```typescript\n'use client';\n\nimport { useState, useEffect } from 'react';\nimport ResultsTable from './components/ResultsTable';\n\nexport default function Home() {\n  const [prompt, setPrompt] = useState('');\n  const [results, setResults] = useState([]);\n  const [loading, setLoading] = useState(false);\n  const [featureStatus, setFeatureStatus] = useState(null);\n  const [totalSpent, setTotalSpent] = useState(0);\n\n  useEffect(() => {\n    checkFeatures();\n    loadTotalSpent();\n  }, []);\n\n  const checkFeatures = async () => {\n    const response = await fetch('/api/status');\n    const status = await response.json();\n    setFeatureStatus(status);\n  };\n\n  const loadTotalSpent = () => {\n    const spent = parseFloat(localStorage.getItem('totalCost') || '0');\n    setTotalSpent(spent);\n  };\n\n  const handleSubmit = async (",
      "latency": 64391,
      "cost": 0.31386,
      "scores": {
        "relevance": 8,
        "reasoning": 3,
        "style": 4,
        "explanation": "The response covers the key components of the assignment but is less detailed in certain sections compared to Model 1. Logical steps are mostly clear, and the style is good but could be more concise in some areas."
      },
      "finalScore": -44.5296
    },
    {
      "model": "google/gemini-2.5-pro",
      "text": "Excellent mission. This is a perfect use case for Vercel's latest Ship 2025 features. We'll build a robust benchmarking app that stays under budget while generating all the data needed for your LinkedIn article.\n\nHere is a complete implementation plan, including code for key files, that accomplishes your mission.\n\n### **Project Setup**\n\n1.  **Initialize Next.js 15 Project:**\n    ```bash\n    npx create-next-app@latest vercel-ship-benchmark --typescript --tailwind --eslint\n    cd vercel-ship-benchmark\n    ```\n2.  **Install Dependencies:**\n    ```bash\n    npm install @vercel/ai @ai-sdk/openai @ai-sdk/anthropic @ai-sdk/google @vercel/kv @vercel/queue @vercel/sandbox zod playwright\n    npm install -D @types/node\n    npx playwright install # Install browser binaries\n    ```\n3.  **Environment Variables (`.env.local`):**\n    You'll need API keys for the models you're benchmarking and a Vercel API token for metric export. Get them from OpenAI, Anthropic, and Google AI Studio.\n\n    ```ini\n    # Vercel AI Gateway - these point to the gateway\n    OPENAI_API_KEY=\"sk-...\"\n    ANTHROPIC_API_KEY=\"sk-...\"\n    GOOGLE_GENERATIVE_LANGUAGE_API_KEY=\"AIza...\"\n\n    # For Vercel API access (metrics export)\n    VERCEL_API_TOKEN=\"your_vercel_api_token\"\n    VERCEL_TEAM_ID=\"your_vercel_team_id\" # Optional, but good practice\n    VERCEL_PROJECT_ID=\"your_vercel_project_id\"\n\n    # For Vercel KV (used with Queues)\n    KV_URL=\"from_vercel_storage_dashboard\"\n    KV_REST_API_URL=\"from_vercel_storage_dashboard\"\n    KV_REST_API_TOKEN=\"from_vercel_storage_dashboard\"\n    KV_REST_API_READ_ONLY_TOKEN=\"from_vercel_storage_dashboard\"\n\n    # For Vercel Queues\n    QUEUE_URL=\"from_vercel_storage_dashboard\"\n    QUEUE_TOKEN=\"from_vercel_storage_dashboard\"\n    ```\n\n4.  **Configure Vercel AI Gateway:**\n    In your Vercel Project Settings, navigate to the AI Gateway tab. Create a new gateway and link your OpenAI, Anthropic, and Google provider keys. This is crucial for centralized logging, caching, and cost tracking.\n\n---\n\n### **1. Feature Availability Probe (`/app/api/status/route.ts`)**\n\nThis route will act as a health check for the Vercel features we plan to use. It will generate a `status.json` on demand.\n\n```typescript\n// /app/api/status/route.ts\nimport { kv } from '@vercel/kv';\nimport { queue } from '@/app/queue'; // We will create this file next\nimport { create as createSandbox } from '@vercel/sandbox';\nimport { NextResponse } from 'next/server';\n\nexport const dynamic = 'force-dynamic'; // Ensures this runs on every request\n\nexport async function GET() {\n  const status = {\n    gateway: { available: false, reason: 'Not checked' },\n    sandbox: { available: false, reason: 'Not checked' },\n    queues: { available: false, reason: 'Not checked' },\n    kv: { available: false, reason: 'Not checked' },\n    timestamp: new Date().toISOString(),\n  };\n\n  // 1. Gateway Ping (by making a tiny, cheap request)\n  try {\n    const response = await fetch('https://api.openai.com/v1/chat/completions', {\n        method: 'POST',\n        headers: {\n            'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,\n            'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({ model: 'gpt-4o-mini', messages: [{ role: 'user', content: 'ping' }], max_tokens: 1 }),\n    });\n    if (response.ok) {\n        status.gateway = { available: true, reason: 'Successfully received 200 OK from proxied model.' };\n    } else {\n        status.gateway = { available: false, reason: `Failed with status: ${response.status}` };\n    }\n  } catch (e: any) {\n    status.gateway = { available: false, reason: e.message };\n  }\n\n  // 2. Sandbox Ping\n  try {\n    const sandbox = await createSandbox({\n        code: 'console.log(\"pong\")',\n        runtime: 'nodejs18.x',\n    });\n    await sandbox.start();\n    const { output } = await sandbox.run();\n    if (output.stdout.includes('pong')) {\n        status.sandbox = { available: true, reason: 'Sandbox executed and returned expected output.' };\n    } else {\n        status.sandbox = { available: false, reason: 'Sandbox ran but output was unexpected.' };\n    }\n    await sandbox.close();\n  } catch (e: any) {\n    status.sandbox = { available: false, reason: e.message };\n  }\n\n  // 3. Queue & KV Probe\n  try {\n    await kv.set('status_probe', 'ok', { ex: 10 });\n    const kvResult = await kv.get('status_probe');\n    if (kvResult === 'ok') {\n        status.kv = { available: true, reason: 'Successfully set and get from Vercel KV.' };\n        // Now try the queue\n        const job = await queue.enqueue(\"status-probe\", { message: \"pong\" });\n        if(job.id) {\n            status.queues = { available: true, reason: 'Successfully enqueued a job.' };\n        } else {\n            status.queues = { available: false, reason: 'Failed to enqueue a job.' };\n        }\n    } else {\n        status.kv = { available: false, reason: 'KV set/get mismatch.' };\n        status.queues = { available: false, reason: 'KV is down, cannot test queues.' };\n    }\n  } catch (e: any) {\n    status.kv.reason = e.message;\n    status.queues.reason = e.message;\n  }\n\n  return NextResponse.json(status);\n}\n```\n\n---\n\n### **2 & 3. Chat Orchestrator & Model Evaluation (`/app/api/chat/route.ts` & `/app/bench/evaluate.ts`)**\n\nFirst, let's define the models and the evaluation logic.\n\n**`/app/bench/evaluate.ts`**\nThis file houses the \"judge\" logic, using a powerful model to score the responses from the benchmarked models.\n\n```typescript\n// /app/bench/evaluate.ts\nimport { generateObject } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\n// Define the structure of the judge's evaluation\nconst EvaluationSchema = z.object({\n  relevance: z.number().min(0).max(10).describe('How relevant is the answer to the user prompt? 10 is perfectly relevant.'),\n  reasoning: z.number().min(0).max(5).describe('Assess the quality of logical reasoning, if applicable. 5 is flawless.'),\n  style: z.number().min(0).max(5).describe('Evaluate the clarity, tone, and formatting. 5 is excellent.'),\n  critique: z.string().describe('A brief one-sentence critique of the response.'),\n});\n\n// The Judge LLM (use a powerful one for best results)\nconst judgeLlm = openai('gpt-4o');\n\nexport async function evaluateResponse(prompt: string, response: string) {\n  try {\n    const { object: evaluation } = await generateObject({\n      model: judgeLlm,\n      schema: EvaluationSchema,\n      prompt: `User Prompt: \"${prompt}\"\\n\\nModel Response: \"${response}\"\\n\\nAs a neutral AI evaluator, please score the model's response based on the provided schema. Be objective and strict.`,\n    });\n    return evaluation;\n  } catch (error) {\n    console.error('Evaluation failed:', error);\n    // Return a default/error state if evaluation fails\n    return { relevance: 0, reasoning: 0, style: 0, critique: 'Evaluation by judge model failed.' };\n  }\n}\n\n// The final scoring formula from the mission brief\nexport function calculateTotalScore(\n    scores: { relevance: number; reasoning: number; style: number; },\n    latency: number, // in ms\n    cost: number // in USD\n) {\n    const { relevance, reasoning, style } = scores;\n    const latencyPenalty = latency / 1000;\n    const costPenalty = cost * 10;\n    const total = (relevance * 2) + reasoning + style - latencyPenalty - costPenalty;\n    return parseFloat(total.toFixed(4));\n}\n```\n\n**`/app/api/chat/route.ts`**\nThis is the main orchestrator. It receives a prompt, queries three models in parallel, evaluates them, and returns the ranked results.\n\n```typescript\n// /app/api/chat/route.ts\nimport { CoreMessage, streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { google } from '@ai-sdk/google';\nimport { NextRequest, NextResponse } from 'next/server';\nimport { evaluateResponse, calculateTotalScore } from '@/app/bench/evaluate';\n\nexport const dynamic = 'force-dynamic';\n\n// Define the models to benchmark\nconst models = {\n  'gpt-4o-mini': openai('gpt-4o-mini'),\n  'claude-3-haiku-20240307': anthropic('claude-3-haiku-20240307'),\n  'gemini-1.5-flash-latest': google('gemini-1.5-flash-latest'),\n};\n\ntype ModelId = keyof typeof models;\n\nexport async function POST(req: NextRequest) {\n  try {\n    const { prompt } = await req.json();\n\n    if (!prompt) {\n      return NextResponse.json({ error: 'Prompt is required' }, { status: 400 });\n    }\n\n    const userMessage: CoreMessage = { role: 'user', content: prompt };\n    \n    // 2. Chat Orchestrator Flow: Parallel calls\n    const benchmarkPromises = Object.keys(models).map(async (modelId: string) => {\n      const model = models[modelId as ModelId];\n      const startTime = Date.now();\n\n      const { text, usage, finishReason, rawResponse } = await streamText({\n        model: model,\n        messages: [userMessage],\n        // Set a reasonable token limit to control costs for the benchmark\n        maxTokens: 1024, \n      });\n\n      const latency = Date.now() - startTime;\n      const cost = (usage.completionTokens / 1_000_000 * (rawResponse?.usage as any)?.output_cost_per_million_tokens || 0) + \n                   (usage.promptTokens / 1_000_000 * (rawResponse?.usage as any)?.input_cost_per_million_tokens || 0);\n\n      // 3. Model Evaluation\n      const evaluationScores = await evaluateResponse(prompt, text);\n      const totalScore = calculateTotalScore(evaluationScores, latency, cost);\n\n      return {\n        model: modelId,\n        response: text,\n        latency,\n        cost: parseFloat(cost.toFixed(6)),\n        tokens: usage.completionTokens,\n        evaluation: evaluationScores,\n        totalScore,\n      };\n    });\n\n    const results = await Promise.all(benchmarkPromises);\n\n    // Rank results by total score\n    const rankedResults = results.sort((a, b) => b.totalScore - a.totalScore);\n    \n    const totalCostOfRequest = rankedResults.reduce((acc, r) => acc + r.cost, 0);\n\n    // 5. Cost Logging (will implement queue handler next)\n    // We will call the queue here later\n\n    return NextResponse.json({ results: rankedResults, totalCostOfRequest });\n\n  } catch (error) {\n    console.error('Orchestration failed:', error);\n    return NextResponse.json({ error: 'An error occurred during benchmarking.' }, { status: 500 });\n  }\n}\n```\n\n---\n\n### **5. Cost Logging (`/app/queue/index.ts` & `/app/api/queues/log-cost/route.ts`)**\n\n**Queue Definition (`/app/queue/index.ts`)**\n\n```typescript\n// /app/queue/index.ts\nimport { Queue } from \"@vercel/queue\";\nimport { updateTotalCost } from \"./handlers\"; // This will be our handler function\n\nexport const queue = Queue(\n  \"log-cost\", // The name of our queue\n  async (payload: { cost: number }) => {\n    await updateTotalCost(payload.cost);\n  }\n);\n```\n\n**Queue Handler (`/app/queue/handlers.ts`)**\nThis logic runs serverlessly when a job is processed. It updates the total cost in Vercel KV.\n\n```typescript\n// /app/queue/handlers.ts\nimport { kv } from '@vercel/kv';\n\nconst TOTAL_COST_KEY = 'benchmark-total-cost';\n\nexport async function updateTotalCost(newCost: number) {\n  try {\n    // Atomically increment the total cost\n    await kv.incrbyfloat(TOTAL_COST_KEY, newCost);\n    console.log(`Successfully logged cost: $${newCost}`);\n  } catch (error) {\n    console.error('Failed to update total cost in KV:', error);\n    // Implement retry logic or dead-letter queue if needed for production\n  }\n}\n\nexport async function getTotalCost() {\n    const totalCost = await kv.get(TOTAL_COST_KEY);\n    return (totalCost || 0) as number;\n}\n```\n\n**Updating the Chat Orchestrator (`/app/api/chat/route.ts`)**\nNow, let's add the queue call to the orchestrator.\n\n```typescript\n// Add this import to /app/api/chat/route.ts\nimport { queue } from '@/app/queue';\n\n// ... inside the POST function, after calculating rankedResults and totalCostOfRequest\n// ...\n\n// 5. Cost Logging\ntry {\n  // Check if queues are available from our status check (or just try)\n  // For simplicity, we'll just fire-and-forget\n  if (process.env.QUEUE_URL) {\n    await queue.enqueue({ cost: totalCostOfRequest });\n  }\n} catch (e) {\n  console.warn(\"Queue not available, cost not logged server-side.\", e);\n}\n\n\nreturn NextResponse.json({ results: rankedResults, totalCostOfRequest });\n```\n\n---\n\n### **4. UI (`/app/page.tsx` and components)**\n\n**Main Page (`/app/page.tsx`)**\nThis is the user-facing interface.\n\n```tsx\n// /app/page.tsx\n'use client';\n\nimport { useState, useEffect, FormEvent } from 'react';\nimport ResultsTable from './components/ResultsTable';\nimport SandboxOutput from './components/SandboxOutput';\n\n// Define the type for our result objects\nexport type BenchmarkResult = {\n  model: string;\n  response: string;\n  latency: number;\n  cost: number;\n  tokens: number;\n  evaluation: {\n    relevance: number;\n    reasoning: number;\n    style: number;\n    critique: string;\n  };\n  totalScore: number;\n};\n\nexport default function HomePage() {\n  const [prompt, setPrompt] = useState('Write a simple Vercel Serverless Function in Node.js that returns \"Hello, World!\".');\n  const [results, setResults] = useState<BenchmarkResult[]>([]);\n  const [isLoading, setIsLoading] = useState(false);\n  const [totalCost, setTotalCost] = useState(0);\n  const [sandboxOutput, setSandboxOutput] = useState('');\n  const [isSandboxRunning, setIsSandboxRunning] = useState(false);\n\n  // Load initial cost from localStorage as a fallback/UI display\n  useEffect(() => {\n    const savedCost = localStorage.getItem('benchmarkTotalCost');\n    if (savedCost) {\n      setTotalCost(parseFloat(savedCost));\n    }\n  }, []);\n\n  const handleSubmit = async (e: FormEvent) => {\n    e.preventDefault();\n    setIsLoading(true);\n    setResults([]);\n    setSandboxOutput('');\n\n    const response = await fetch('/api/chat', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ prompt }),\n    });\n\n    setIsLoading(false);\n    if (response.ok) {\n      const data = await response.json();\n      setResults(data.results);\n      \n      // Update cumulative cost\n      const newTotalCost = totalCost + data.totalCostOfRequest;\n      setTotalCost(newTotalCost);\n      localStorage.setItem('benchmarkTotalCost', newTotalCost.toString());\n\n      // Execute the winner in the sandbox\n      const winner = data.results[0];\n      if (winner) {\n        executeInSandbox(winner.response);\n      }\n    } else {\n      console.error('Failed to fetch benchmark results');\n    }\n  };\n\n  const executeInSandbox = async (code: string) => {\n      setIsSandboxRunning(true);\n      // A simple regex to extract code from markdown code blocks\n      const codeBlockMatch = code.match(/```(?:\\w+\\n)?([\\s\\S]+)```/);\n      const codeToRun = codeBlockMatch ? codeBlockMatch[1] : code;\n\n      try {\n        const res = await fetch('/api/sandbox/execute', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ code: codeToRun }),\n        });\n        const result = await res.json();\n        setSandboxOutput(`Exit Code: ${result.exitCode}\\n\\nSTDOUT:\\n${result.stdout}\\n\\nSTDERR:\\n${result.stderr}`);\n      } catch(e) {\n        setSandboxOutput('Failed to execute code in sandbox.');\n      } finally {\n        setIsSandboxRunning(false);\n      }\n  };\n\n\n  return (\n    <main className=\"container mx-auto p-4 md:p-8 bg-gray-50 min-h-screen\">\n      <header className=\"text-center mb-8\">\n        <h1 className=\"text-4xl font-bold text-black\">Vercel Ship 2025 Feature Benchmark</h1>\n        <p className=\"text-gray-600 mt-2\">I spent $10 on Vercel's latest features—here's what I found.</p>\n      </header>\n      \n      <div className=\"bg-white p-6 rounded-lg shadow-md mb-8\">\n        <form onSubmit={handleSubmit}>\n          <label htmlFor=\"prompt\" className=\"block text-sm font-medium text-gray-700 mb-2\">SWE-bench Style Prompt:</label>\n          <textarea\n            id=\"prompt\"\n            value={prompt}\n            onChange={(e) => setPrompt(e.target.value)}\n            className=\"w-full h-24 p-2 border border-gray-300 rounded-md focus:ring-2 focus:ring-black\"\n            placeholder=\"e.g., Write a React component...\"\n          />\n          <div className=\"flex justify-between items-center mt-4\">\n            <button\n              type=\"submit\"\n              disabled={isLoading}\n              className=\"px-6 py-2 bg-black text-white font-semibold rounded-md hover:bg-gray-800 disabled:bg-gray-400\"\n            >\n              {isLoading ? 'Benchmarking...' : 'Run Benchmark'}\n            </button>\n            <div className=\"text-right\">\n                <span className=\"font-bold text-lg text-gray-800\">Cumulative Cost: ${totalCost.toFixed(6)}</span>\n                <p className=\"text-xs text-gray-500\">Tracked via Vercel Queues & KV (fallback: localStorage)</p>\n            </div>\n          </div>\n        </form>\n      </div>\n      \n      {results.length > 0 && <ResultsTable results={results} />}\n      \n      { (isSandboxRunning || sandboxOutput) && <SandboxOutput output={sandboxOutput} isLoading={isSandboxRunning} /> }\n\n    </main>\n  );\n}\n```\n\n**Results Table Component (`/app/components/ResultsTable.tsx`)**\n\n```tsx\n// /app/components/ResultsTable.tsx\nimport { BenchmarkResult } from '../page';\n\nexport default function ResultsTable({ results }: { results: BenchmarkResult[] }) {\n  return (\n    <div className=\"bg-white p-6 rounded-lg shadow-md mb-8 overflow-x-auto\">\n      <h2 className=\"text-2xl font-bold mb-4\">Benchmark Results</h2>\n      <table className=\"w-full text-left border-collapse\">\n        <thead>\n          <tr className=\"border-b\">\n            <th className=\"p-2\">Rank</th>\n            <th className=\"p-2\">Model</th>\n            <th className=\"p-2\">Total Score</th>\n            <th className=\"p-2\">Relevance</th>\n            <th className=\"p-2\">Reasoning</th>\n            <th className=\"p-2\">Style</th>\n            <th className=\"p-2\">Latency (ms)</th>\n            <th className=\"p-2\">Cost ($)</th>\n            <th className=\"p-2\">Critique</th>\n          </tr>\n        </thead>\n        <tbody>\n          {results.map((r, index) => (\n            <tr key={r.model} className={`border-b ${index === 0 ? 'bg-green-50' : ''}`}>\n              <td className=\"p-2 font-bold\">{index + 1} {index === 0 && '🏆'}</td>\n              <td className=\"p-2 font-semibold\">{r.model}</td>\n              <td className=\"p-2 font-bold text-blue-600\">{r.totalScore}</td>\n              <td className=\"p-2\">{r.evaluation.relevance}/10</td>\n              <td className=\"p-2\">{r.evaluation.reasoning}/5</td>\n              <td className=\"p-2\">{r.evaluation.style}/5</td>\n              <td className=\"p-2\">{r.latency}</td>\n              <td className=\"p-2\">{r.cost.toFixed(6)}</td>\n              <td className=\"p-2 text-sm text-gray-600\">{r.evaluation.critique}</td>\n            </tr>\n          ))}\n        </tbody>\n      </table>\n    </div>\n  );\n}\n```\n\n**Sandbox Component (`/app/components/SandboxOutput.tsx` & `/app/api/sandbox/execute/route.ts`)**\n\n```typescript\n// /app/api/sandbox/execute/route.ts\nimport { create as createSandbox } from '@vercel/sandbox';\nimport { NextRequest, NextResponse } from 'next/server';\n\nexport async function POST(req: NextRequest) {\n    const { code } = await req.json();\n    try {\n        const sandbox = await createSandbox({\n            code: code,\n            runtime: 'nodejs18.x',\n        });\n        await sandbox.start();\n        const { exitCode, stdout, stderr } = await sandbox.run();\n        await sandbox.close();\n        return NextResponse.json({ exitCode, stdout, stderr });\n    } catch (e: any) {\n        return NextResponse.json({ error: e.message }, { status: 500 });\n    }\n}\n```\n\n```tsx\n// /app/components/SandboxOutput.tsx\nexport default function SandboxOutput({ output, isLoading }: { output: string; isLoading: boolean }) {\n  return (\n    <div className=\"bg-gray-900 text-white p-6 rounded-lg shadow-md font-mono\">\n      <h3 className=\"text-xl font-bold mb-4 text-gray-300\">Winner's Code Execution (Vercel Sandbox)</h3>\n      {isLoading ? (\n        <p className=\"text-yellow-400\">Executing code...</p>\n      ) : (\n        <pre className=\"whitespace-pre-wrap text-sm\">{output}</pre>\n      )}\n    </div>\n  );\n}\n```\n\n---\n\n### **6. Screenshots & Data Export**\n\n**Playwright Automation (`/tests/e2e.spec.ts`)**\nCreate this file to automate capturing the final UI state.\n\n```typescript\n// /tests/e2e.spec.ts\nimport { test, expect } from '@playwright/test';\n\ntest('run benchmark and capture screenshot', async ({ page }) => {\n  await page.goto('/');\n\n  // Wait for the page to be ready\n  await expect(page.getByRole('heading', { name: 'Vercel Ship 2025 Feature Benchmark' })).toBeVisible();\n\n  // Click the benchmark button\n  await page.getByRole('button', { name: 'Run Benchmark' }).click();\n\n  // Wait for the results table to appear, with a generous timeout\n  await expect(page.getByRole('heading', { name: 'Benchmark Results' })).toBeVisible({ timeout: 90000 });\n\n  // Wait for the sandbox output to appear\n  await expect(page.getByRole('heading', { name: \"Winner's Code Execution (Vercel Sandbox)\" })).toBeVisible({ timeout: 30000 });\n\n  // Take a full-page screenshot for the article\n  await page.screenshot({ path: 'vercel-benchmark-results.png', fullPage: true });\n});\n```\nRun it from your terminal: `npx playwright test`\n\n**Data Export (`/app/api/export/route.ts`)**\nThis route uses the Vercel API to pull the raw AI Gateway logs.\n\n```typescript\n// /app/api/export/route.ts\nimport { NextRequest, NextResponse } from 'next/server';\n\nexport async function GET(req: NextRequest) {\n    const { VERCEL_API_TOKEN, VERCEL_PROJECT_ID, VERCEL_TEAM_ID } = process.env;\n\n    if (!VERCEL_API_TOKEN || !VERCEL_PROJECT_ID) {\n        return NextResponse.json({ error: 'Missing Vercel API credentials' }, { status: 500 });\n    }\n\n    const teamIdParam = VERCEL_TEAM_ID ? `?teamId=${VERCEL_TEAM_ID}` : '';\n    // Fetch last 100 requests for this project from the AI Gateway logs\n    const url = `https://api.vercel.com/v1/analytics/request-logs${teamIdParam}&projectId=${VERCEL_PROJECT_ID}&limit=100`;\n\n    try {\n        const response = await fetch(url, {\n            headers: { Authorization: `Bearer ${VERCEL_API_TOKEN}` },\n        });\n\n        if (!response.ok) {\n            const error = await response.json();\n            return NextResponse.json({ error: 'Failed to fetch from Vercel API', details: error }, { status: response.status });\n        }\n\n        const { logs } = await response.json();\n\n        // Convert JSON to CSV\n        const headers = ['id', 'timestamp', 'path', 'model', 'statusCode', 'promptTokens', 'completionTokens', 'latency', 'cacheStatus'];\n        const csvRows = [\n            headers.join(','),\n            ...logs.map((log: any) => [\n                log.id,\n                new Date(log.timestamp).toISOString(),\n                log.path,\n                log.proxy.model,\n                log.proxy.statusCode,\n                log.proxy.usage?.promptTokens || 0,\n                log.proxy.usage?.completionTokens || 0,\n                log.proxy.latency,\n                log.proxy.cache,\n            ].join(',')),\n        ];\n\n        return new NextResponse(csvRows.join('\\n'), {\n            status: 200,\n            headers: {\n                'Content-Type': 'text/csv',\n                'Content-Disposition': 'attachment; filename=\"gateway-metrics.csv\"',\n            },\n        });\n    } catch (error: any) {\n        return NextResponse.json({ error: error.message }, { status: 500 });\n    }\n}\n```\nYou can now visit `/api/export` in your browser to download the CSV.\n\n### **Final Checklist for Your LinkedIn Article**\n\n*   **Deploy to Vercel:** Push your code to a GitHub repository and link it to your Vercel Pro account.\n*   **Run Benchmarks:** Use the UI to run a few different SWE-bench style prompts.\n*   **Capture Screenshots:** Run `npx playwright test`. You'll have `vercel-benchmark-results.png`.\n*   **Export Metrics:** Navigate to `your-deployment-url/api/export` to get `gateway-metrics.csv`.\n*   **Check Costs:** Go to your Vercel AI Gateway dashboard to see the real-time cost and usage stats. This will be a powerful visual for your article.\n*   **Check Queues & KV:** Inspect your Vercel Storage dashboard to see the queue jobs being processed and the total cost value in KV.\n\nYou have successfully weaponized Vercel's Ship 2025 features to create a data-rich, cost-controlled benchmark. The total cost for this entire operation should be well under your $10 budget, likely in the range of $0.50 - $2.00, depending on the complexity of your prompts.",
      "latency": 95842,
      "cost": 0.026594999999999997,
      "scores": {
        "relevance": 7,
        "reasoning": 3,
        "style": 4,
        "explanation": "The response includes a comprehensive structure and touches on all necessary points but lacks depth in some implementation details. The reasoning is generally logical, with room for improvement in clarity of steps. The style is clear but somewhat lengthy."
      },
      "finalScore": -75.10795
    }
  ],
  "summary": {
    "winner": "xai/grok-3",
    "totalCost": 0.37773999999999996
  }
}