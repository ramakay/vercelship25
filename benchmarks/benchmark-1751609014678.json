{
  "timestamp": "2025-07-04T06:03:34.677Z",
  "prompt": "Design a Next.js 15 app using Vercel's AI Gateway to compare 3 AI models (Grok, Claude, Gemini) with a $10 budget. Include:\n\n1. AI Gateway integration to call all 3 models in parallel\n2. A judge component that scores responses on relevance, reasoning, and style\n3. Cost tracking to stay under $10 total\n4. Results dashboard showing model rankings and costs\n\nUse Vercel's latest features: AI Gateway (Beta), Active CPU billing, and optionally Sandbox/Queues if available.",
  "evaluations": [
    {
      "model": "xai/grok-3",
      "response": "I'll design a Next.js 15 app that compares three AI models using Vercel's AI Gateway, with cost tracking and a scoring system. The app will use modern Next.js features, Tailwind CSS for styling, and Vercel's latest infrastructure offerings. I'll break this down into components and explain the implementation choices.\n\n### Project Structure & Assumptions\n- **Next.js 15**: Using App Router with React Server Components (RSC) for efficient rendering.\n- **Vercel AI Gateway (Beta)**: For unified API access to Grok (xAI), Claude (Anthropic), and Gemini (Google).\n- **Cost Tracking**: Simple in-memory tracking with a persistent store (e.g., Vercel KV) to monitor the $10 budget.\n- **Judge Component**: A client-side scoring logic for relevance, reasoning, and style (mocked for simplicity; could be enhanced with another AI model).\n- **Dashboard**: Displays rankings and costs using shadcn/ui components.\n- **Active CPU Billing**: Leveraged via Vercel's deployment model for cost efficiency.\n- **Sandbox/Queues**: Not explicitly used as they're optional and not critical for this MVP, but noted for scalability.\n\n### Step 1: Setup and Dependencies\nCreate a new Next.js 15 project and install necessary dependencies:\n```bash\nnpx create-next-app@latest ai-model-comparator --typescript --app --tailwind\n# Select App Router, include Tailwind\nnpm install @vercel/ai-gateway @shadcn/ui recharts\n```\n\nConfigure environment variables in `.env.local` and Vercel dashboard:\n```env\nVERCEL_AI_API_KEY=your_vercel_ai_gateway_key\n```\n\n### Step 2: Core Implementation\n#### 1. AI Gateway Integration (`app/api/ai/route.ts`)\nThis API route handles parallel calls to the three models using Vercel's AI Gateway.\n```typescript\n// app/api/ai/route.ts\nimport { createClient } from \"@vercel/ai-gateway\";\nimport { NextResponse } from \"next/server\";\n\nconst aiClient = createClient({\n  apiKey: process.env.VERCEL_AI_API_KEY,\n});\n\nexport async function POST(req: Request) {\n  try {\n    const { prompt } = await req.json();\n    if (!prompt) return NextResponse.json({ error: \"Prompt required\" }, { status: 400 });\n\n    // Parallel calls to Grok, Claude, and Gemini\n    const [grokRes, claudeRes, geminiRes] = await Promise.all([\n      aiClient.chat.completions.create({\n        model: \"grok-1\",\n        messages: [{ role: \"user\", content: prompt }],\n      }),\n      aiClient.chat.completions.create({\n        model: \"claude-3-opus-20240229\",\n        messages: [{ role: \"user\", content: prompt }],\n      }),\n      aiClient.chat.completions.create({\n        model: \"gemini-1.5-pro\",\n        messages: [{ role: \"user\", content: prompt }],\n      }),\n    ]);\n\n    // Mock cost calculation (replace with actual API cost tracking if available)\n    const costs = {\n      grok: 0.0005, // $0.0005 per request (hypothetical)\n      claude: 0.001, // $0.001 per request\n      gemini: 0.0007, // $0.0007 per request\n    };\n\n    return NextResponse.json({\n      responses: {\n        grok: grokRes.choices[0].message.content,\n        claude: claudeRes.choices[0].message.content,\n        gemini: geminiRes.choices[0].message.content,\n      },\n      costs,\n    });\n  } catch (error) {\n    console.error(error);\n    return NextResponse.json({ error: \"AI Gateway error\" }, { status: 500 });\n  }\n}\n```\n\n#### 2. Cost Tracking (`lib/costTracker.ts`)\nTrack costs in-memory and persist to Vercel KV to stay under $10.\n```typescript\n// lib/costTracker.ts\nimport { kv } from \"@vercel/kv\";\n\nconst BUDGET_LIMIT = 10; // $10 limit\n\nexport async function updateCost(newCosts: { grok: number; claude: number; gemini: number }) {\n  const current = await kv.get(\"total_cost\") as { grok: number; claude: number; gemini: number } || { grok: 0, claude: 0, gemini: 0 };\n  const updated = {\n    grok: current.grok + newCosts.grok,\n    claude: current.claude + newCosts.claude,\n    gemini: current.gemini + newCosts.gemini,\n  };\n  const total = Object.values(updated).reduce((a, b) => a + b, 0);\n  \n  if (total > BUDGET_LIMIT) {\n    throw new Error(\"Budget limit of $10 exceeded\");\n  }\n\n  await kv.set(\"total_cost\", updated);\n  return updated;\n}\n\nexport async function getTotalCost() {\n  return (await kv.get(\"total_cost\")) || { grok: 0, claude: 0, gemini: 0 };\n}\n```\n\n#### 3. Judge Component (`components/Judge.tsx`)\nA client-side component to score responses based on relevance, reasoning, and style.\n```typescript\n// components/Judge.tsx\n\"use client\";\n\nimport { useState } from \"react\";\nimport { Button } from \"@/components/ui/button\";\n\ntype ResponseSet = { grok: string; claude: string; gemini: string };\ntype Scores = { model: string; relevance: number; reasoning: number; style: number; total: number }[];\n\nexport function Judge({ responses }: { responses: ResponseSet }) {\n  const [scores, setScores] = useState<Scores>([]);\n\n  const calculateScores = () => {\n    // Mock scoring logic (1-10 scale); in production, use AI or detailed heuristics\n    const newScores: Scores = [\n      { model: \"Grok\", relevance: mockScore(responses.grok, \"relevance\"), reasoning: mockScore(responses.grok, \"reasoning\"), style: mockScore(responses.grok, \"style\"), total: 0 },\n      { model: \"Claude\", relevance: mockScore(responses.claude, \"relevance\"), reasoning: mockScore(responses.claude, \"reasoning\"), style: mockScore(responses.claude, \"style\"), total: 0 },\n      { model: \"Gemini\", relevance: mockScore(responses.gemini, \"relevance\"), reasoning: mockScore(responses.gemini, \"reasoning\"), style: mockScore(responses.gemini, \"style\"), total: 0 },\n    ].map(s => ({ ...s, total: s.relevance + s.reasoning + s.style }));\n    setScores(newScores.sort((a, b) => b.total - a.total));\n  };\n\n  const mockScore = (text: string, criterion: string) => {\n    // Simple length-based scoring for demo\n    return Math.min(10, Math.max(1, Math.floor(text.length / (criterion === \"style\" ? 50 : 30))));\n  };\n\n  return (\n    <div className=\"mt-6\">\n      <Button onClick={calculateScores} className=\"mb-4\">Score Responses</Button>\n      {scores.length > 0 && (\n        <div className=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n          {scores.map((s) => (\n            <div key={s.model} className=\"p-4 border rounded shadow\">\n              <h3 className=\"font-bold\">{s.model}</h3>\n              <p>Relevance: {s.relevance}/10</p>\n              <p>Reasoning: {s.reasoning}/10</p>\n              <p>Style: {s.style}/10</p>\n              <p>Total: {s.total}/30</p>\n            </div>\n          ))}\n        </div>\n      )}\n    </div>\n  );\n}\n```\n\n#### 4. Results Dashboard (`app/page.tsx`)\nMain page with input form, responses, judge scores, and cost tracking.\n```typescript\n// app/page.tsx\nimport { Judge } from \"@/components/Judge\";\nimport { getTotalCost, updateCost } from \"@/lib/costTracker\";\nimport { Button } from \"@/components/ui/button\";\nimport { Input } from \"@/components/ui/input\";\nimport { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip, Legend, ResponsiveContainer } from \"recharts\";\n\nexport default async function Home() {\n  const totalCost = await getTotalCost();\n\n  // Server-side rendering for initial state\n  return (\n    <div className=\"container mx-auto p-6\">\n      <h1 className=\"text-3xl font-bold mb-6\">AI Model Comparator</h1>\n      \n      {/* Prompt Form */}\n      <form action=\"/api/ai\" method=\"POST\" className=\"mb-8\">\n        <Input name=\"prompt\" placeholder=\"Enter a prompt to compare AI models\" className=\"mb-4\" />\n        <Button type=\"submit\">Compare Models</Button>\n      </form>\n\n      {/* Cost Tracker */}\n      <div className=\"mb-8\">\n        <h2 className=\"text-xl font-semibold\">Cost Usage ($10 Budget)</h2>\n        <p>Total Spent: ${(Object.values(totalCost) as number[]).reduce((a, b) => a + b, 0).toFixed(4)}</p>\n        <p>Grok: ${totalCost.grok.toFixed(4)}</p>\n        <p>Claude: ${totalCost.claude.toFixed(4)}</p>\n        <p>Gemini: ${totalCost.gemini.toFixed(4)}</p>\n      </div>\n\n      {/* Results and Judge Component (Client-side after API fetch) */}\n      {/* Note: Dynamic client-side handling would be added here for responses */}\n      {/* For simplicity, assume responses are passed after API call */}\n      \n      {/* Mock Dashboard Chart for Costs */}\n      <div className=\"h-64 w-full\">\n        <ResponsiveContainer>\n          <LineChart data={[{ name: \"Costs\", grok: totalCost.grok, claude: totalCost.claude, gemini: totalCost.gemini }]}>\n            <CartesianGrid strokeDasharray=\"3 3\" />\n            <XAxis dataKey=\"name\" />\n            <YAxis />\n            <Tooltip />\n            <Legend />\n            <Line type=\"monotone\" dataKey=\"grok\" stroke=\"#8884d8\" />\n            <Line type=\"monotone\" dataKey=\"claude\" stroke=\"#82ca9d\" />\n            <Line type=\"monotone\" dataKey=\"gemini\" stroke=\"#ffc658\" />\n          </LineChart>\n        </ResponsiveContainer>\n      </div>\n    </div>\n  );\n}\n```\n\n### Step 3: Vercel Features Integration\n- **AI Gateway (Beta)**: Used in `api/ai/route.ts` for unified access to models, reducing integration complexity.\n- **Active CPU Billing**: Automatically leveraged by deploying to Vercel, ensuring costs scale with usage (minimal for this small app).\n- **Vercel KV**: Used for persistent cost tracking across requests.\n- **Sandbox/Queues (Optional)**: Could be added for background processing of scoring or rate-limiting API calls if budget constraints tighten, but omitted for MVP.\n\n### Step 4: Deployment\nDeploy to Vercel with:\n```bash\nvercel\n```\nEnsure environment variables are set in the Vercel dashboard. Monitor costs via Vercel's usage dashboard alongside the app's internal tracker.\n\n### Design Choices & Trade-offs\n- **Parallel API Calls**: Using `Promise.all` ensures fast response times but increases cost per request. Sequential calls could save money but slow down UX.\n- **Cost Tracking**: Mock costs are used since exact pricing depends on token usage. In production, integrate actual cost data from API responses or Vercel logs.\n- **Judge Scoring**: Simplified to a length-based heuristic for demo. A real implementation could use another AI model (e.g., GPT-4) for scoring, increasing costs.\n- **Budget Limit**: Hard-coded at $10 with a simple check. Could be enhanced with alerts or graceful degradation (e.g., switching to cheaper models near limit).\n- **UI**: Tailwind and shadcn/ui provide a clean, responsive design. Recharts adds a visual cost dashboard.\n\n### Future Enhancements\n- Add real-time streaming of AI responses using Vercel AI SDK's streaming capabilities.\n- Implement Sandbox for isolated testing of scoring logic.\n- Use Queues for throttling API calls if budget nears limit.\n- Enhance judge logic with a secondary AI model for accurate scoring.\n\nThis implementation stays within the $10 budget (assuming low usage) by tracking costs and limiting requests, while providing a functional comparison of AI models with Vercel's latest tools.",
      "scores": {
        "relevance": 5,
        "reasoning": 2.5,
        "style": 2.5,
        "accuracy": 5,
        "honesty": 2.5,
        "explanation": "Evaluation failed",
        "totalScore": 25,
        "soundnessScore": 5
      },
      "latency": 34767,
      "cost": 0.040965,
      "finalScore": -10.176650000000002
    },
    {
      "model": "anthropic/claude-4-opus",
      "response": "I'll design a Next.js 15 app that uses Vercel's AI Gateway to compare AI models with budget tracking. Here's the complete implementation:\n\n## Project Structure\n\n```\nai-model-comparison/\n├── app/\n│   ├── api/\n│   │   ├── compare/\n│   │   │   └── route.ts\n│   │   └── judge/\n│   │       └── route.ts\n│   ├── components/\n│   │   ├── ModelComparison.tsx\n│   │   ├── JudgePanel.tsx\n│   │   ├── CostTracker.tsx\n│   │   └── ResultsDashboard.tsx\n│   ├── lib/\n│   │   ├── ai-gateway.ts\n│   │   ├── cost-calculator.ts\n│   │   └── db.ts\n│   ├── layout.tsx\n│   └── page.tsx\n├── package.json\n├── next.config.js\n└── .env.local\n```\n\n## 1. Package.json\n\n```json\n{\n  \"name\": \"ai-model-comparison\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\"\n  },\n  \"dependencies\": {\n    \"@vercel/ai\": \"^3.0.0\",\n    \"@vercel/kv\": \"^1.0.0\",\n    \"ai\": \"^3.0.0\",\n    \"next\": \"15.0.0\",\n    \"react\": \"^18.3.1\",\n    \"react-dom\": \"^18.3.1\",\n    \"recharts\": \"^2.12.0\",\n    \"zod\": \"^3.22.0\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20\",\n    \"@types/react\": \"^18\",\n    \"@types/react-dom\": \"^18\",\n    \"autoprefixer\": \"^10.0.0\",\n    \"postcss\": \"^8\",\n    \"tailwindcss\": \"^3.4.0\",\n    \"typescript\": \"^5\"\n  }\n}\n```\n\n## 2. Environment Variables (.env.local)\n\n```env\n# AI Gateway Configuration\nVERCEL_AI_GATEWAY_URL=https://gateway.vercel.sh\nANTHROPIC_API_KEY=your_claude_key\nGOOGLE_GENERATIVE_AI_API_KEY=your_gemini_key\nXAI_API_KEY=your_grok_key\n\n# Vercel KV for cost tracking\nKV_URL=your_kv_url\nKV_REST_API_URL=your_kv_rest_api_url\nKV_REST_API_TOKEN=your_kv_rest_api_token\nKV_REST_API_READ_ONLY_TOKEN=your_kv_read_only_token\n```\n\n## 3. AI Gateway Configuration (lib/ai-gateway.ts)\n\n```typescript\nimport { createGateway } from '@vercel/ai';\n\n// Model configurations with cost per 1K tokens\nexport const MODEL_CONFIGS = {\n  grok: {\n    name: 'Grok',\n    model: 'grok-beta',\n    endpoint: 'https://api.x.ai/v1',\n    costPer1kInput: 0.01,\n    costPer1kOutput: 0.03,\n  },\n  claude: {\n    name: 'Claude',\n    model: 'claude-3-sonnet-20240229',\n    endpoint: 'https://api.anthropic.com/v1',\n    costPer1kInput: 0.003,\n    costPer1kOutput: 0.015,\n  },\n  gemini: {\n    name: 'Gemini',\n    model: 'gemini-pro',\n    endpoint: 'https://generativelanguage.googleapis.com/v1beta',\n    costPer1kInput: 0.00025,\n    costPer1kOutput: 0.0005,\n  },\n};\n\n// Initialize AI Gateway\nexport const aiGateway = createGateway({\n  providers: [\n    {\n      id: 'grok',\n      type: 'openai-compatible',\n      baseURL: MODEL_CONFIGS.grok.endpoint,\n      headers: {\n        'Authorization': `Bearer ${process.env.XAI_API_KEY}`,\n      },\n    },\n    {\n      id: 'claude',\n      type: 'anthropic',\n      baseURL: MODEL_CONFIGS.claude.endpoint,\n      headers: {\n        'x-api-key': process.env.ANTHROPIC_API_KEY!,\n      },\n    },\n    {\n      id: 'gemini',\n      type: 'google',\n      baseURL: MODEL_CONFIGS.gemini.endpoint,\n      apiKey: process.env.GOOGLE_GENERATIVE_AI_API_KEY!,\n    },\n  ],\n  cache: {\n    enabled: true,\n    ttl: 3600, // 1 hour cache\n  },\n  rateLimit: {\n    requests: 100,\n    window: '1m',\n  },\n});\n```\n\n## 4. Cost Calculator (lib/cost-calculator.ts)\n\n```typescript\nimport { kv } from '@vercel/kv';\nimport { MODEL_CONFIGS } from './ai-gateway';\n\ninterface Usage {\n  inputTokens: number;\n  outputTokens: number;\n  modelId: string;\n}\n\nexport class CostTracker {\n  private static BUDGET_LIMIT = 10.00; // $10 budget\n  private static COST_KEY = 'ai_comparison_costs';\n\n  static async recordUsage(usage: Usage): Promise<void> {\n    const config = MODEL_CONFIGS[usage.modelId as keyof typeof MODEL_CONFIGS];\n    const cost = this.calculateCost(usage, config);\n    \n    const currentTotal = await this.getTotalSpent();\n    if (currentTotal + cost > this.BUDGET_LIMIT) {\n      throw new Error('Budget limit exceeded');\n    }\n\n    await kv.hincrby(this.COST_KEY, 'total', cost * 100); // Store in cents\n    await kv.hincrby(this.COST_KEY, usage.modelId, cost * 100);\n    await kv.hincrby(this.COST_KEY, `${usage.modelId}_requests`, 1);\n  }\n\n  static calculateCost(usage: Usage, config: any): number {\n    const inputCost = (usage.inputTokens / 1000) * config.costPer1kInput;\n    const outputCost = (usage.outputTokens / 1000) * config.costPer1kOutput;\n    return inputCost + outputCost;\n  }\n\n  static async getTotalSpent(): Promise<number> {\n    const total = await kv.hget(this.COST_KEY, 'total') || 0;\n    return Number(total) / 100; // Convert from cents\n  }\n\n  static async getRemainingBudget(): Promise<number> {\n    const spent = await this.getTotalSpent();\n    return Math.max(0, this.BUDGET_LIMIT - spent);\n  }\n\n  static async getModelCosts(): Promise<Record<string, number>> {\n    const costs: Record<string, number> = {};\n    for (const modelId of Object.keys(MODEL_CONFIGS)) {\n      const cost = await kv.hget(this.COST_KEY, modelId) || 0;\n      costs[modelId] = Number(cost) / 100;\n    }\n    return costs;\n  }\n}\n```\n\n## 5. API Route for Model Comparison (app/api/compare/route.ts)\n\n```typescript\nimport { NextRequest, NextResponse } from 'next/server';\nimport { aiGateway, MODEL_CONFIGS } from '@/app/lib/ai-gateway';\nimport { CostTracker } from '@/app/lib/cost-calculator';\nimport { z } from 'zod';\n\nconst CompareRequestSchema = z.object({\n  prompt: z.string().min(1).max(1000),\n});\n\nexport async function POST(req: NextRequest) {\n  try {\n    const body = await req.json();\n    const { prompt } = CompareRequestSchema.parse(body);\n\n    // Check budget before making requests\n    const remainingBudget = await CostTracker.getRemainingBudget();\n    if (remainingBudget <= 0) {\n      return NextResponse.json(\n        { error: 'Budget limit exceeded' },\n        { status: 402 }\n      );\n    }\n\n    // Call all models in parallel using AI Gateway\n    const modelPromises = Object.entries(MODEL_CONFIGS).map(async ([id, config]) => {\n      try {\n        const startTime = Date.now();\n        \n        const response = await aiGateway.chat.completions.create({\n          model: config.model,\n          messages: [{ role: 'user', content: prompt }],\n          max_tokens: 500,\n          temperature: 0.7,\n          provider: id,\n        });\n\n        const endTime = Date.now();\n        \n        // Record usage for cost tracking\n        await CostTracker.recordUsage({\n          modelId: id,\n          inputTokens: response.usage?.prompt_tokens || 0,\n          outputTokens: response.usage?.completion_tokens || 0,\n        });\n\n        return {\n          modelId: id,\n          modelName: config.name,\n          response: response.choices[0]?.message?.content || '',\n          latency: endTime - startTime,\n          tokens: {\n            input: response.usage?.prompt_tokens || 0,\n            output: response.usage?.completion_tokens || 0,\n          },\n        };\n      } catch (error) {\n        console.error(`Error with ${config.name}:`, error);\n        return {\n          modelId: id,\n          modelName: config.name,\n          response: '',\n          error: error instanceof Error ? error.message : 'Unknown error',\n          latency: 0,\n          tokens: { input: 0, output: 0 },\n        };\n      }\n    });\n\n    const responses = await Promise.all(modelPromises);\n    \n    // Get updated costs\n    const modelCosts = await CostTracker.getModelCosts();\n    const totalSpent = await CostTracker.getTotalSpent();\n\n    return NextResponse.json({\n      responses,\n      costs: {\n        models: modelCosts,\n        total: totalSpent,\n        remaining: remainingBudget - totalSpent,\n      },\n    });\n  } catch (error) {\n    console.error('Comparison error:', error);\n    return NextResponse.json(\n      { error: 'Failed to compare models' },\n      { status: 500 }\n    );\n  }\n}\n```\n\n## 6. Judge API Route (app/api/judge/route.ts)\n\n```typescript\nimport { NextRequest, NextResponse } from 'next/server';\nimport { aiGateway } from '@/app/lib/ai-gateway';\nimport { z } from 'zod';\n\nconst JudgeRequestSchema = z.object({\n  prompt: z.string(),\n  responses: z.array(z.object({\n    modelId: z.string(),\n    modelName: z.string(),\n    response: z.string(),\n  })),\n});\n\ninterface JudgeScore {\n  relevance: number;\n  reasoning: number;\n  style: number;\n  overall: number;\n  feedback: string;\n}\n\nexport async function POST(req: NextRequest) {\n  try {\n    const body = await req.json();\n    const { prompt, responses } = JudgeRequestSchema.parse(body);\n\n    // Use Claude as the judge (most balanced for evaluation)\n    const judgePrompt = `\nYou are an AI response evaluator. Score each response on a scale of 0-10 for:\n- Relevance: How well it addresses the prompt\n- Reasoning: Logic and coherence\n- Style: Clarity and engagement\n\nOriginal prompt: \"${prompt}\"\n\nResponses to evaluate:\n${responses.map((r, i) => `\nResponse ${i + 1} (${r.modelName}):\n${r.response}\n`).join('\\n')}\n\nProvide scores in JSON format:\n{\n  \"scores\": [\n    {\n      \"modelId\": \"model_id\",\n      \"relevance\": 0-10,\n      \"reasoning\": 0-10,\n      \"style\": 0-10,\n      \"overall\": 0-10,\n      \"feedback\": \"brief explanation\"\n    }\n  ]\n}\n`;\n\n    const judgeResponse = await aiGateway.chat.completions.create({\n      model: 'claude-3-sonnet-20240229',\n      messages: [{ role: 'user', content: judgePrompt }],\n      temperature: 0.3,\n      provider: 'claude',\n    });\n\n    const judgeContent = judgeResponse.choices[0]?.message?.content || '{}';\n    const judgeData = JSON.parse(judgeContent);\n\n    return NextResponse.json({\n      scores: judgeData.scores,\n    });\n  } catch (error) {\n    console.error('Judge error:', error);\n    return NextResponse.json(\n      { error: 'Failed to judge responses' },\n      { status: 500 }\n    );\n  }\n}\n```\n\n## 7. Main Page Component (app/page.tsx)\n\n```tsx\n'use client';\n\nimport { useState } from 'react';\nimport ModelComparison from './components/ModelComparison';\nimport JudgePanel from './components/JudgePanel';\nimport CostTracker from './components/CostTracker';\nimport ResultsDashboard from './components/ResultsDashboard';\n\nexport default function Home() {\n  const [comparisonData, setComparisonData] = useState(null);\n  const [judgeScores, setJudgeScores] = useState(null);\n  const [isLoading, setIsLoading] = useState(false);\n\n  const handleCompare = async (prompt: string) => {\n    setIsLoading(true);\n    try {\n      // Get model responses\n      const compareRes = await fetch('/api/compare', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ prompt }),\n      });\n      \n      if (!compareRes.ok) throw new Error('Comparison failed');\n      const compareData = await compareRes.json();\n      setComparisonData(compareData);\n\n      // Get judge scores\n      const judgeRes = await fetch('/api/judge', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ \n          prompt, \n          responses: compareData.responses \n        }),\n      });\n      \n      if (judgeRes.ok) {\n        const scores = await judgeRes.json();\n        setJudgeScores(scores);\n      }\n    } catch (error) {\n      console.error('Error:', error);\n    } finally {\n      setIsLoading(false);\n    }\n  };\n\n  return (\n    <div className=\"min-h-screen bg-gray-50 p-8\">\n      <div className=\"max-w-7xl mx-auto\">\n        <h1 className=\"text-4xl font-bold text-gray-900 mb-8\">\n          AI Model Comparison Platform\n        </h1>\n        \n        <div className=\"grid grid-cols-1 lg:grid-cols-3 gap-8\">\n          <div className=\"lg:col-span-2\">\n            <ModelComparison \n              onCompare={handleCompare}\n              isLoading={isLoading}\n              responses={comparisonData?.responses}\n            />\n            \n            {judgeScores && (\n              <JudgePanel scores={judgeScores.scores} />\n            )}\n          </div>\n          \n          <div className=\"space-y-6\">\n            <CostTracker costs={comparisonData?.costs} />\n            \n            {comparisonData && judgeScores && (\n              <ResultsDashboard \n                responses={comparisonData.responses}\n                scores={judgeScores.scores}\n                costs={comparisonData.costs}\n              />\n            ",
      "scores": {
        "relevance": 5,
        "reasoning": 2.5,
        "style": 2.5,
        "accuracy": 5,
        "honesty": 2.5,
        "explanation": "Evaluation failed",
        "totalScore": 25,
        "soundnessScore": 5
      },
      "latency": 64439,
      "cost": 0.30921,
      "finalScore": -42.531099999999995
    },
    {
      "model": "google/gemini-2.5-pro",
      "response": "Of course. Here is a comprehensive design for a Next.js 15 application that uses Vercel's AI Gateway to compare Grok, Claude, and Gemini, all while staying within a $10 budget.\n\nThis design leverages the latest Vercel and Next.js features for an efficient, scalable, and cost-effective solution.\n\n---\n\n### **Application Overview: \"AI Model Arena\"**\n\nThe app will allow a user to enter a single prompt, which is then sent in parallel to Grok, Claude 3 Haiku, and Gemini 1.5 Flash via Vercel's AI Gateway. The responses are displayed side-by-side. A \"Judge\" component allows the user to score each response. A dashboard visualizes the cumulative results, including model rankings and total spend, ensuring the $10 budget is not exceeded.\n\n### **Architectural Diagram**\n\n```\n+----------------+      +---------------------+      +---------------------+\n|   Next.js 15   |      | Vercel Server       |      | Vercel AI Gateway   |\n|   Frontend     |      | Action / API Route  |      | (Beta)              |\n| (React 19)     |----->| (Active CPU Billing)|----->| (Logging & Caching) |\n+----------------+      +---------------------+      +----------+----------+\n       ^                        |                                |\n       |                        |                        /-------+-------\\\n       |                        |                       /        |        \\\n       |                        |              +--------v-+ +------v---+ +------v---+\n       |                        |              | Grok API | | Claude API| | Gemini API|\n       |                        |              +----------+ +-----------+ +-----------+\n       |                        |\n+------v---------+      +-------v--------+\n|   Results UI   |      | Vercel KV      |\n|  - Judge Comp  |      | (for Redis)    |\n|  - Dashboard   |<-----| - Results      |\n+----------------+      | - Scores       |\n                        | - Cost Tracking|\n                        +----------------+\n```\n\n---\n\n### **1. AI Gateway Integration & Parallel Model Calls**\n\nThe core of the application is the Vercel AI Gateway. It acts as a single, resilient endpoint for all our AI calls, providing centralized logging, caching, and cost analysis.\n\n#### **Setup in Vercel Dashboard:**\n\n1.  Navigate to your Vercel project dashboard.\n2.  Go to the **AI Gateway** tab and create a new gateway.\n3.  **Connect Providers:** In the gateway settings, connect your AI provider accounts by adding their respective API keys (xAI for Grok, Anthropic for Claude, Google AI for Gemini). The AI Gateway will securely store these keys.\n4.  **Model Routing:** The Gateway automatically routes requests to the correct provider based on the `model` name specified in your API call. You don't need to manage different endpoints.\n5.  **Get the Gateway URL:** Vercel will provide a unique URL for your Gateway (e.g., `https://gateway.vercel.ai/v1/chat/completions`). This is the **only** AI endpoint your Next.js app will ever need to call.\n\n#### **Backend Implementation (Next.js 15 Server Action)**\n\nWe'll use a Server Action to handle the form submission. This keeps the client light and the logic on the server, where it can securely interact with the AI Gateway and our database.\n\n**File: `app/actions.ts`**\n\n```typescript\n'use server';\n\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport { kv } from '@vercel/kv';\nimport { revalidatePath } from 'next/cache';\n\n// Prices per 1M tokens (as of May 2024, check for updates)\n// We choose the fastest/cheapest models to stay in budget.\nconst MODEL_PRICING = {\n  'grok-1': { input: 0.25, output: 0.25 }, // Placeholder, use Grok's official pricing\n  'claude-3-haiku-20240307': { input: 0.25, output: 1.25 },\n  'gemini-1.5-flash-latest': { input: 0.35, output: 1.05 }, // Adjusted for 1.5 Flash\n};\n\nconst modelsToCompare = Object.keys(MODEL_PRICING);\n\n// Instantiate the AI SDK to use our Vercel AI Gateway\nconst vercelAI = createOpenAI({\n  baseURL: 'https://gateway.vercel.ai/v1', // Use your Vercel AI Gateway URL\n  // The API key can be a placeholder, as the Gateway injects the real keys.\n  apiKey: process.env.OPENAI_API_KEY, \n});\n\nexport interface AIResponse {\n  id: string;\n  model: string;\n  response: string;\n  cost: number;\n  promptTokens: number;\n  completionTokens: number;\n  scores?: {\n    relevance: number;\n    reasoning: number;\n    style: number;\n  };\n}\n\nexport interface ComparisonResult {\n  id: string;\n  prompt: string;\n  timestamp: number;\n  responses: AIResponse[];\n}\n\nexport async function getModelResponses(prompt: string) {\n  // Budget Check\n  const totalCost: number = (await kv.get('total_cost')) || 0;\n  if (totalCost >= 10.0) {\n    throw new Error('Budget exceeded. No more requests can be made.');\n  }\n\n  const comparisonId = `comp_${Date.now()}`;\n\n  const promises = modelsToCompare.map(model =>\n    streamText({\n      model: vercelAI(model), // The SDK sends the model name to the Gateway\n      prompt: prompt,\n    }).then(result => ({\n      model,\n      ...result,\n    }))\n  );\n\n  const results = await Promise.allSettled(promises);\n\n  const successfulResponses: AIResponse[] = [];\n  let batchCost = 0;\n\n  for (const result of results) {\n    if (result.status === 'fulfilled') {\n      const { model, text, usage } = result.value;\n      const { promptTokens, completionTokens } = usage;\n      \n      const pricing = MODEL_PRICING[model as keyof typeof MODEL_PRICING];\n      const cost = \n        (promptTokens / 1_000_000) * pricing.input +\n        (completionTokens / 1_000_000) * pricing.output;\n      \n      batchCost += cost;\n\n      successfulResponses.push({\n        id: `${comparisonId}_${model}`,\n        model,\n        response: text,\n        cost,\n        promptTokens,\n        completionTokens,\n      });\n    } else {\n      console.error('A model failed:', result.reason);\n    }\n  }\n  \n  // Store results in Vercel KV\n  const comparisonData: ComparisonResult = {\n    id: comparisonId,\n    prompt,\n    timestamp: Date.now(),\n    responses: successfulResponses,\n  };\n  await kv.set(comparisonId, comparisonData);\n  await kv.lpush('all_comparisons', comparisonId);\n  \n  // Atomically update total cost\n  await kv.incrbyfloat('total_cost', batchCost);\n  \n  revalidatePath('/'); // Revalidate home page to show new result\n  revalidatePath('/dashboard'); // Revalidate dashboard\n\n  return comparisonData;\n}\n```\n\n---\n\n### **2. Judge Component for Scoring**\n\nThis client component allows users to provide structured feedback on each AI's output.\n\n**File: `components/JudgeComponent.tsx`**\n\n```tsx\n'use client';\n\nimport { useState } from 'react';\nimport { saveScores } from '@/app/actions'; // We'll create this action next\n\ninterface JudgeComponentProps {\n  comparisonId: string;\n  modelName: string;\n  hasBeenScored: boolean;\n}\n\nexport function JudgeComponent({ comparisonId, modelName, hasBeenScored }: JudgeComponentProps) {\n  const [relevance, setRelevance] = useState(3);\n  const [reasoning, setReasoning] = useState(3);\n  const [style, setStyle] = useState(3);\n  const [isSubmitting, setIsSubmitting] = useState(false);\n  const [submitted, setSubmitted] = useState(hasBeenScored);\n\n  const handleSubmit = async (e: React.FormEvent) => {\n    e.preventDefault();\n    setIsSubmitting(true);\n    try {\n      await saveScores(comparisonId, modelName, { relevance, reasoning, style });\n      setSubmitted(true);\n    } catch (error) {\n      console.error('Failed to save scores:', error);\n    } finally {\n      setIsSubmitting(false);\n    }\n  };\n\n  if (submitted) {\n    return <p className=\"text-sm text-green-500 font-bold mt-4\">✓ Scores Saved</p>;\n  }\n\n  return (\n    <form onSubmit={handleSubmit} className=\"mt-4 p-4 border-t\">\n      <h4 className=\"font-semibold mb-2\">Rate this Response (1-5)</h4>\n      <div className=\"space-y-3\">\n        {/* Simple slider example, use shadcn/ui for better aesthetics */}\n        <label>Relevance: {relevance}</label>\n        <input type=\"range\" min=\"1\" max=\"5\" value={relevance} onChange={(e) => setRelevance(Number(e.target.value))} />\n        <label>Reasoning: {reasoning}</label>\n        <input type=\"range\" min=\"1\" max=\"5\" value={reasoning} onChange={(e) => setReasoning(Number(e.target.value))} />\n        <label>Style: {style}</label>\n        <input type=\"range\" min=\"1\" max=\"5\" value={style} onChange={(e) => setStyle(Number(e.target.value))} />\n      </div>\n      <button type=\"submit\" disabled={isSubmitting} className=\"mt-4 ...\">\n        {isSubmitting ? 'Saving...' : 'Save Scores'}\n      </button>\n    </form>\n  );\n}\n```\n\n**Action to save scores (`app/actions.ts`):**\n\n```typescript\n// Add this to your app/actions.ts file\nexport async function saveScores(\n  comparisonId: string,\n  modelName: string,\n  scores: { relevance: number; reasoning: number; style: number }\n) {\n  const comparison: ComparisonResult | null = await kv.get(comparisonId);\n  if (!comparison) {\n    throw new Error('Comparison not found.');\n  }\n\n  const responseIndex = comparison.responses.findIndex(r => r.model === modelName);\n  if (responseIndex === -1) {\n    throw new Error('Model response not found in this comparison.');\n  }\n\n  comparison.responses[responseIndex].scores = scores;\n\n  await kv.set(comparisonId, comparison);\n  revalidatePath('/dashboard'); // Update dashboard with new scores\n}\n```\n\n---\n\n### **3. Cost Tracking to Stay Under $10**\n\nOur cost tracking is built into the `getModelResponses` action and uses Vercel KV for persistence.\n\n1.  **Price Constants:** We define model prices at the top of our action file. This makes them easy to update.\n2.  **Per-Request Calculation:** After each AI call, we use the `usage` metadata (prompt and completion tokens) returned by the AI SDK to calculate the exact cost of that single request.\n3.  **Atomic Updates in Vercel KV:** We store the total cumulative cost in a single KV key, `total_cost`. Using `kv.incrbyfloat()` ensures that even if multiple requests happen concurrently, the total cost is updated safely and accurately.\n4.  **Budget Gate:** Before any new requests are sent to the AI Gateway, the function first checks the `total_cost` from KV. If it's `$10` or more, it throws an error, preventing further spending. The frontend can catch this error and disable the UI.\n\n---\n\n### **4. Results Dashboard**\n\nThe dashboard provides an at-a-glance view of the experiment's progress and results. It fetches all data from Vercel KV.\n\n**File: `app/dashboard/page.tsx`**\n\n```tsx\nimport { kv } from '@vercel/kv';\nimport { ComparisonResult } from '@/app/actions';\nimport { BarChart } from 'recharts'; // Example charting library\n\n// This is a Server Component, fetching data directly\nexport default async function DashboardPage() {\n  const totalCost: number = (await kv.get('total_cost')) || 0;\n  const comparisonIds: string[] = await kv.lrange('all_comparisons', 0, -1);\n  const comparisons: ComparisonResult[] = await (comparisonIds.length > 0 ? kv.mget(...comparisonIds) : []);\n\n  // --- Data Processing for Charts ---\n  const modelStats: { [key: string]: any } = {};\n  \n  for (const comp of comparisons.filter(c => c)) {\n    for (const resp of comp.responses) {\n      if (!modelStats[resp.model]) {\n        modelStats[resp.model] = { name: resp.model, totalCost: 0, requestCount: 0, totalScore: 0, scoredCount: 0 };\n      }\n      modelStats[resp.model].totalCost += resp.cost;\n      modelStats[resp.model].requestCount++;\n      if (resp.scores) {\n        modelStats[resp.model].totalScore += resp.scores.relevance + resp.scores.reasoning + resp.scores.style;\n        modelStats[resp.model].scoredCount++;\n      }\n    }\n  }\n  \n  const chartData = Object.values(modelStats).map(m => ({\n      ...m,\n      avgScore: m.scoredCount > 0 ? (m.totalScore / (m.scoredCount * 3)).toFixed(2) : 'N/A'\n  }));\n  \n  const budgetPercentage = (totalCost / 10) * 100;\n\n  return (\n    <div className=\"p-8\">\n      <h1 className=\"text-3xl font-bold mb-6\">AI Model Arena: Dashboard</h1>\n      \n      {/* Budget Tracker */}\n      <div className=\"mb-8\">\n        <h2 className=\"text-xl font-semibold\">Budget Usage</h2>\n        <p className=\"text-2xl font-mono\">${totalCost.toFixed(4)} / $10.00</p>\n        <div className=\"w-full bg-gray-200 rounded-full h-4 mt-2\">\n            <div \n                className=\"bg-blue-600 h-4 rounded-full\" \n                style={{ width: `${budgetPercentage}%` }}\n            ></div>\n        </div>\n      </div>\n      \n      {/* TODO: Use Recharts to build Bar charts with `chartData` */}\n      {/* Example: Bar chart for Average Score per Model */}\n      {/* Example: Bar chart for Total Cost per Model */}\n\n      <pre>{JSON.stringify(chartData, null, 2)}</pre>\n    </div>\n  );\n}\n```\n\n### **Leveraging Vercel's Platform Features**\n\n*   **Vercel AI Gateway:** This is the cornerstone. It decouples our app from specific AI SDKs, provides a unified interface, and gives us invaluable observability (logs, analytics, cost tracking) out of the box.\n*   **Active CPU Billing:** Our `getModelResponses` Server Action is a Vercel Function. When it runs `Promise.all`, it has a short, intense burst of activity. With Active CPU billing, we only pay for the compute time used during this burst, not for idle time before or after. This is more cost-effective than traditional \"duration-based\" billing for this type of workload.\n*   **Vercel KV:** A serverless Redis database that is incredibly fast and easy to use. It's perfect for our use case: storing semi-structured JSON data, tracking comparison history, and, most importantly, providing atomic operations (`incrbyfloat`) for reliable cost tracking.\n*   **Next.js 15 with Server Actions:** By using Server Actions, we co-locate our backend logic with our frontend components, simplifying the codebase. This avoids the need to create separate API route files and makes data fetching and mutations feel seamless.",
      "scores": {
        "relevance": 5,
        "reasoning": 2.5,
        "style": 2.5,
        "accuracy": 5,
        "honesty": 2.5,
        "explanation": "Evaluation failed",
        "totalScore": 25,
        "soundnessScore": 5
      },
      "latency": 73425,
      "cost": 0.014280000000000001,
      "finalScore": -48.5678
    }
  ],
  "summary": {
    "winner": "xai/grok-3",
    "totalCost": 0.36445500000000003,
    "totalLatency": 174636
  }
}