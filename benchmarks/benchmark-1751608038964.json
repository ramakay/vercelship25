{
  "timestamp": "2025-07-04T05:47:18.964Z",
  "prompt": "Design a Next.js 15 app using Vercel's AI Gateway to compare 3 AI models (Grok, Claude, Gemini) with a $10 budget. Include:\n\n1. AI Gateway integration to call all 3 models in parallel\n2. A judge component that scores responses on relevance, reasoning, and style\n3. Cost tracking to stay under $10 total\n4. Results dashboard showing model rankings and costs\n\nUse Vercel's latest features: AI Gateway (Beta), Active CPU billing, and optionally Sandbox/Queues if available.",
  "evaluations": [
    {
      "model": "xai/grok-3",
      "response": "I'll design a Next.js 15 app that compares AI models using Vercel's AI Gateway while maintaining a $10 budget. The solution will use modern React patterns, TypeScript, and Vercel's latest features. I'll break this down into key components and provide code for the main files.\n\n### Project Structure and Assumptions\n- **Next.js 15** with App Router and Server Actions for API calls.\n- **Vercel AI Gateway (Beta)** for unified API access to Grok, Claude, and Gemini.\n- **Active CPU Billing** to optimize costs (serverless functions scale to zero when idle).\n- **Cost Tracking** using a simple in-memory store (or Vercel KV if available) to monitor API usage.\n- **Judge Component** with a scoring algorithm for relevance, reasoning, and style.\n- **Dashboard** using shadcn/ui for visualization.\n- Budget: $10 total, assuming approximate costs per API call (will track dynamically).\n\n**Note**: Sandbox/Queues are not explicitly used as they're optional and not critical for this use case, but Iâ€™ll mention where they could fit.\n\n### Project Setup\n1. Initialize Next.js 15 project with TypeScript:\n   ```bash\n   npx create-next-app@latest ai-model-comparator --typescript --app\n   ```\n   Select App Router and include Tailwind CSS.\n\n2. Install dependencies:\n   ```bash\n   npm install @vercel/ai-gateway lucide-react recharts\n   npx shadcn@latest init\n   npx shadcn@latest add button card table\n   ```\n\n3. Set up Vercel AI Gateway:\n   - Obtain API keys for Grok (xAI), Claude (Anthropic), and Gemini (Google).\n   - Configure environment variables in `.env.local` and Vercel dashboard:\n     ```env\n     VERCEL_AI_GATEWAY_URL=https://api.vercel.ai/v1\n     VERCEL_AI_GATEWAY_KEY=your_vercel_ai_key\n     GROK_API_KEY=your_grok_key\n     CLAUDE_API_KEY=your_claude_key\n     GEMINI_API_KEY=your_gemini_key\n     ```\n\n### Key Components and Code\n\n#### 1. AI Gateway Integration (`app/api/compare/route.ts`)\nThis server route uses Vercel's AI Gateway to call all 3 models in parallel. It tracks costs per call (approximate, based on token usage).\n\n```typescript\n// app/api/compare/route.ts\nimport { NextResponse } from \"next/server\";\nimport { createClient } from \"@vercel/ai-gateway\";\n\nconst client = createClient({\n  endpoint: process.env.VERCEL_AI_GATEWAY_URL || \"\",\n  apiKey: process.env.VERCEL_AI_GATEWAY_KEY || \"\",\n});\n\nconst COST_PER_1K_TOKENS = {\n  grok: 0.0005, // Approx $0.50/million tokens\n  claude: 0.003, // Approx $3/million tokens\n  gemini: 0.00035, // Approx $0.35/million tokens\n};\n\nlet totalCost = 0; // In-memory cost tracking (replace with Vercel KV for prod)\n\nexport async function POST(req: Request) {\n  try {\n    const { prompt } = await req.json();\n    if (totalCost >= 10) {\n      return NextResponse.json({ error: \"Budget exceeded\" }, { status: 403 });\n    }\n\n    // Parallel calls to all models via AI Gateway\n    const [grokRes, claudeRes, geminiRes] = await Promise.all([\n      client.chat.completions.create({\n        model: \"grok-1\",\n        messages: [{ role: \"user\", content: prompt }],\n        headers: { \"Authorization\": `Bearer ${process.env.GROK_API_KEY}` },\n      }),\n      client.chat.completions.create({\n        model: \"claude-3-opus-20240229\",\n        messages: [{ role: \"user\", content: prompt }],\n        headers: { \"Authorization\": `Bearer ${process.env.CLAUDE_API_KEY}` },\n      }),\n      client.chat.completions.create({\n        model: \"gemini-1.5-pro\",\n        messages: [{ role: \"user\", content: prompt }],\n        headers: { \"Authorization\": `Bearer ${process.env.GEMINI_API_KEY}` },\n      }),\n    ]);\n\n    // Extract responses and calculate costs (mock token count for simplicity)\n    const responses = {\n      grok: grokRes.choices[0].message.content || \"\",\n      claude: claudeRes.choices[0].message.content || \"\",\n      gemini: geminiRes.choices[0].message.content || \"\",\n    };\n\n    const costs = {\n      grok: COST_PER_1K_TOKENS.grok * 2, // Assume 2K tokens per call\n      claude: COST_PER_1K_TOKENS.claude * 2,\n      gemini: COST_PER_1K_TOKENS.gemini * 2,\n    };\n\n    totalCost += Object.values(costs).reduce((a, b) => a + b, 0);\n\n    return NextResponse.json({\n      responses,\n      costs,\n      totalCost,\n      remainingBudget: 10 - totalCost,\n    });\n  } catch (error) {\n    return NextResponse.json({ error: String(error) }, { status: 500 });\n  }\n}\n```\n\n**Note**: Active CPU Billing ensures costs are minimal when the app is idle. For production, replace `totalCost` with Vercel KV or a database for persistence.\n\n#### 2. Judge Component (`components/Judge.tsx`)\nThis component scores responses based on relevance, reasoning, and style (simple heuristic-based scoring for demo purposes).\n\n```typescript\n// components/Judge.tsx\n\"use client\";\n\nimport { useMemo } from \"react\";\nimport { Card, CardContent, CardHeader, CardTitle } from \"@/components/ui/card\";\n\ntype ResponseScores = {\n  model: string;\n  relevance: number;\n  reasoning: number;\n  style: number;\n  total: number;\n};\n\nexport function Judge({\n  responses,\n}: {\n  responses: Record<string, string>;\n}) {\n  const scores: ResponseScores[] = useMemo(() => {\n    return Object.entries(responses).map(([model, response]) => {\n      const relevance = response.includes(\"relevant\") ? 8 : Math.min(10, response.length / 50); // Mock scoring\n      const reasoning = response.split(\"because\").length > 1 ? 9 : Math.min(10, response.length / 100);\n      const style = response.split(\".\").length > 3 ? 8 : Math.min(10, response.length / 75);\n      return {\n        model,\n        relevance,\n        reasoning,\n        style,\n        total: relevance + reasoning + style,\n      };\n    }).sort((a, b) => b.total - a.total);\n  }, [responses]);\n\n  return (\n    <Card className=\"w-full\">\n      <CardHeader>\n        <CardTitle>Model Scores</CardTitle>\n      </CardHeader>\n      <CardContent>\n        <table className=\"w-full\">\n          <thead>\n            <tr>\n              <th>Model</th>\n              <th>Relevance (0-10)</th>\n              <th>Reasoning (0-10)</th>\n              <th>Style (0-10)</th>\n              <th>Total (0-30)</th>\n            </tr>\n          </thead>\n          <tbody>\n            {scores.map((score) => (\n              <tr key={score.model}>\n                <td>{score.model}</td>\n                <td>{score.relevance.toFixed(1)}</td>\n                <td>{score.reasoning.toFixed(1)}</td>\n                <td>{score.style.toFixed(1)}</td>\n                <td>{score.total.toFixed(1)}</td>\n              </tr>\n            ))}\n          </tbody>\n        </table>\n      </CardContent>\n    </Card>\n  );\n}\n```\n\n#### 3. Cost Tracking and Budget Management\nCost tracking is handled in the API route above. The dashboard below displays the current spend and warns if nearing the $10 limit.\n\n#### 4. Results Dashboard (`app/page.tsx`)\nThe main page includes a form to submit prompts, displays responses, scores, and costs using a clean UI.\n\n```typescript\n// app/page.tsx\n\"use client\";\n\nimport { useState } from \"react\";\nimport { Button } from \"@/components/ui/button\";\nimport { Card, CardContent, CardHeader, CardTitle } from \"@/components/ui/card\";\nimport { Judge } from \"@/components/Judge\";\nimport { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip, Legend, ResponsiveContainer } from \"recharts\";\n\nexport default function Home() {\n  const [prompt, setPrompt] = useState(\"\");\n  const [responses, setResponses] = useState<Record<string, string>>({});\n  const [costs, setCosts] = useState<Record<string, number>>({});\n  const [totalCost, setTotalCost] = useState(0);\n  const [loading, setLoading] = useState(false);\n  const [error, setError] = useState(\"\");\n\n  const handleSubmit = async (e: React.FormEvent) => {\n    e.preventDefault();\n    setLoading(true);\n    setError(\"\");\n    try {\n      const res = await fetch(\"/api/compare\", {\n        method: \"POST\",\n        headers: { \"Content-Type\": \"application/json\" },\n        body: JSON.stringify({ prompt }),\n      });\n      const data = await res.json();\n      if (data.error) {\n        setError(data.error);\n      } else {\n        setResponses(data.responses);\n        setCosts(data.costs);\n        setTotalCost(data.totalCost);\n      }\n    } catch (err) {\n      setError(String(err));\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  const costData = [\n    { name: \"Grok\", cost: costs.grok || 0 },\n    { name: \"Claude\", cost: costs.claude || 0 },\n    { name: \"Gemini\", cost: costs.gemini || 0 },\n  ];\n\n  return (\n    <div className=\"container mx-auto p-4\">\n      <h1 className=\"text-2xl font-bold mb-4\">AI Model Comparator</h1>\n      <Card className=\"mb-6\">\n        <CardHeader>\n          <CardTitle>Submit a Prompt</CardTitle>\n        </CardHeader>\n        <CardContent>\n          <form onSubmit={handleSubmit} className=\"space-y-4\">\n            <input\n              type=\"text\"\n              value={prompt}\n              onChange={(e) => setPrompt(e.target.value)}\n              placeholder=\"Enter a prompt to compare models\"\n              className=\"w-full p-2 border rounded-md\"\n              disabled={loading || totalCost >= 10}\n            />\n            <Button type=\"submit\" disabled={loading || totalCost >= 10}>\n              {loading ? \"Comparing...\" : \"Compare Models\"}\n            </Button>\n            {error && <p className=\"text-red-500\">{error}</p>}\n          </form>\n        </CardContent>\n      </Card>\n\n      {Object.keys(responses).length > 0 && (\n        <>\n          <Judge responses={responses} />\n          <Card className=\"mt-6\">\n            <CardHeader>\n              <CardTitle>Cost Overview (Budget: $10)</CardTitle>\n            </CardHeader>\n            <CardContent>\n              <p className=\"mb-4\">Total Spent: ${totalCost.toFixed(4)}</p>\n              <p className=\"mb-4\">Remaining: ${(10 - totalCost).toFixed(4)}</p>\n              <ResponsiveContainer width=\"100%\" height={300}>\n                <LineChart data={costData}>\n                  <CartesianGrid strokeDasharray=\"3 3\" />\n                  <XAxis dataKey=\"name\" />\n                  <YAxis />\n                  <Tooltip />\n                  <Legend />\n                  <Line type=\"monotone\" dataKey=\"cost\" stroke=\"#8884d8\" />\n                </LineChart>\n              </ResponsiveContainer>\n            </CardContent>\n          </Card>\n        </>\n      )}\n    </div>\n  );\n}\n```\n\n### Cost Management and Optimization\n- **Budget Cap**: The API route rejects requests if the total cost exceeds $10.\n- **Active CPU Billing**: Vercelâ€™s serverless functions scale to zero when idle, minimizing costs outside of API calls.\n- **Approximate Costs**: Based on typical pricing (e.g., Claude is more expensive), Iâ€™ve set mock per-call costs. In production, use actual token counts from API responses.\n- **Potential Use of Queues**: If Vercel Queues are available, queue API calls to throttle usage and manage costs more granularly.\n\n### Deployment on Vercel\n1. Push to GitHub and deploy on Vercel.\n2. Set environment variables in the Vercel dashboard.\n3. Monitor usage via Vercelâ€™s billing dashboard to ensure staying under $10.\n\n### Additional Notes\n- **Scalability**: For production, replace in-memory `totalCost` with Vercel KV or a database.\n- **Sandbox**: If Vercel Sandbox is available, use it to test API integrations without real costs during development.\n- **Error Handling**: Enhanced error handling can be added for rate limits and API failures.\n- **UI Polish**: Add more shadcn/ui components (e.g., alerts for budget warnings) for better UX.\n\nThis design provides a functional, budget-conscious app to compare AI models with Vercelâ€™s latest features. Let me know if you'd like deeper details on any part!",
      "scores": {
        "relevance": 8,
        "reasoning": 4,
        "style": 4,
        "accuracy": 6,
        "honesty": 3,
        "explanation": "The response is largely relevant as it provides a comprehensive design for a Next.js 15 app using Vercel's AI Gateway to compare AI models, addressing the user's request. However, it misrepresents the AI Gateway integration by incorrectly suggesting the use of a `createClient` function, which is not accurate according to the current documentation. The explanation of cost tracking is reasonable, but it lacks clarity on how to implement Vercel KV for production use. The style is clear and structured, but some sections could be more concise. The accuracy is affected by the misrepresentation of the AI Gateway integration and the incorrect use of the term 'Active CPU Billing,' which is not an official feature name. The response does not explicitly acknowledge any knowledge limitations, which affects the honesty score.",
        "totalScore": 39,
        "soundnessScore": 8
      },
      "latency": 36092,
      "cost": 0.043185,
      "finalScore": 2.4761500000000014
    },
    {
      "model": "google/gemini-2.5-pro",
      "response": "Of course. Here is a comprehensive design for a Next.js 15 app that uses Vercel's AI Gateway to compare Grok, Claude, and Gemini, all while staying within a $10 budget.\n\nThis design leverages Vercel's latest features for a modern, efficient, and cost-effective solution.\n\n---\n\n### **1. High-Level Architecture**\n\nThe application will function as follows:\n1.  **User Interface (Next.js 15):** A user enters a prompt into a simple web form.\n2.  **Server Action:** The form submission triggers a Next.js 15 Server Action.\n3.  **Parallel Fan-Out (AI Gateway):** The Server Action makes three parallel API calls to the Vercel AI Gateway, each routed to a different model (Grok, Claude, Gemini).\n4.  **AI Gateway Proxy:** The Gateway forwards these requests to the actual model APIs, logging usage, and handling authentication.\n5.  **Response Aggregation:** The Server Action waits for all three responses to return.\n6.  **AI Judge:** It then sends the original prompt and all three responses to a fourth, more powerful \"judge\" model (e.g., Claude 3 Opus) via the AI Gateway. This judge is prompted to return a structured JSON with scores.\n7.  **Cost Calculation:** The Server Action calculates the estimated cost of the four API calls based on token counts.\n8.  **Data Persistence (Vercel KV):** The entire result (prompt, responses, scores, cost) is saved to Vercel KV for a persistent dashboard.\n9.  **UI Update:** The Server Action completes, and the UI updates reactively to display the results on a dashboard.\n\n\n\n### **2. Technology Stack**\n\n*   **Framework:** Next.js 15 (React 19)\n*   **AI Orchestration:** Vercel AI Gateway\n*   **AI SDK:** Vercel AI SDK 3.0\n*   **UI:** Tailwind CSS with shadcn/ui for components\n*   **Data:** Vercel KV for storing results\n*   **Deployment:** Vercel (leveraging Serverless Functions & Active CPU Billing)\n*   **Models:**\n    *   **Competitors:** Grok (via GroqCloud), Claude 3 Sonnet (via Anthropic), Gemini 1.5 Flash (via Google)\n    *   **Judge:** Claude 3 Opus (via Anthropic) for its superior reasoning.\n\n---\n\n### **3. Step-by-Step Implementation Plan**\n\n#### **Step 1: Project Setup & Vercel AI Gateway Configuration**\n\n1.  **Initialize Next.js App:**\n    ```bash\n    npx create-next-app@latest ai-model-comparator\n    cd ai-model-comparator\n    ```\n\n2.  **Install Dependencies:**\n    ```bash\n    npm install ai zod @vercel/kv groq-sdk @anthropic-ai/sdk @google/generative-ai\n    npm install recharts # For charts\n    npx shadcn-ui@latest init # Follow prompts\n    npx shadcn-ui@latest add button textarea card label chart # Add components\n    ```\n\n3.  **Set up Vercel AI Gateway:**\n    *   Go to your Vercel Dashboard -> **AI Gateway**.\n    *   Create a new Gateway and connect it to your `ai-model-comparator` project.\n    *   In the Gateway settings, create four routes. The route name is what you'll use in your code.\n        *   **Route 1:** `grok-lpu` -> Provider: Groq, Model: `llama3-70b-8192`\n        *   **Route 2:** `claude-sonnet` -> Provider: Anthropic, Model: `claude-3-sonnet-20240229`\n        *   **Route 3:** `gemini-flash` -> Provider: Google, Model: `gemini-1.5-flash-latest`\n        *   **Route 4 (Judge):** `claude-opus-judge` -> Provider: Anthropic, Model: `claude-3-opus-20240229`\n    *   This setup gives you a single base URL (`process.env.AI_GATEWAY_URL`) that you can use with different path suffixes (`/grok-lpu`, `/claude-sonnet`, etc.).\n\n4.  **Environment Variables (`.env.local`):**\n    Vercel will automatically inject the AI Gateway variables when connected. Add your provider keys.\n    ```env\n    # Vercel will provide these after connecting the Gateway\n    AI_GATEWAY_URL=\"https://gw.vercel.ai/...\"\n\n    # Add API Keys for the providers\n    ANTHROPIC_API_KEY=\"sk-...\"\n    GROQ_API_KEY=\"gsk_...\"\n    GOOGLE_GENERATIVE_AI_API_KEY=\"AIza...\"\n\n    # Vercel KV variables (from Vercel dashboard)\n    KV_URL=\"...\"\n    KV_REST_API_URL=\"...\"\n    KV_REST_API_TOKEN=\"...\"\n    KV_REST_API_READ_ONLY_TOKEN=\"...\"\n    ```\n\n#### **Step 2: The Core Logic (Server Action)**\n\nThis is where we call the models in parallel and then the judge.\n\n`app/actions.ts`\n```typescript\n'use server';\n\nimport { CoreMessage, generateObject, generateText } from 'ai';\nimport { createAnthropic } from '@ai-sdk/anthropic';\nimport { createGoogle } from '@ai-sdk/google';\nimport { createOpenAI } from '@ai-sdk/openai'; // Groq uses an OpenAI-compatible API\nimport { z } from 'zod';\nimport { kv } from '@vercel/kv';\nimport { revalidatePath } from 'next/cache';\n\n// Use the generic OpenAI provider for Groq, pointing it to the AI Gateway route\nconst groq = createOpenAI({\n  baseURL: `${process.env.AI_GATEWAY_URL}/grok-lpu`,\n  apiKey: process.env.GROQ_API_KEY,\n});\n\nconst anthropic = createAnthropic({\n  baseURL: `${process.env.AI_GATEWAY_URL}/claude-sonnet`,\n  apiKey: process.env.ANTHROPIC_API_KEY,\n});\n\nconst google = createGoogle({\n  baseURL: `${process.env.AI_GATEWAY_URL}/gemini-flash`,\n  apiKey: process.env.GOOGLE_GENERATIVE_AI_API_KEY,\n});\n\n// A separate, powerful model instance for the judge\nconst judgeAnthropic = createAnthropic({\n  baseURL: `${process.env.AI_GATEWAY_URL}/claude-opus-judge`,\n  apiKey: process.env.ANTHROPIC_API_KEY,\n});\n\n// Define the schema for the judge's response\nconst à¤œà¤œSchema = z.object({\n  grok: z.object({\n    relevance: z.number().min(1).max(10),\n    reasoning: z.number().min(1).max(10),\n    style: z.number().min(1).max(10),\n    critique: z.string(),\n  }),\n  claude: z.object({\n    relevance: z.number().min(1).max(10),\n    reasoning: z.number().min(1).max(10),\n    style: z.number().min(1).max(10),\n    critique: z.string(),\n  }),\n  gemini: z.object({\n    relevance: z.number().min(1).max(10),\n    reasoning: z.number().min(1).max(10),\n    style: z.number().min(1).max(10),\n    critique: z.string(),\n  }),\n  winner: z.enum(['grok', 'claude', 'gemini', 'tie']),\n});\n\n// --- Main Server Action ---\nexport async function compareModels(prevState: any, formData: FormData) {\n  const prompt = formData.get('prompt') as string;\n  if (!prompt) return { error: 'Prompt is required.' };\n\n  const messages: CoreMessage[] = [{ role: 'user', content: prompt }];\n  \n  try {\n    // 1. Call all 3 models in parallel\n    const [grokResult, claudeResult, geminiResult] = await Promise.all([\n      generateText({ model: groq('llama3-70b-8192'), messages }),\n      generateText({ model: anthropic('claude-3-sonnet-20240229'), messages }),\n      generateText({ model: google('gemini-1.5-flash-latest'), messages }),\n    ]);\n\n    // 2. Call the Judge AI\n    const judgePrompt = `\n      Original Prompt: \"${prompt}\"\n\n      --- Grok's Response ---\n      ${grokResult.text}\n\n      --- Claude's Response ---\n      ${claudeResult.text}\n\n      --- Gemini's Response ---\n      ${geminiResult.text}\n\n      --- Task ---\n      You are an expert AI evaluator. Please score each response on a scale of 1-10 for relevance, reasoning, and style. Provide a brief critique for each. Finally, declare a winner. Respond ONLY with a valid JSON object matching the required schema.\n    `;\n\n    const { object: scores } = await generateObject({\n      model: judgeAnthropic('claude-3-opus-20240229'),\n      schema: à¤œà¤œSchema,\n      prompt: judgePrompt,\n    });\n    \n    // 3. Cost Calculation (Simplified - see Cost Tracking section for details)\n    const totalInputTokens = \n      grokResult.usage.promptTokens + \n      claudeResult.usage.promptTokens + \n      geminiResult.usage.promptTokens +\n      (await judgeAnthropic('claude-3-opus-20240229').countTokens({ prompt: judgePrompt }));\n      \n    const totalOutputTokens = \n      grokResult.usage.completionTokens +\n      claudeResult.usage.completionTokens +\n      geminiResult.usage.completionTokens;\n      \n    // NOTE: This is an estimation. Real cost is on the Vercel dashboard.\n    // Prices are per 1M tokens.\n    // Example prices: Flash In/$0.35 Out/$0.70 | Sonnet In/$3 Out/$15 | Llama3-70b In/$0.59 Out/$0.79 | Opus In/$15 Out/$75\n    const estimatedCost = \n        (totalInputTokens / 1_000_000) * 19 + // Sum of input prices\n        (totalOutputTokens / 1_000_000) * 91; // Sum of output prices\n\n    // 4. Store result in Vercel KV\n    const comparisonResult = {\n      id: crypto.randomUUID(),\n      prompt,\n      responses: {\n        grok: grokResult.text,\n        claude: claudeResult.text,\n        gemini: geminiResult.text,\n      },\n      scores,\n      estimatedCost,\n      createdAt: new Date().toISOString(),\n    };\n    \n    await kv.set(`comparison:${comparisonResult.id}`, comparisonResult);\n    \n    // Revalidate the path to show new results on the dashboard\n    revalidatePath('/');\n    \n    return { result: comparisonResult };\n\n  } catch (error) {\n    console.error(error);\n    return { error: 'An error occurred while comparing models.' };\n  }\n}\n```\n\n#### **Step 3: Frontend Form and Results Dashboard**\n\n`app/page.tsx`\n```tsx\nimport { useFormState, useFormStatus } from 'react-dom';\nimport { compareModels } from './actions';\nimport { Button } from '@/components/ui/button';\nimport { Textarea } from '@/components/ui/textarea';\nimport { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';\nimport { kv } from '@vercel/kv';\nimport { ResultsDashboard } from './results-dashboard';\n\nconst initialState = { error: null, result: null };\n\nfunction SubmitButton() {\n  const { pending } = useFormStatus();\n  return <Button type=\"submit\" disabled={pending}>{pending ? 'Comparing...' : 'Compare Models'}</Button>;\n}\n\nexport default async function HomePage() {\n  const [state, formAction] = useFormState(compareModels, initialState);\n  \n  // Fetch all past results from KV for the dashboard\n  const keys = await kv.keys('comparison:*');\n  const pastResults = keys.length > 0 ? await kv.mget(...keys) : [];\n\n  return (\n    <main className=\"container mx-auto p-4 space-y-8\">\n      <Card>\n        <CardHeader><CardTitle>AI Model Comparator</CardTitle></CardHeader>\n        <CardContent>\n          <form action={formAction} className=\"space-y-4\">\n            <Textarea name=\"prompt\" placeholder=\"Enter your prompt here...\" required />\n            <SubmitButton />\n            {state.error && <p className=\"text-red-500\">{state.error}</p>}\n          </form>\n        </CardContent>\n      </Card>\n      \n      {/* Display the most recent result immediately */}\n      {state.result && <ResultsDashboard results={[state.result]} title=\"Latest Comparison\" />}\n      \n      {/* Display all past results */}\n      {pastResults.length > 0 && <ResultsDashboard results={pastResults} title=\"Comparison History\" />}\n    </main>\n  );\n}\n```\n\n`app/results-dashboard.tsx`\n```tsx\n'use client'; // This component uses client-side charting\n\nimport { Card, CardContent, CardDescription, CardHeader, CardTitle } from \"@/components/ui/card\";\nimport { Bar, BarChart, ResponsiveContainer, XAxis, YAxis, Tooltip } from 'recharts';\n\n// Define the type for a single result\ntype ComparisonResult = {\n  id: string;\n  prompt: string;\n  responses: { grok: string; claude: string; gemini: string; };\n  scores: any; // Use 'any' for simplicity, or import the Zod schema type\n  estimatedCost: number;\n  createdAt: string;\n}\n\nexport function ResultsDashboard({ results, title }: { results: ComparisonResult[], title: string }) {\n  // Aggregate data for charting total costs and average scores\n  const totalCost = results.reduce((acc, r) => acc + r.estimatedCost, 0);\n\n  return (\n    <Card>\n      <CardHeader>\n        <CardTitle>{title}</CardTitle>\n        <CardDescription>Total Estimated Cost for this set: ${totalCost.toFixed(6)}</CardDescription>\n      </CardHeader>\n      <CardContent className=\"space-y-6\">\n        {results.sort((a,b) => new Date(b.createdAt).getTime() - new Date(a.createdAt).getTime()).map(result => (\n          <div key={result.id} className=\"border p-4 rounded-lg\">\n            <h3 className=\"font-bold\">Prompt: \"{result.prompt}\"</h3>\n            <p className=\"text-sm text-muted-foreground\">Winner: <span className=\"font-semibold capitalize\">{result.scores.winner}</span> | Est. Cost: ${result.scores.estimatedCost.toFixed(6)}</p>\n\n            {/* Chart for scores */}\n            <ScoreChart scores={result.scores} />\n\n            {/* Responses Side-by-Side */}\n            <div className=\"grid md:grid-cols-3 gap-4 mt-4\">\n              <ModelResponse title=\"Grok (Llama3-70b)\" response={result.responses.grok} score={result.scores.grok} />\n              <ModelResponse title=\"Claude 3 Sonnet\" response={result.responses.claude} score={result.scores.claude} />\n              <ModelResponse title=\"Gemini 1.5 Flash\" response={result.responses.gemini} score={result.scores.gemini} />\n            </div>\n          </div>\n        ))}\n      </CardContent>\n    </Card>\n  );\n}\n\n// Chart Component\nconst ScoreChart = ({ scores }: { scores: any }) => {\n    const data = [\n        { name: 'Grok', total: scores.grok.relevance + scores.grok.reasoning + scores.grok.style },\n        { name: 'Claude', total: scores.claude.relevance + scores.claude.reasoning + scores.claude.style },\n        { name: 'Gemini', total: scores.gemini.relevance + scores.gemini.reasoning + scores.gemini.style },\n    ];\n    return (\n        <div className=\"h-40 my-4\">\n            <ResponsiveContainer width=\"100%\" height=\"100%\">\n                <BarChart data={data} layout=\"vertical\">\n                    <XAxis type=\"number\" hide />\n                    <YAxis type=\"category\" dataKey=\"name\" width={60} stroke=\"#888888\" />\n                    <Tooltip />\n                    <Bar dataKey=\"total\" fill=\"#8884d8\" radius={[4, 4, 0, 0]} />\n                </BarChart>\n            </ResponsiveContainer>\n        </div>\n    );\n};\n\n// Single Model Response Component\nconst ModelResponse = ({ title, response, score }: { title: string, response: string, score: any }) => (\n  <Card>\n    <CardHeader><CardTitle>{title}</CardTitle></CardHeader>\n    <CardContent>\n      <p className=\"whitespace-pre-wrap\">{response}</p>\n      <div className=\"mt-4 border-t pt-2 text-sm\">\n        <p><strong>Scores:</strong> R: {score.relevance} | L: {score.reasoning} | S: {score.style}</p>\n        <p className=\"italic text-muted-foreground\"><strong>Critique:</strong> {score.critique}</p>\n      </div>\n    </CardContent>\n  </Card>\n);\n```\n\n---\n\n### **4. Cost Tracking and Budget Management ($10)**\n\nThis is the most critical part of the project.\n\n**1. In-App Estimation (Code Implemented Above)**\n*   The `compareModels` action includes a rough cost calculation.\n*   **Limitation:** This is an *estimate*. It doesn't account for potential overhead or slight differences in tokenization between the SDK and the provider. It's great for real-time feedback but not for billing.\n\n**2. Vercel AI Gateway Dashboard (Source of Truth)**\n*   This is your primary tool. The AI Gateway dashboard provides **near-real-time cost reporting**.\n*   **Strategy:**\n    1.  After running a few tests, go to the AI Gateway dashboard.\n    2.  Check the \"Cost\" column. This will show you the actual cost per request.\n    3.  Calculate the average cost of one full comparison (3 model calls + 1 judge call).\n    4.  **Example Calculation:** If a full comparison costs ~$0.04, you can run `$10 / $0.04 = 250` comparisons.\n\n**3. Staying Under the $10 Budget**\n*   **Start with Cheap Models:** For initial development, you can temporarily switch the Gateway routes to cheaper models like `claude-3-haiku` and `gemini-1.0-pro` to debug the pipeline for a fraction of the cost.\n*   **Set Vercel Spending Alerts:** Go to your Vercel account settings -> **Billing** -> **Spend Management**. Set an alert for when your usage approaches $8. This gives you a buffer.\n*   **Active CPU Billing Benefit:** Vercel's new billing model is perfect for this app. Your Serverless Function (`compareModels`) is only billed for the milliseconds it's actively processing requests. Since the app is idle most of the time, you pay almost nothing for compute, ensuring the budget is spent on AI, not infrastructure.\n\n---\n\n### **5. Optional: Using Vercel Queues for Scalability**\n\nIf you wanted to build a system that could handle hundreds of comparisons without blocking the UI or timing out, you could integrate Vercel Queues.\n\n1.  **Modify the Action:** The Server Action would become very simple: it just sends a job to the queue.\n    ```typescript\n    // app/actions.ts (modified)\n    import { queue } from '@vercel/queue';\n    \n    export async function queueComparison(prevState: any, formData: FormData) {\n      const prompt = formData.get('prompt') as string;\n      await queue.send('model-comparator', { prompt });\n      return { message: \"Comparison has been queued! Results will appear on the dashboard shortly.\" };\n    }\n    ```\n\n2.  **Create a Queue Handler:** This new file contains the heavy lifting logic.\n    `app/api/queue/model-comparator/route.ts`\n    ```typescript\n    import {-queue-handler-placeholder-} from '@vercel/queue';\n    // ... import all the AI clients, Zod schemas, etc.\n    \n    // This is the function that will be executed by the queue worker\n    async function handler(payload: { prompt: string }) {\n      // Paste the entire logic from the original `compareModels` Server Action here\n      // from the `try...catch` block.\n    }\n    \n    export const POST = queue(handler);\n    ```\n\nThis architecture decouples the user request from the long-running AI process, making the application more robust and scalable.",
      "scores": {
        "relevance": 10,
        "reasoning": 5,
        "style": 5,
        "accuracy": 10,
        "honesty": 5,
        "explanation": "The response thoroughly addresses the user's prompt by providing a comprehensive design for a Next.js 15 app that utilizes Vercel's AI Gateway to compare three AI models. It includes detailed steps for implementation, technology stack, and cost management strategies, all of which align with the user's requirements. The logical flow of the architecture is clear, and the explanations for each component are well-articulated. The response accurately reflects the current status of Vercel features, including the AI Gateway, Active CPU Billing, and Vercel KV, as confirmed by the web search results. Pricing information is also correct, and the technical approaches suggested are valid for the Vercel platform. The model acknowledges potential limitations in cost estimation and suggests using Vercel's dashboard for real-time tracking, demonstrating a reasonable level of honesty about its capabilities.",
        "totalScore": 55,
        "soundnessScore": 10
      },
      "latency": 76658,
      "cost": 0.018600000000000002,
      "finalScore": -21.844
    },
    {
      "model": "anthropic/claude-4-opus",
      "response": "I'll help you design a Next.js 15 app that compares AI models using Vercel's AI Gateway. Here's a complete implementation:\n\n## Project Structure\n\n```\nai-model-comparison/\nâ”œâ”€â”€ app/\nâ”‚   â”œâ”€â”€ layout.tsx\nâ”‚   â”œâ”€â”€ page.tsx\nâ”‚   â”œâ”€â”€ api/\nâ”‚   â”‚   â”œâ”€â”€ compare/\nâ”‚   â”‚   â”‚   â””â”€â”€ route.ts\nâ”‚   â”‚   â””â”€â”€ judge/\nâ”‚   â”‚       â””â”€â”€ route.ts\nâ”œâ”€â”€ components/\nâ”‚   â”œâ”€â”€ ModelComparison.tsx\nâ”‚   â”œâ”€â”€ ResultsDashboard.tsx\nâ”‚   â”œâ”€â”€ CostTracker.tsx\nâ”‚   â””â”€â”€ JudgePanel.tsx\nâ”œâ”€â”€ lib/\nâ”‚   â”œâ”€â”€ ai-gateway.ts\nâ”‚   â”œâ”€â”€ cost-calculator.ts\nâ”‚   â””â”€â”€ types.ts\nâ”œâ”€â”€ package.json\nâ””â”€â”€ .env.local\n```\n\n## 1. Setup and Dependencies\n\n```json\n{\n  \"name\": \"ai-model-comparison\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\"\n  },\n  \"dependencies\": {\n    \"next\": \"15.0.0\",\n    \"react\": \"^18.3.0\",\n    \"react-dom\": \"^18.3.0\",\n    \"@vercel/ai\": \"^3.0.0\",\n    \"@vercel/edge\": \"^1.1.0\",\n    \"ai\": \"^3.0.0\",\n    \"openai\": \"^4.0.0\",\n    \"@anthropic-ai/sdk\": \"^0.20.0\",\n    \"@google/generative-ai\": \"^0.1.0\",\n    \"recharts\": \"^2.10.0\",\n    \"lucide-react\": \"^0.300.0\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20.0.0\",\n    \"@types/react\": \"^18.0.0\",\n    \"@types/react-dom\": \"^18.0.0\",\n    \"typescript\": \"^5.0.0\",\n    \"tailwindcss\": \"^3.4.0\",\n    \"autoprefixer\": \"^10.4.0\",\n    \"postcss\": \"^8.4.0\"\n  }\n}\n```\n\n## 2. Environment Configuration\n\n```env\n# .env.local\nOPENAI_API_KEY=your_openai_key\nANTHROPIC_API_KEY=your_anthropic_key\nGOOGLE_AI_API_KEY=your_google_key\nVERCEL_AI_GATEWAY_URL=https://gateway.vercel.com\nVERCEL_AI_GATEWAY_KEY=your_gateway_key\n```\n\n## 3. Types Definition\n\n```typescript\n// lib/types.ts\nexport interface ModelResponse {\n  model: 'grok' | 'claude' | 'gemini';\n  response: string;\n  latency: number;\n  cost: number;\n  timestamp: Date;\n}\n\nexport interface JudgeScore {\n  relevance: number;\n  reasoning: number;\n  style: number;\n  overall: number;\n}\n\nexport interface ComparisonResult {\n  prompt: string;\n  responses: ModelResponse[];\n  scores: Record<string, JudgeScore>;\n  totalCost: number;\n}\n\nexport interface CostTracking {\n  totalSpent: number;\n  remaining: number;\n  breakdown: Record<string, number>;\n}\n```\n\n## 4. AI Gateway Integration\n\n```typescript\n// lib/ai-gateway.ts\nimport { createGateway } from '@vercel/ai-gateway';\n\nconst BUDGET_LIMIT = 10.00;\nlet totalSpent = 0;\n\nexport const aiGateway = createGateway({\n  baseURL: process.env.VERCEL_AI_GATEWAY_URL,\n  apiKey: process.env.VERCEL_AI_GATEWAY_KEY,\n  cache: {\n    enabled: true,\n    ttl: 3600, // 1 hour cache\n  },\n  rateLimiting: {\n    enabled: true,\n    requests: 100,\n    window: '1h',\n  },\n});\n\nexport async function callModelsInParallel(prompt: string): Promise<ModelResponse[]> {\n  if (totalSpent >= BUDGET_LIMIT) {\n    throw new Error('Budget limit reached');\n  }\n\n  const startTime = Date.now();\n\n  const modelCalls = [\n    {\n      name: 'grok' as const,\n      call: () => aiGateway.chat.completions.create({\n        model: 'grok-beta',\n        messages: [{ role: 'user', content: prompt }],\n        max_tokens: 500,\n      }),\n    },\n    {\n      name: 'claude' as const,\n      call: () => aiGateway.chat.completions.create({\n        model: 'claude-3-sonnet',\n        messages: [{ role: 'user', content: prompt }],\n        max_tokens: 500,\n      }),\n    },\n    {\n      name: 'gemini' as const,\n      call: () => aiGateway.chat.completions.create({\n        model: 'gemini-pro',\n        messages: [{ role: 'user', content: prompt }],\n        max_tokens: 500,\n      }),\n    },\n  ];\n\n  const responses = await Promise.allSettled(\n    modelCalls.map(async ({ name, call }) => {\n      const modelStart = Date.now();\n      try {\n        const response = await call();\n        const latency = Date.now() - modelStart;\n        const cost = calculateCost(name, response.usage);\n        \n        return {\n          model: name,\n          response: response.choices[0].message.content || '',\n          latency,\n          cost,\n          timestamp: new Date(),\n        };\n      } catch (error) {\n        console.error(`Error calling ${name}:`, error);\n        return null;\n      }\n    })\n  );\n\n  return responses\n    .filter((r): r is PromiseFulfilledResult<ModelResponse> => \n      r.status === 'fulfilled' && r.value !== null\n    )\n    .map(r => r.value);\n}\n```\n\n## 5. Cost Calculator\n\n```typescript\n// lib/cost-calculator.ts\ninterface Usage {\n  prompt_tokens: number;\n  completion_tokens: number;\n}\n\nconst COST_PER_1K_TOKENS = {\n  grok: { input: 0.001, output: 0.002 },\n  claude: { input: 0.003, output: 0.015 },\n  gemini: { input: 0.0005, output: 0.0015 },\n};\n\nexport function calculateCost(model: string, usage?: Usage): number {\n  if (!usage) return 0;\n  \n  const rates = COST_PER_1K_TOKENS[model as keyof typeof COST_PER_1K_TOKENS];\n  if (!rates) return 0;\n\n  const inputCost = (usage.prompt_tokens / 1000) * rates.input;\n  const outputCost = (usage.completion_tokens / 1000) * rates.output;\n  \n  return inputCost + outputCost;\n}\n\nexport function updateBudgetTracking(costs: number[]): CostTracking {\n  const totalSpent = costs.reduce((sum, cost) => sum + cost, 0);\n  const remaining = 10.00 - totalSpent;\n  \n  return {\n    totalSpent,\n    remaining,\n    breakdown: {\n      grok: costs[0] || 0,\n      claude: costs[1] || 0,\n      gemini: costs[2] || 0,\n    },\n  };\n}\n```\n\n## 6. API Routes\n\n```typescript\n// app/api/compare/route.ts\nimport { NextRequest, NextResponse } from 'next/server';\nimport { callModelsInParallel } from '@/lib/ai-gateway';\nimport { updateBudgetTracking } from '@/lib/cost-calculator';\n\nexport const runtime = 'edge'; // Use Vercel Edge Runtime for better performance\n\nexport async function POST(request: NextRequest) {\n  try {\n    const { prompt } = await request.json();\n    \n    if (!prompt) {\n      return NextResponse.json({ error: 'Prompt required' }, { status: 400 });\n    }\n\n    const responses = await callModelsInParallel(prompt);\n    const costs = responses.map(r => r.cost);\n    const budgetStatus = updateBudgetTracking(costs);\n\n    return NextResponse.json({\n      responses,\n      budgetStatus,\n    });\n  } catch (error) {\n    console.error('Comparison error:', error);\n    return NextResponse.json(\n      { error: error instanceof Error ? error.message : 'Internal error' },\n      { status: 500 }\n    );\n  }\n}\n```\n\n```typescript\n// app/api/judge/route.ts\nimport { NextRequest, NextResponse } from 'next/server';\nimport { aiGateway } from '@/lib/ai-gateway';\nimport { JudgeScore } from '@/lib/types';\n\nexport const runtime = 'edge';\n\nexport async function POST(request: NextRequest) {\n  try {\n    const { prompt, responses } = await request.json();\n    \n    const judgePrompt = `\n      As an AI judge, evaluate these responses to the prompt: \"${prompt}\"\n      \n      Responses:\n      ${responses.map((r: any, i: number) => `\n        Response ${i + 1} (${r.model}):\n        ${r.response}\n      `).join('\\n\\n')}\n      \n      Score each response on:\n      - Relevance (0-10): How well it addresses the prompt\n      - Reasoning (0-10): Quality of logic and argumentation\n      - Style (0-10): Clarity, conciseness, and readability\n      \n      Return JSON: { \"model_name\": { \"relevance\": X, \"reasoning\": Y, \"style\": Z, \"overall\": AVG } }\n    `;\n\n    const judgeResponse = await aiGateway.chat.completions.create({\n      model: 'gpt-4-turbo',\n      messages: [{ role: 'user', content: judgePrompt }],\n      response_format: { type: 'json_object' },\n    });\n\n    const scores = JSON.parse(judgeResponse.choices[0].message.content || '{}');\n    \n    return NextResponse.json({ scores });\n  } catch (error) {\n    console.error('Judge error:', error);\n    return NextResponse.json(\n      { error: 'Failed to judge responses' },\n      { status: 500 }\n    );\n  }\n}\n```\n\n## 7. Main Components\n\n```tsx\n// components/ModelComparison.tsx\n'use client';\n\nimport { useState } from 'react';\nimport { Loader2, Send } from 'lucide-react';\nimport { ModelResponse, ComparisonResult } from '@/lib/types';\n\nexport default function ModelComparison({ \n  onResult \n}: { \n  onResult: (result: ComparisonResult) => void \n}) {\n  const [prompt, setPrompt] = useState('');\n  const [loading, setLoading] = useState(false);\n\n  const handleCompare = async () => {\n    if (!prompt.trim()) return;\n    \n    setLoading(true);\n    try {\n      // Call models\n      const compareRes = await fetch('/api/compare', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ prompt }),\n      });\n      \n      const { responses, budgetStatus } = await compareRes.json();\n      \n      // Get judge scores\n      const judgeRes = await fetch('/api/judge', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ prompt, responses }),\n      });\n      \n      const { scores } = await judgeRes.json();\n      \n      onResult({\n        prompt,\n        responses,\n        scores,\n        totalCost: budgetStatus.totalSpent,\n      });\n    } catch (error) {\n      console.error('Comparison failed:', error);\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  return (\n    <div className=\"bg-white rounded-lg shadow-lg p-6\">\n      <h2 className=\"text-2xl font-bold mb-4\">Compare AI Models</h2>\n      \n      <div className=\"space-y-4\">\n        <textarea\n          value={prompt}\n          onChange={(e) => setPrompt(e.target.value)}\n          placeholder=\"Enter your prompt here...\"\n          className=\"w-full p-3 border rounded-lg resize-none h-32\"\n          disabled={loading}\n        />\n        \n        <button\n          onClick={handleCompare}\n          disabled={loading || !prompt.trim()}\n          className=\"flex items-center gap-2 px-6 py-3 bg-blue-600 text-white rounded-lg hover:bg-blue-700 disabled:opacity-50\"\n        >\n          {loading ? (\n            <>\n              <Loader2 className=\"animate-spin h-4 w-4\" />\n              Comparing...\n            </>\n          ) : (\n            <>\n              <Send className=\"h-4 w-4\" />\n              Compare Models\n            </>\n          )}\n        </button>\n      </div>\n    </div>\n  );\n}\n```\n\n```tsx\n// components/ResultsDashboard.tsx\n'use client';\n\nimport { ComparisonResult } from '@/lib/types';\nimport { BarChart, Bar, XAxis, YAxis, CartesianGrid, Tooltip, Legend } from 'recharts';\n\nexport default function ResultsDashboard({ \n  results \n}: { \n  results: ComparisonResult[] \n}) {\n  if (results.length === 0) {\n    return (\n      <div className=\"bg-gray-100 rounded-lg p-8 text-center text-gray-500\">\n        No results yet. Start comparing models!\n      </div>\n    );\n  }\n\n  const latestResult = results[results.length - 1];\n  \n  const chartData = latestResult.responses.map(r => ({\n    model: r.model,\n    relevance: latestResult.scores[r.model]?.relevance || 0,\n    reasoning: latestResult.scores[r.model]?.reasoning || 0,\n    style: latestResult.scores[r.model]?.style || 0,\n    overall: latestResult.scores[r.model]?.overall || 0,\n    latency: r.latency,\n    cost: r.cost,\n  }));\n\n  return (\n    <div className=\"space-y-6\">\n      <div className=\"bg-white rounded-lg shadow-lg p-6\">\n        <h3 className=\"text-xl font-bold mb-4\">Latest Comparison</h3>\n        \n        <div className=\"grid grid-cols-1 md:grid-cols-3 gap-4 mb-6\">\n          {latestResult.responses.map((response) => (\n            <div key={response.model} className=\"border rounded-lg p-4\">\n              <h4 className=\"font-semibold text-lg capitalize mb-2\">\n                {response.model}\n              </h4>\n              <div className=\"text-sm text-gray-600 space-y-1\">\n                <p>Latency: {response.latency}ms</p>\n                <p>Cost: ${response.cost.toFixed(4)}</p>\n                <p>Score: {latestResult.scores[response.model]?.overall.toFixed(1) || 'N/A'}</p>\n              </div>\n              <div className=\"mt-3 text-sm\">\n                <p className=\"line-clamp-3\">{response.response}</p>\n              </div>\n            </div>\n          ))}\n        </div>\n\n        <div className=\"mt-6\">\n          <h4 className=\"font-semibold mb-3\">Performance Scores</h4>\n          <BarChart width={600} height={300} data={chartData}>\n            <CartesianGrid stroke",
      "scores": {
        "relevance": 8,
        "reasoning": 4,
        "style": 4,
        "accuracy": 6,
        "honesty": 3,
        "explanation": "The response provides a comprehensive implementation for a Next.js 15 app using Vercel's AI Gateway, addressing the user's request to compare three AI models. However, it inaccurately includes the `createGateway` function, which is not how the AI Gateway is configured according to the latest information. The response is mostly complete but lacks mention of the Vercel Sandbox and Queues, which were part of the user's prompt. The reasoning is logical, with clear steps outlined for the implementation, but it could benefit from more citations or references to Vercel's documentation. The style is clear and well-structured, making it easy to follow. The accuracy is somewhat compromised due to the incorrect implementation details and the omission of certain features. The honesty score reflects that while the response does not explicitly state knowledge limitations, it does not fully acknowledge the potential inaccuracies in the implementation details.",
        "totalScore": 39,
        "soundnessScore": 8
      },
      "latency": 65072,
      "cost": 0.30921,
      "finalScore": -29.1641
    }
  ],
  "summary": {
    "winner": "xai/grok-3",
    "totalCost": 0.370995,
    "totalLatency": 179826
  }
}