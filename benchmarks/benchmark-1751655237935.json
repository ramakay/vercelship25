{
  "timestamp": "2025-07-04T18:53:57.935Z",
  "prompt": "Design a Next.js 15 app using Vercel's AI Gateway to compare 3 AI models (Grok, Claude, Gemini) with a $10 budget. Include:\n\n1. AI Gateway integration to call all 3 models in parallel\n2. A judge component that scores responses on relevance, reasoning, and style\n3. Cost tracking to stay under $10 total\n4. Results dashboard showing model rankings and costs\n\nUse Vercel's latest features: AI Gateway (Beta), Active CPU billing, and optionally Sandbox/Queues if available.",
  "evaluations": [
    {
      "model": "xai/grok-3",
      "response": "I'll design a Next.js 15 app that compares AI models using Vercel's AI Gateway while maintaining a $10 budget. The solution will include parallel API calls, a scoring system, cost tracking, and a results dashboard. I'll use TypeScript, Tailwind CSS, and Vercel's latest features.\n\n### Project Structure and Assumptions\n- **Next.js 15** with App Router and Server Components for optimal performance.\n- **Vercel AI Gateway (Beta)** to proxy requests to Grok, Claude, and Gemini with unified API handling.\n- **Cost Tracking**: Store usage data in Vercel KV (Redis) to monitor spending against the $10 budget.\n- **Scoring**: A client-side judge component to evaluate responses based on predefined criteria.\n- **Dashboard**: Display rankings and costs using shadcn/ui components.\n- **Active CPU Billing**: Leverage Vercel's billing model for efficient serverless execution.\n- **No Sandbox/Queues**: As these are not widely documented or stable in beta, I'll stick to standard serverless functions.\n\n### Step 1: Setup and Dependencies\nCreate a new Next.js 15 project and install necessary dependencies:\n```bash\nnpx create-next-app@latest ai-model-comparator --typescript --app --tailwind\ncd ai-model-comparator\nnpm install @vercel/kv ai @shadcn/ui recharts\n```\n\nConfigure Vercel AI Gateway in your `vercel.json` (or environment variables) with API keys for Grok (xAI), Claude (Anthropic), and Gemini (Google). Ensure you have a Vercel account with AI Gateway enabled.\n\n### Step 2: Cost Estimation and Budget Guard\nWith a $10 budget, assuming average costs per 1k tokens (input + output):\n- Grok (xAI): ~$0.005/1k tokens\n- Claude (Anthropic): ~$0.015/1k tokens\n- Gemini (Google): ~$0.007/1k tokens\n\nFor a query of ~200 tokens input and ~300 tokens output per model, a single comparison (3 models) costs ~$0.0135. This allows ~740 comparisons within $10. We'll track costs in Vercel KV and block requests if the budget is exceeded.\n\n### Step 3: Implementation\n\n#### 1. AI Gateway Integration (`app/api/compare/route.ts`)\nThis serverless route uses Vercel AI Gateway to call all models in parallel and track costs.\n\n```typescript\nimport { NextResponse } from \"next/server\";\nimport { kv } from \"@vercel/kv\";\nimport { createClient } from \"ai\";\n\nconst aiClient = createClient({\n  apiKey: process.env.VERCEL_AI_GATEWAY_KEY,\n  baseUrl: \"https://gateway.ai.vercel.app\",\n});\n\nconst MODEL_CONFIG = {\n  grok: { provider: \"xai\", model: \"grok-1\", costPer1kTokens: 0.005 },\n  claude: { provider: \"anthropic\", model: \"claude-3-opus-20240229\", costPer1kTokens: 0.015 },\n  gemini: { provider: \"google\", model: \"gemini-1.5-pro\", costPer1kTokens: 0.007 },\n};\n\nexport async function POST(req: Request) {\n  try {\n    const { prompt } = await req.json();\n    const budgetKey = \"budget:usage\";\n    const currentSpend = (await kv.get(budgetKey)) || 0;\n\n    if (currentSpend >= 10) {\n      return NextResponse.json({ error: \"Budget limit of $10 exceeded\" }, { status: 402 });\n    }\n\n    // Parallel calls to all models via AI Gateway\n    const responses = await Promise.all(\n      Object.entries(MODEL_CONFIG).map(async ([name, config]) => {\n        const startTime = Date.now();\n        const response = await aiClient.chat.completions.create({\n          model: `${config.provider}/${config.model}`,\n          messages: [{ role: \"user\", content: prompt }],\n          maxTokens: 300,\n        });\n        const latency = Date.now() - startTime;\n        const tokensUsed = response.usage?.totalTokens || 0;\n        const cost = (tokensUsed / 1000) * config.costPer1kTokens;\n\n        return {\n          model: name,\n          response: response.choices[0].message.content || \"No response\",\n          latency,\n          tokens: tokensUsed,\n          cost,\n        };\n      })\n    );\n\n    const totalCost = responses.reduce((sum, r) => sum + r.cost, 0);\n    await kv.set(budgetKey, currentSpend + totalCost);\n\n    return NextResponse.json({\n      results: responses,\n      totalCost,\n      remainingBudget: 10 - (currentSpend + totalCost),\n    });\n  } catch (error) {\n    console.error(error);\n    return NextResponse.json({ error: \"Failed to fetch responses\" }, { status: 500 });\n  }\n}\n```\n\n#### 2. Judge Component (`components/Judge.tsx`)\nA client-side component to score responses based on relevance, reasoning, and style (1-5 scale).\n\n```typescript\n\"use client\";\n\nimport { useState } from \"react\";\nimport { Button } from \"@/components/ui/button\";\n\ninterface Response {\n  model: string;\n  response: string;\n  latency: number;\n  tokens: number;\n  cost: number;\n}\n\ninterface Score {\n  relevance: number;\n  reasoning: number;\n  style: number;\n  total: number;\n}\n\nexport function Judge({ responses }: { responses: Response[] }) {\n  const [scores, setScores] = useState<Record<string, Score>>({});\n\n  const calculateScore = (response: string): Score => {\n    // Simple heuristic-based scoring (in production, could use another AI model)\n    const relevance = response.includes(\"example\") ? 4 : 3;\n    const reasoning = response.length > 200 ? 4 : 2;\n    const style = response.includes(\"! \") ? 5 : 3;\n    return { relevance, reasoning, style, total: relevance + reasoning + style };\n  };\n\n  const handleScore = () => {\n    const newScores = responses.reduce((acc, res) => {\n      acc[res.model] = calculateScore(res.response);\n      return acc;\n    }, {} as Record<string, Score>);\n    setScores(newScores);\n  };\n\n  return (\n    <div className=\"mt-6\">\n      <Button onClick={handleScore} className=\"mb-4\">Score Responses</Button>\n      {Object.entries(scores).length > 0 && (\n        <div className=\"grid gap-4\">\n          {responses.map((res) => {\n            const score = scores[res.model];\n            return (\n              <div key={res.model} className=\"p-4 border rounded-md\">\n                <h3 className=\"text-lg font-bold\">{res.model}</h3>\n                <p>Response: {res.response.slice(0, 100)}...</p>\n                {score && (\n                  <div>\n                    <p>Relevance: {score.relevance}/5</p>\n                    <p>Reasoning: {score.reasoning}/5</p>\n                    <p>Style: {score.style}/5</p>\n                    <p>Total: {score.total}/15</p>\n                  </div>\n                )}\n              </div>\n            );\n          })}\n        </div>\n      )}\n    </div>\n  );\n}\n```\n\n#### 3. Cost Tracking and Budget Guard\nCost tracking is handled in the API route using Vercel KV. The UI will display the remaining budget and block further requests if exceeded.\n\n#### 4. Results Dashboard (`app/page.tsx`)\nThe main page integrates the input form, judge component, and dashboard with charts using `recharts`.\n\n```typescript\n\"use client\";\n\nimport { useState } from \"react\";\nimport { Input } from \"@/components/ui/input\";\nimport { Button } from \"@/components/ui/button\";\nimport { BarChart, Bar, XAxis, YAxis, CartesianGrid, Tooltip, ResponsiveContainer } from \"recharts\";\nimport { Judge } from \"@/components/Judge\";\n\ninterface Response {\n  model: string;\n  response: string;\n  latency: number;\n  tokens: number;\n  cost: number;\n}\n\nexport default function Home() {\n  const [prompt, setPrompt] = useState(\"\");\n  const [results, setResults] = useState<Response[]>([]);\n  const [totalCost, setTotalCost] = useState(0);\n  const [remainingBudget, setRemainingBudget] = useState(10);\n  const [loading, setLoading] = useState(false);\n  const [error, setError] = useState(\"\");\n\n  const handleCompare = async () => {\n    if (!prompt) return;\n    setLoading(true);\n    setError(\"\");\n\n    try {\n      const res = await fetch(\"/api/compare\", {\n        method: \"POST\",\n        headers: { \"Content-Type\": \"application/json\" },\n        body: JSON.stringify({ prompt }),\n      });\n      const data = await res.json();\n      if (res.ok) {\n        setResults(data.results);\n        setTotalCost(data.totalCost);\n        setRemainingBudget(data.remainingBudget);\n      } else {\n        setError(data.error || \"Failed to compare models\");\n      }\n    } catch (err) {\n      setError(\"Network error occurred\");\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  const chartData = results.map((r) => ({\n    model: r.model,\n    latency: r.latency,\n    cost: r.cost * 1000, // Scale for visibility\n  }));\n\n  return (\n    <div className=\"container mx-auto p-6 max-w-7xl\">\n      <h1 className=\"text-3xl font-bold mb-6\">AI Model Comparator</h1>\n      <div className=\"flex gap-4 mb-6\">\n        <Input\n          value={prompt}\n          onChange={(e) => setPrompt(e.target.value)}\n          placeholder=\"Enter a prompt to compare AI models\"\n          className=\"flex-1\"\n          disabled={remainingBudget <= 0 || loading}\n        />\n        <Button onClick={handleCompare} disabled={remainingBudget <= 0 || loading}>\n          {loading ? \"Comparing...\" : \"Compare\"}\n        </Button>\n      </div>\n\n      <div className=\"mb-6\">\n        <p className=\"text-lg\">Total Cost: ${totalCost.toFixed(4)}</p>\n        <p className=\"text-lg text-red-500\">Remaining Budget: ${remainingBudget.toFixed(2)}</p>\n        {error && <p className=\"text-red-500\">{error}</p>}\n      </div>\n\n      {results.length > 0 && (\n        <>\n          <div className=\"mb-6\">\n            <h2 className=\"text-2xl font-semibold mb-4\">Performance Metrics</h2>\n            <ResponsiveContainer width=\"100%\" height={300}>\n              <BarChart data={chartData}>\n                <CartesianGrid strokeDasharray=\"3 3\" />\n                <XAxis dataKey=\"model\" />\n                <YAxis yAxisId=\"left\" orientation=\"left\" stroke=\"#8884d8\" label={{ value: \"Latency (ms)\", angle: -90, position: \"insideLeft\" }} />\n                <YAxis yAxisId=\"right\" orientation=\"right\" stroke=\"#82ca9d\" label={{ value: \"Cost (x1000)\", angle: 90, position: \"insideRight\" }} />\n                <Tooltip />\n                <Bar yAxisId=\"left\" dataKey=\"latency\" fill=\"#8884d8\" name=\"Latency (ms)\" />\n                <Bar yAxisId=\"right\" dataKey=\"cost\" fill=\"#82ca9d\" name=\"Cost (scaled)\" />\n              </BarChart>\n            </ResponsiveContainer>\n          </div>\n          <Judge responses={results} />\n        </>\n      )}\n    </div>\n  );\n}\n```\n\n### Step 4: Deployment and Vercel Features\n1. **Active CPU Billing**: The serverless function in `app/api/compare/route.ts` benefits from Vercel's Active CPU billing, ensuring costs are based on actual compute time during parallel API calls.\n2. **Vercel KV**: Used for persistent cost tracking across requests.\n3. **Environment Variables**: Store API keys and Gateway credentials in Vercel’s environment variables dashboard.\n\nDeploy to Vercel:\n```bash\nvercel deploy\n```\n\n### Step 5: Budget and Limitations\n- The app stops requests if the $10 budget is exceeded, tracked via Vercel KV.\n- Scoring is heuristic-based for simplicity; a real app might use a secondary AI model for judging, increasing costs.\n- Latency and cost are visualized in the dashboard for transparency.\n\n### Final Notes\nThis design leverages Next.js 15’s Server Components for efficient rendering, Vercel AI Gateway for unified model access, and shadcn/ui with recharts for a polished UI. The $10 budget is strictly enforced, and the dashboard provides clear insights into model performance and costs. If Sandbox or Queues become stable, they could be integrated for background scoring or cost updates, but they’re omitted here due to beta status uncertainty.",
      "scores": {
        "relevance": 9,
        "reasoning": 4,
        "style": 5,
        "accuracy": 8,
        "honesty": 3,
        "explanation": "The response effectively addresses the user's prompt by outlining a comprehensive design for a Next.js 15 app that utilizes Vercel's AI Gateway to compare three AI models within a $10 budget. It includes detailed steps for implementation, cost tracking, and a results dashboard, which are all relevant to the user's request. However, it could have included more explicit details about the limitations of the AI Gateway and the potential implications of using beta features. The reasoning is mostly logical, but some assumptions about costs and model performance could be better justified. The structure is clear and well-organized, making it easy to follow. The accuracy is high, but there are minor uncertainties regarding the status of the AI Gateway and other features, which could have been acknowledged more explicitly. Overall, the response is strong but could benefit from a few clarifications and disclaimers.",
        "totalScore": 46,
        "soundnessScore": 9
      },
      "latency": 41344,
      "cost": 0.04185,
      "finalScore": 4.237499999999999
    },
    {
      "model": "anthropic/claude-4-opus",
      "response": "I'll help you design a Next.js 15 app with Vercel's AI Gateway to compare AI models. Here's a complete implementation:\n\n## Project Structure\n\n```\nai-model-comparison/\n├── app/\n│   ├── api/\n│   │   ├── compare/\n│   │   │   └── route.ts\n│   │   └── judge/\n│   │       └── route.ts\n│   ├── components/\n│   │   ├── ModelComparison.tsx\n│   │   ├── CostTracker.tsx\n│   │   ├── ResultsDashboard.tsx\n│   │   └── JudgePanel.tsx\n│   ├── layout.tsx\n│   └── page.tsx\n├── lib/\n│   ├── ai-gateway.ts\n│   ├── cost-calculator.ts\n│   └── judge.ts\n├── types/\n│   └── index.ts\n└── package.json\n```\n\n## 1. Package.json\n\n```json\n{\n  \"name\": \"ai-model-comparison\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\"\n  },\n  \"dependencies\": {\n    \"next\": \"15.0.0\",\n    \"react\": \"^18.3.0\",\n    \"react-dom\": \"^18.3.0\",\n    \"@vercel/ai\": \"^3.0.0\",\n    \"@anthropic-ai/sdk\": \"^0.20.0\",\n    \"@google/generative-ai\": \"^0.5.0\",\n    \"openai\": \"^4.40.0\",\n    \"recharts\": \"^2.10.0\",\n    \"zustand\": \"^4.5.0\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20\",\n    \"@types/react\": \"^18\",\n    \"@types/react-dom\": \"^18\",\n    \"typescript\": \"^5\"\n  }\n}\n```\n\n## 2. Types Definition\n\n```typescript\n// types/index.ts\nexport interface ModelResponse {\n  model: 'grok' | 'claude' | 'gemini';\n  response: string;\n  latency: number;\n  cost: number;\n  timestamp: number;\n}\n\nexport interface JudgeScore {\n  relevance: number;\n  reasoning: number;\n  style: number;\n  overall: number;\n}\n\nexport interface ComparisonResult {\n  prompt: string;\n  responses: ModelResponse[];\n  scores: Record<string, JudgeScore>;\n  totalCost: number;\n}\n\nexport interface CostTracking {\n  totalSpent: number;\n  budget: number;\n  breakdown: Record<string, number>;\n}\n```\n\n## 3. AI Gateway Integration\n\n```typescript\n// lib/ai-gateway.ts\nimport { createGateway } from '@vercel/ai';\n\n// Cost per 1K tokens (estimated)\nconst PRICING = {\n  grok: { input: 0.001, output: 0.002 },\n  claude: { input: 0.003, output: 0.015 },\n  gemini: { input: 0.0005, output: 0.0015 }\n};\n\nexport class AIGateway {\n  private gateway: any;\n  \n  constructor() {\n    this.gateway = createGateway({\n      baseURL: process.env.VERCEL_AI_GATEWAY_URL || '/api/gateway',\n      apiKey: process.env.VERCEL_AI_GATEWAY_KEY\n    });\n  }\n\n  async compareModels(prompt: string): Promise<ModelResponse[]> {\n    const models = ['grok', 'claude', 'gemini'] as const;\n    \n    // Run all models in parallel\n    const promises = models.map(async (model) => {\n      const startTime = Date.now();\n      \n      try {\n        const response = await this.callModel(model, prompt);\n        const latency = Date.now() - startTime;\n        const cost = this.calculateCost(model, prompt, response);\n        \n        return {\n          model,\n          response,\n          latency,\n          cost,\n          timestamp: Date.now()\n        };\n      } catch (error) {\n        console.error(`Error calling ${model}:`, error);\n        return {\n          model,\n          response: `Error: ${error.message}`,\n          latency: Date.now() - startTime,\n          cost: 0,\n          timestamp: Date.now()\n        };\n      }\n    });\n\n    return Promise.all(promises);\n  }\n\n  private async callModel(model: string, prompt: string): Promise<string> {\n    const endpoint = `/v1/models/${model}/completions`;\n    \n    const response = await this.gateway.post(endpoint, {\n      messages: [{ role: 'user', content: prompt }],\n      temperature: 0.7,\n      max_tokens: 500\n    });\n\n    return response.choices[0].message.content;\n  }\n\n  private calculateCost(model: string, prompt: string, response: string): number {\n    // Rough token estimation (1 token ≈ 4 chars)\n    const inputTokens = prompt.length / 4;\n    const outputTokens = response.length / 4;\n    \n    const pricing = PRICING[model];\n    return (inputTokens * pricing.input + outputTokens * pricing.output) / 1000;\n  }\n}\n```\n\n## 4. Cost Calculator\n\n```typescript\n// lib/cost-calculator.ts\nimport { create } from 'zustand';\nimport { persist } from 'zustand/middleware';\n\ninterface CostStore {\n  totalSpent: number;\n  budget: number;\n  breakdown: Record<string, number>;\n  addCost: (model: string, cost: number) => void;\n  getRemainingBudget: () => number;\n  canAfford: (estimatedCost: number) => boolean;\n  reset: () => void;\n}\n\nexport const useCostStore = create<CostStore>()(\n  persist(\n    (set, get) => ({\n      totalSpent: 0,\n      budget: 10,\n      breakdown: {\n        grok: 0,\n        claude: 0,\n        gemini: 0\n      },\n      \n      addCost: (model, cost) => set((state) => ({\n        totalSpent: state.totalSpent + cost,\n        breakdown: {\n          ...state.breakdown,\n          [model]: (state.breakdown[model] || 0) + cost\n        }\n      })),\n      \n      getRemainingBudget: () => {\n        const state = get();\n        return state.budget - state.totalSpent;\n      },\n      \n      canAfford: (estimatedCost) => {\n        const state = get();\n        return state.totalSpent + estimatedCost <= state.budget;\n      },\n      \n      reset: () => set({\n        totalSpent: 0,\n        breakdown: { grok: 0, claude: 0, gemini: 0 }\n      })\n    }),\n    {\n      name: 'ai-comparison-costs'\n    }\n  )\n);\n```\n\n## 5. Judge Component\n\n```typescript\n// lib/judge.ts\nexport class Judge {\n  async scoreResponse(\n    prompt: string,\n    response: string,\n    model: string\n  ): Promise<JudgeScore> {\n    // Simple scoring algorithm - in production, use another AI model\n    const relevance = this.scoreRelevance(prompt, response);\n    const reasoning = this.scoreReasoning(response);\n    const style = this.scoreStyle(response);\n    \n    const overall = (relevance + reasoning + style) / 3;\n    \n    return {\n      relevance,\n      reasoning,\n      style,\n      overall\n    };\n  }\n\n  private scoreRelevance(prompt: string, response: string): number {\n    // Check if response addresses the prompt\n    const promptKeywords = prompt.toLowerCase().split(' ');\n    const responseWords = response.toLowerCase();\n    \n    let matches = 0;\n    promptKeywords.forEach(keyword => {\n      if (responseWords.includes(keyword)) matches++;\n    });\n    \n    return Math.min((matches / promptKeywords.length) * 100, 100);\n  }\n\n  private scoreReasoning(response: string): number {\n    // Check for logical structure\n    const hasStructure = /\\b(because|therefore|however|thus|hence)\\b/i.test(response);\n    const hasExamples = /\\b(for example|such as|like)\\b/i.test(response);\n    const hasConclusion = /\\b(in conclusion|to summarize|overall)\\b/i.test(response);\n    \n    let score = 60;\n    if (hasStructure) score += 15;\n    if (hasExamples) score += 15;\n    if (hasConclusion) score += 10;\n    \n    return Math.min(score, 100);\n  }\n\n  private scoreStyle(response: string): number {\n    // Check clarity and conciseness\n    const words = response.split(' ').length;\n    const sentences = response.split(/[.!?]+/).length;\n    const avgWordsPerSentence = words / sentences;\n    \n    let score = 70;\n    if (avgWordsPerSentence > 25) score -= 10;\n    if (avgWordsPerSentence < 10) score -= 10;\n    if (response.length > 1000) score -= 10;\n    if (response.length < 100) score -= 20;\n    \n    return Math.max(score, 0);\n  }\n}\n```\n\n## 6. API Routes\n\n```typescript\n// app/api/compare/route.ts\nimport { NextRequest, NextResponse } from 'next/server';\nimport { AIGateway } from '@/lib/ai-gateway';\nimport { Judge } from '@/lib/judge';\n\nexport async function POST(request: NextRequest) {\n  try {\n    const { prompt } = await request.json();\n    \n    if (!prompt) {\n      return NextResponse.json(\n        { error: 'Prompt is required' },\n        { status: 400 }\n      );\n    }\n\n    const gateway = new AIGateway();\n    const judge = new Judge();\n    \n    // Get responses from all models\n    const responses = await gateway.compareModels(prompt);\n    \n    // Score each response\n    const scores = {};\n    for (const response of responses) {\n      scores[response.model] = await judge.scoreResponse(\n        prompt,\n        response.response,\n        response.model\n      );\n    }\n    \n    const totalCost = responses.reduce((sum, r) => sum + r.cost, 0);\n    \n    return NextResponse.json({\n      prompt,\n      responses,\n      scores,\n      totalCost\n    });\n  } catch (error) {\n    return NextResponse.json(\n      { error: error.message },\n      { status: 500 }\n    );\n  }\n}\n```\n\n## 7. Main Components\n\n```tsx\n// app/components/ModelComparison.tsx\n'use client';\n\nimport { useState } from 'react';\nimport { useCostStore } from '@/lib/cost-calculator';\nimport { ComparisonResult } from '@/types';\n\nexport function ModelComparison() {\n  const [prompt, setPrompt] = useState('');\n  const [loading, setLoading] = useState(false);\n  const [results, setResults] = useState<ComparisonResult | null>(null);\n  const { canAfford, addCost } = useCostStore();\n\n  const handleCompare = async () => {\n    if (!prompt.trim()) return;\n    \n    // Estimate cost (rough)\n    const estimatedCost = 0.05; // ~$0.05 per comparison\n    \n    if (!canAfford(estimatedCost)) {\n      alert('Budget exceeded! Please reset or increase budget.');\n      return;\n    }\n\n    setLoading(true);\n    try {\n      const response = await fetch('/api/compare', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ prompt })\n      });\n\n      const data = await response.json();\n      setResults(data);\n      \n      // Track costs\n      data.responses.forEach(r => {\n        addCost(r.model, r.cost);\n      });\n    } catch (error) {\n      console.error('Comparison failed:', error);\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  return (\n    <div className=\"space-y-4\">\n      <div className=\"flex gap-2\">\n        <input\n          type=\"text\"\n          value={prompt}\n          onChange={(e) => setPrompt(e.target.value)}\n          placeholder=\"Enter your prompt...\"\n          className=\"flex-1 p-2 border rounded\"\n        />\n        <button\n          onClick={handleCompare}\n          disabled={loading || !prompt.trim()}\n          className=\"px-4 py-2 bg-blue-500 text-white rounded disabled:opacity-50\"\n        >\n          {loading ? 'Comparing...' : 'Compare'}\n        </button>\n      </div>\n\n      {results && (\n        <div className=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n          {results.responses.map((response) => (\n            <div key={response.model} className=\"border p-4 rounded\">\n              <h3 className=\"font-bold text-lg capitalize\">{response.model}</h3>\n              <p className=\"text-sm text-gray-600\">\n                Latency: {response.latency}ms | Cost: ${response.cost.toFixed(4)}\n              </p>\n              <div className=\"mt-2 text-sm\">{response.response}</div>\n              \n              <div className=\"mt-4 space-y-1\">\n                <div>Relevance: {results.scores[response.model]?.relevance.toFixed(0)}%</div>\n                <div>Reasoning: {results.scores[response.model]?.reasoning.toFixed(0)}%</div>\n                <div>Style: {results.scores[response.model]?.style.toFixed(0)}%</div>\n                <div className=\"font-bold\">\n                  Overall: {results.scores[response.model]?.overall.toFixed(0)}%\n                </div>\n              </div>\n            </div>\n          ))}\n        </div>\n      )}\n    </div>\n  );\n}\n```\n\n```tsx\n// app/components/CostTracker.tsx\n'use client';\n\nimport { useCostStore } from '@/lib/cost-calculator';\n\nexport function CostTracker() {\n  const { totalSpent, budget, breakdown, getRemainingBudget, reset } = useCostStore();\n  const remaining = getRemainingBudget();\n  const percentUsed = (totalSpent / budget) * 100;\n\n  return (\n    <div className=\"bg-gray-100 p-4 rounded\">\n      <div className=\"flex justify-between items-center mb-2\">\n        <h3 className=\"font-bold\">Cost Tracking</h3>\n        <button\n          onClick={reset}\n          className=\"text-sm text-blue-500 hover:text-blue-700\"\n        >\n          Reset\n        </button>\n      </div>\n      \n      <div className=\"mb-2\">\n        <div className=\"flex justify-between text-sm\">\n          <span>Budget: ${budget.toFixed(2)}</span>\n          <span>Spent: ${totalSpent.toFixed(2)}</span>\n        </div>\n        <div className=\"w-full bg-gray-300 rounded-full h-2",
      "scores": {
        "relevance": 10,
        "reasoning": 5,
        "style": 5,
        "accuracy": 9,
        "honesty": 3,
        "explanation": "The response is highly relevant as it directly addresses the user's prompt by providing a complete implementation plan for a Next.js 15 app using Vercel's AI Gateway to compare three AI models. It includes all requested components: AI Gateway integration, a judge component, cost tracking, and a results dashboard. The logical flow of the implementation is clear, with well-structured code snippets and explanations for each part. The reasoning is strong, as it outlines the steps taken to achieve the desired functionality. The style is clear and concise, making it easy to follow. However, while the response is mostly accurate regarding Vercel's features, it does not explicitly mention that the AI Gateway is in Beta, which could lead to some confusion. The honesty score is lower because it does not acknowledge any limitations or potential changes in Vercel's features since the model's last training cut-off, which is important for users to consider.",
        "totalScore": 51,
        "soundnessScore": 10
      },
      "latency": 65160,
      "cost": 0.298935,
      "finalScore": -17.14935
    },
    {
      "model": "google/gemini-2.5-pro",
      "response": "Of course. Here is a comprehensive design for a Next.js 15 app that uses Vercel's AI Gateway to compare Grok, Claude, and Gemini, all while staying under a $10 budget.\n\nThis design leverages Vercel's latest features to create a modern, efficient, and cost-aware application.\n\n---\n\n### **1. Core Concept & Architecture**\n\nThe application will allow a user to submit a single prompt. This prompt is sent to a Next.js Server Action, which then uses the **Vercel AI Gateway** to make three parallel API calls to Grok, Claude 3 (Sonnet or Opus), and Gemini 1.5 Pro.\n\nOnce the three responses are received, a *fourth* AI call is made to a \"judge\" model (we'll use the cost-effective Gemini 1.5 Flash for this) to score the other three responses.\n\nAll results, including detailed scores and cumulative costs, are then displayed on a client-side dashboard.\n\n**Vercel Features Utilized:**\n\n*   **Next.js 15 App Router:** For the core application structure and React Server Components.\n*   **Server Actions:** To securely handle the API calls on the server side without needing to write separate API routes.\n*   **Vercel AI Gateway (Beta):** The central nervous system for our AI calls. It provides:\n    *   **A single API endpoint** for our app.\n    *   **Unified authentication** (we pass our Vercel token, the Gateway uses the securely stored provider keys).\n    *   **Parallel request handling.**\n    *   **Cost & Usage Monitoring:** The key to staying under our $10 budget.\n    *   **Caching & Rate Limiting:** Reduces redundant calls and prevents abuse.\n*   **Active CPU Billing:** Vercel's new billing model ensures we only pay for compute when the Server Action is actively processing the AI requests, making it highly cost-effective for this bursty workload.\n\n---\n\n### **2. Vercel AI Gateway Setup**\n\nThis is the first and most critical step.\n\n1.  **Navigate to your Vercel Project Dashboard > AI Gateway.**\n2.  **Create a New Gateway.**\n3.  **Configure the Routes:** You will create four routes within this single gateway. A \"route\" maps a slug in the Gateway URL to a specific backend model provider.\n\n| Route Slug (Example) | Model Provider | API Key (Stored in Vercel Env Vars) |\n| :--- | :--- | :--- |\n| `grok-70b` | Grok | `GROK_API_KEY` |\n| `claude-3-sonnet` | Anthropic | `ANTHROPIC_API_KEY` |\n| `gemini-1-5-pro` | Google (via Vertex AI or AI Studio) | `GOOGLE_API_KEY` |\n| `judge-gemini-flash` | Google (via Vertex AI or AI Studio) | `GOOGLE_API_KEY` |\n\n4.  **Environment Variables:** In your Vercel Project Settings, add the API keys for Grok, Anthropic, and Google. The AI Gateway will automatically and securely access these keys. You will also need to add your Gateway's URL to the environment variables.\n\n    ```bash\n    # .env.local (and in Vercel Project Settings)\n    VERCEL_AI_GATEWAY_URL=\"https://gw.vercel.ai/...\"\n    ANTHROPIC_API_KEY=\"sk-ant-...\"\n    GROK_API_KEY=\"gsk_...\"\n    GOOGLE_API_KEY=\"AIzaSy...\"\n    ```\n\n---\n\n### **3. Project Structure**\n\n```\n/my-ai-comparator\n|\n├── app/\n│   ├── layout.tsx         # Root layout\n│   ├── page.tsx           # Main page component (Client Component)\n│   └── actions.ts         # Server Action for processing the prompt\n|\n├── components/\n│   ├── PromptForm.tsx       # The user input form\n│   ├── ResultsDashboard.tsx # Displays the final comparison\n│   └── CostTracker.tsx      # Shows the budget progress\n|\n├── lib/\n│   ├── ai-sdk.ts          # AI SDK configuration for the Gateway\n│   └── costs.ts           # Utility for cost estimation\n|\n└── package.json\n```\n\n---\n\n### **4. Component & Logic Design**\n\n#### **a. `lib/ai-sdk.ts` - AI Gateway SDK Initialization**\n\nWe'll use the `@vercel/ai` package to interact with our Gateway.\n\n```typescript\n// lib/ai-sdk.ts\nimport { createOpenAI } from '@ai-sdk/openai';\n\n// This is the key part: we use the generic `createOpenAI` provider\n// but point it to our Vercel AI Gateway URL. The Gateway handles\n// the translation to Grok, Claude, etc. based on the model name (slug).\nexport const vercelAIGateway = createOpenAI({\n  baseURL: process.env.VERCEL_AI_GATEWAY_URL,\n  // The API key can be a placeholder as Vercel's Gateway injects auth.\n  // However, it's good practice to pass a Vercel Auth Token for added security/logging.\n  apiKey: process.env.VERCEL_ACCESS_TOKEN || 'dummy-key',\n});\n```\n\n#### **b. `app/actions.ts` - The Core Server Action**\n\nThis is where the parallel calls and judging logic reside.\n\n```typescript\n// app/actions.ts\n'use server';\n\nimport { streamText } from 'ai';\nimport { vercelAIGateway } from '@/lib/ai-sdk';\nimport { z } from 'zod';\n\n// Define a schema for the judge's response for safe parsing\nconst JudgeScoreSchema = z.object({\n  relevance: z.number().min(1).max(10),\n  reasoning: z.number().min(1).max(10),\n  style: z.number().min(1).max(10),\n});\n\nconst AllScoresSchema = z.object({\n  grok: JudgeScoreSchema,\n  claude: JudgeScoreSchema,\n  gemini: JudgeScoreSchema,\n});\n\nexport async function compareModels(prompt: string) {\n  if (!prompt) {\n    throw new Error('Prompt cannot be empty.');\n  }\n\n  // --- Step 1: Call all 3 models in parallel ---\n  const [grokResult, claudeResult, geminiResult] = await Promise.all([\n    streamText({\n      model: vercelAIGateway('grok-70b'), // Route Slug from Gateway\n      prompt,\n    }),\n    streamText({\n      model: vercelAIGateway('claude-3-sonnet'), // Route Slug\n      prompt,\n    }),\n    streamText({\n      model: vercelAIGateway('gemini-1-5-pro'), // Route Slug\n      prompt,\n    }),\n  ]);\n\n  const grokResponse = await grokResult.text;\n  const claudeResponse = await claudeResult.text;\n  const geminiResponse = await geminiResult.text;\n\n  // --- Step 2: Prepare the prompt for the Judge model ---\n  const judgePrompt = `\n    As an expert AI model evaluator, your task is to score three different AI responses to a user's prompt.\n    Score each response on a scale of 1 to 10 for the following criteria: relevance, reasoning, and style.\n    Provide your evaluation in a JSON object format. Do not include any other text or explanations.\n\n    Original User Prompt: \"${prompt}\"\n\n    --- Responses ---\n\n    [Grok's Response]:\n    ${grokResponse}\n\n    [Claude's Response]:\n    ${claudeResponse}\n\n    [Gemini's Response]:\n    ${geminiResponse}\n\n    --- Evaluation ---\n    Return ONLY the JSON object with your scores. Example format:\n    {\n      \"grok\": { \"relevance\": 8, \"reasoning\": 9, \"style\": 7 },\n      \"claude\": { \"relevance\": 9, \"reasoning\": 8, \"style\": 9 },\n      \"gemini\": { \"relevance\": 10, \"reasoning\": 10, \"style\": 8 }\n    }\n  `;\n\n  // --- Step 3: Call the Judge model ---\n  const judgeResult = await streamText({\n    model: vercelAIGateway('judge-gemini-flash'), // Our cost-effective judge\n    prompt: judgePrompt,\n  });\n\n  // --- Step 4: Parse the judge's response and calculate costs ---\n  let scores;\n  try {\n    const jsonResponse = await judgeResult.text;\n    scores = AllScoresSchema.parse(JSON.parse(jsonResponse));\n  } catch (error) {\n    console.error('Failed to parse judge response:', error);\n    // Handle error, maybe return a default score structure\n    throw new Error('The judge AI failed to provide a valid score.');\n  }\n\n  // The AI SDK's result object contains usage details\n  const costs = {\n    grok: (grokResult.usage.promptTokens * 0.00000027) + (grokResult.usage.completionTokens * 0.00000059),\n    claude: (claudeResult.usage.promptTokens * 0.000003) + (claudeResult.usage.completionTokens * 0.000015),\n    gemini: (geminiResult.usage.promptTokens * 0.0000035) + (geminiResult.usage.completionTokens * 0.0000105),\n    judge: (judgeResult.usage.promptTokens * 0.00000035) + (judgeResult.usage.completionTokens * 0.0000007),\n  };\n  // Note: Prices are examples and should be updated from official pricing pages.\n\n  // --- Step 5: Return the complete, structured result ---\n  return {\n    responses: {\n      grok: grokResponse,\n      claude: claudeResponse,\n      gemini: geminiResponse,\n    },\n    scores,\n    costs,\n  };\n}\n```\n\n#### **c. `app/page.tsx` - The Main Client Page**\n\nThis component manages the state and orchestrates the UI.\n\n```tsx\n// app/page.tsx\n'use client';\n\nimport { useState } from 'react';\nimport { useFormState, useFormStatus } from 'react-dom';\nimport { compareModels } from './actions';\nimport PromptForm from '@/components/PromptForm';\nimport ResultsDashboard from '@/components/ResultsDashboard';\nimport CostTracker from '@/components/CostTracker';\n\n// Define the type for our results for type safety\ntype ComparisonResult = Awaited<ReturnType<typeof compareModels>>;\n\nexport default function HomePage() {\n  const [totalCost, setTotalCost] = useState(0);\n  const [result, setResult] = useState<ComparisonResult | null>(null);\n  const [error, setError] = useState<string | null>(null);\n  const [isLoading, setIsLoading] = useState(false);\n\n  const budget = 10;\n  const isBudgetExceeded = totalCost >= budget;\n\n  const handleFormSubmit = async (prompt: string) => {\n    setIsLoading(true);\n    setError(null);\n    try {\n      const newResult = await compareModels(prompt);\n      setResult(newResult);\n      const newCost = Object.values(newResult.costs).reduce((a, b) => a + b, 0);\n      setTotalCost((prevCost) => prevCost + newCost);\n    } catch (e: any) {\n      setError(e.message);\n    } finally {\n      setIsLoading(false);\n    }\n  };\n\n  return (\n    <main className=\"container mx-auto p-4 md:p-8\">\n      <header className=\"text-center mb-8\">\n        <h1 className=\"text-4xl font-bold\">AI Model Comparator</h1>\n        <p className=\"text-muted-foreground\">\n          Grok vs. Claude vs. Gemini - Powered by Vercel AI Gateway\n        </p>\n      </header>\n\n      <CostTracker currentCost={totalCost} budget={budget} />\n\n      <PromptForm\n        onSubmit={handleFormSubmit}\n        isLoading={isLoading}\n        isDisabled={isBudgetExceeded}\n      />\n\n      {isLoading && <div className=\"text-center p-8\">Comparing models...</div>}\n      {error && <div className=\"text-center text-red-500 p-4\">{error}</div>}\n      {isBudgetExceeded && !isLoading && (\n         <div className=\"text-center text-amber-500 p-4 font-bold\">\n            Budget of ${budget} has been reached. Please deploy a new instance to continue.\n         </div>\n      )}\n\n      {result && <ResultsDashboard result={result} />}\n    </main>\n  );\n}\n```\n\n#### **d. `components/ResultsDashboard.tsx`**\n\nThis component visualizes the final results and rankings.\n\n```tsx\n// components/ResultsDashboard.tsx\nimport React from 'react';\n\n// Assuming the same `ComparisonResult` type is imported\n// ...\n\nfunction calculateOverallScore(scores: { relevance: number; reasoning: number; style: number }) {\n  // Weighted average: Reasoning is most important\n  return (scores.relevance * 0.25 + scores.reasoning * 0.5 + scores.style * 0.25).toFixed(2);\n}\n\nexport default function ResultsDashboard({ result }: { result: ComparisonResult }) {\n  const models = [\n    { name: 'Grok', response: result.responses.grok, score: result.scores.grok, cost: result.costs.grok },\n    { name: 'Claude', response: result.responses.claude, score: result.scores.claude, cost: result.costs.claude },\n    { name: 'Gemini', response: result.responses.gemini, score: result.scores.gemini, cost: result.costs.gemini },\n  ];\n\n  const rankedModels = [...models].sort(\n    (a, b) => parseFloat(calculateOverallScore(b.score)) - parseFloat(calculateOverallScore(a.score))\n  );\n\n  return (\n    <div className=\"mt-8\">\n      {/* --- Ranking Section --- */}\n      <div className=\"mb-8\">\n        <h2 className=\"text-2xl font-bold text-center mb-4\">Rankings</h2>\n        <div className=\"flex justify-center gap-4 text-center\">\n          {rankedModels.map((model, index) => (\n            <div key={model.name} className=\"p-4 border rounded-lg\">\n              <span className=\"text-2xl\">{['🥇', '🥈', '🥉'][index]}</span>\n              <p className=\"font-bold text-lg\">{model.name}</p>\n              <p className=\"text-sm text-muted-foreground\">\n                Score: {calculateOverallScore(model.score)}\n              </p>\n            </div>\n          ))}\n        </div>\n      </div>\n\n      {/* --- Detailed Responses Section --- */}\n      <div className=\"grid grid-cols-1 md:grid-cols-3 gap-6\">\n        {models.map((model) => (\n          <div key={model.name} className=\"border rounded-lg p-4 bg-card\">\n            <h3 className=\"text-xl font-semibold mb-2\">{model.name}</h3>\n            <div className=\"mb-4 space-y-1 text-sm bg-muted p-2 rounded\">\n              <p>Relevance: {model.score.relevance}/10</p>\n              <p>Reasoning: {model.score.reasoning}/10</p>\n              <p>Style: {model.score.style}/10</p>\n              <p className=\"font-bold\">Cost: ${model.cost.toFixed(6)}</p>\n            </div>\n            <div className=\"prose prose-sm max-w-none max-h-96 overflow-y-auto\">\n              {model.response}\n            </div>\n          </div>\n        ))}\n      </div>\n    </div>\n  );\n}\n```\n\n#### **e. `components/CostTracker.tsx`**\n\nA simple visual component to keep an eye on the budget.\n\n```tsx\n// components/CostTracker.tsx\nimport React from 'react';\n\nexport default function CostTracker({ currentCost, budget }: { currentCost: number; budget: number }) {\n  const percentage = (currentCost / budget) * 100;\n\n  return (\n    <div className=\"mb-6 p-4 border rounded-lg bg-secondary\">\n      <div className=\"flex justify-between items-center mb-1\">\n        <h3 className=\"font-semibold\">Budget Usage</h3>\n        <p className=\"text-sm font-mono\">\n          ${currentCost.toFixed(4)} / ${budget.toFixed(2)}\n        </p>\n      </div>\n      <div className=\"w-full bg-muted rounded-full h-2.5\">\n        <div\n          className=\"bg-primary h-2.5 rounded-full\"\n          style={{ width: `${Math.min(percentage, 100)}%` }}\n        ></div>\n      </div>\n    </div>\n  );\n}\n```\n\n---\n\n### **5. Deployment and Monitoring**\n\n1.  **Push to GitHub/GitLab:** Connect your Vercel project to your Git repository.\n2.  **Deploy:** Every `git push` will trigger a new deployment.\n3.  **Monitor Costs:** The most important part for this project.\n    *   **In-App Tracker:** The `CostTracker` component gives you a real-time *estimate*.\n    *   **Vercel AI Gateway Dashboard:** This is your source of truth. Go to the Gateway dashboard to see the exact, up-to-the-minute costs, token counts, and request logs for each model route. You can set up alerts if Vercel provides them or check this dashboard frequently to ensure you don't cross the $10 threshold.\n\nThis design provides a robust, scalable, and cost-aware solution for comparing AI models, built entirely on the modern Vercel and Next.js 15 stack.",
      "scores": {
        "relevance": 10,
        "reasoning": 5,
        "style": 5,
        "accuracy": 9,
        "honesty": 3,
        "explanation": "The response is highly relevant, addressing all aspects of the user's prompt in detail, including the integration of the AI Gateway, the judge component, cost tracking, and the results dashboard. The logical flow is clear, with well-structured sections that guide the user through the design process. The reasoning is strong, with appropriate explanations for each step and feature utilized. The style is clear and concise, making it easy to follow. However, while the accuracy is mostly high, there are minor inaccuracies regarding the AI Gateway's status (it is in Beta, but the response refers to it as 'latest features' without clarifying its beta status). The mention of Sandbox and Queues is also somewhat misleading, as they are in Public and Limited Beta, respectively, which could affect the implementation. The response does not explicitly acknowledge any knowledge limitations or the potential for outdated information, which would have warranted a higher honesty score.",
        "totalScore": 51,
        "soundnessScore": 10
      },
      "latency": 68508,
      "cost": 0.0158325,
      "finalScore": -17.666324999999997
    }
  ],
  "summary": {
    "winner": "xai/grok-3",
    "totalCost": 0.35661750000000003,
    "totalLatency": 177014
  }
}